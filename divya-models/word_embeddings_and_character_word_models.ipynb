{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Word Embeddings\n",
    "2. Translation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: boto3 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.9.46)\n",
      "Requirement already satisfied: boto>=2.32 in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: bz2file in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: requests in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.20.1)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.46 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.46)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.46->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.46->boto3->smart-open>=1.2.1->gensim) (2.7.5)\n",
      "Requirement already satisfied: google-compute-engine in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (2.8.3)\n",
      "Requirement already satisfied: distro in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (from google-compute-engine) (1.3.0)\n",
      "Requirement already satisfied: boto in /home/divyagorantla/anaconda3/lib/python3.6/site-packages (from google-compute-engine) (2.49.0)\n",
      "Requirement already satisfied: setuptools in /home/tiffanyjaya/.local/lib/python3.6/site-packages (from google-compute-engine) (40.6.2)\n"
     ]
    }
   ],
   "source": [
    "# install gensim libraries\n",
    "!pip install gensim\n",
    "!pip install google-compute-engine\n",
    "\n",
    "# afterwards, you need to reinstall python-boto package because of circular installation with google-compute-engine\n",
    "# sudo apt-get remove python-boto\n",
    "# sudo apt-get install python-boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import datetime \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# model definition\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# reduce feature sets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# data visualization\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# define current time now to be used as saving the model\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the region\n",
    "# also defined in tweets_processor.py but listed here for my own reference\n",
    "regions_mapping = {\n",
    "    \"albuquerque\":0,\n",
    "    \"billings\":1,\n",
    "    \"calgary\":2,\n",
    "    \"charlotte\":3,\n",
    "    \"chicago\":4,\n",
    "    \"cincinnati\":5,\n",
    "    \"denver\":6,\n",
    "    \"houston\":7,\n",
    "    \"kansas city\":8,\n",
    "    \"las vegas\":9,\n",
    "    \"los angeles\":10,\n",
    "    \"minneapolis\":11,\n",
    "    \"montreal\":12,\n",
    "    \"nashville\":13,\n",
    "    \"new york\":14,\n",
    "    \"oklahoma city\":15,\n",
    "    \"phoenix\":16,\n",
    "    \"pittsburgh\":17,\n",
    "    \"san francisco\":18,\n",
    "    \"seattle\":19,\n",
    "    \"tampa\":20,\n",
    "    \"toronto\":21,\n",
    "    \"washington\":22\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "Word embeddings overcome the many limitations that Bag of Words produced, in particular large sparse vectors that do not describe the meaning of the words. It provides a projection in the vector space where words with similar meanings cluster together. There are two main algorithms used for training the word vectors or Word2Vec: Continuous Bag of Words (CBOW), which predicts the word given its context, and Skip-gram, which predicts the context given the word. We will employ the latter than the former because we have limited training data. Even though CBOW trains several times faster than Skip-gram and is slightly better in terms of accuracy for the frequency of words, it also requires a large text corpus ranging from 1-100B words. \n",
    "\n",
    "GloVe is another successful word embedding algorithms with generally better word embeddings because it combines both the global statistics of matrix factorization techniques like LSA with the local context-based learning in Word2Vec. Since we would like the word embeddings to learn from live Twitter feeds, we will stick with Word2Vec as our word embedding used for the translation model later down the line. \n",
    "\n",
    "\n",
    "### Notes to self\n",
    "\n",
    "The problem with word embedding is that you have to know the pair of words in advance.\n",
    "\n",
    "Also, for classification tasks, fasttext performs faster than word2vec, might want to check this out later.\n",
    "\n",
    "How to transfer learning and word embeddings:\n",
    "1. learn word embeddings from large text corpus (1-100B words)\n",
    "2. Transfer embedding to a new task with smaller training set\n",
    "3. Continue to finetune word embeddings with new data\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "* https://www.youtube.com/watch?v=5PL0TmQhItY&t=583s\n",
    "* https://machinelearningmastery.com/what-are-word-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyagorantla/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does anyone live in the Charleston area and wa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You followed my friends to watch meüòê</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not a totally new subscriber, but just a few d...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no he doodoo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait that's  not an Anime!!</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text region\n",
       "0  does anyone live in the Charleston area and wa...      3\n",
       "1               You followed my friends to watch meüòê      3\n",
       "2  Not a totally new subscriber, but just a few d...     20\n",
       "3                                       no he doodoo     10\n",
       "4                        Wait that's  not an Anime!!     13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the datasets\n",
    "df = pd.read_csv(\"/var/data/tweets_labelled.csv\", sep=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['does',\n",
       "  'anyone',\n",
       "  'live',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Charleston',\n",
       "  'area',\n",
       "  'and',\n",
       "  'want',\n",
       "  'a',\n",
       "  'job',\n",
       "  '10',\n",
       "  'hr',\n",
       "  'for',\n",
       "  'Nov',\n",
       "  '14',\n",
       "  '21',\n",
       "  'hmu'],\n",
       " ['You', 'followed', 'my', 'friends', 'to', 'watch', 'me'],\n",
       " ['Not',\n",
       "  'a',\n",
       "  'totally',\n",
       "  'new',\n",
       "  'subscriber',\n",
       "  'but',\n",
       "  'just',\n",
       "  'a',\n",
       "  'few',\n",
       "  'days',\n",
       "  'into',\n",
       "  'it'],\n",
       " ['no', 'he', 'doodoo'],\n",
       " ['Wait', 'that', 's', 'not', 'an', 'Anime']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert each tweet to a list of words\n",
    "sentences = list()\n",
    "for sentence in df.text.tolist():\n",
    "    word_list = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
    "    sentences.append(word_list)\n",
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=767446, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "model.save('/var/models/word_embeddings_{}'.format(now.strftime('%Y%m%d_%H%M%S')))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# 40k: /var/data/word_embeddings_20181127_054247\n",
    "# 2.5M: /var/data/word_embeddings_20181127_070749\n",
    "\"\"\"\n",
    "model = Word2Vec.load('/var/data/word_embeddings_20181127_070749')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does', 'anyone', 'live', 'in', 'the']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "words = list(model.wv.vocab)\n",
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyagorantla/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# retrieve all of the vectors from the trained model\n",
    "x  = model[model.wv.vocab]\n",
    "\n",
    "# fit a 2D PCA model to the vectors \n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(x)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings for each region\n",
    "\n",
    "Redo the entire word embeddings done above into an encapsulated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/var/data/tweets_labelled_40k.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_embeddings(data):\n",
    "    \n",
    "    # convert each tweet to a list of words\n",
    "    sentences = list()\n",
    "    for sentence in df.text.tolist():\n",
    "        word_list = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
    "        sentences.append(word_list)\n",
    "\n",
    "    # train the model \n",
    "    model = Word2Vec(sentences, min_count=1)\n",
    "    model.save('/var/models/word_embeddings_{}'.format(now.strftime('%Y%m%d_%H%M%S')))\n",
    "\n",
    "    # list the words\n",
    "    words = list(model.wv.vocab)\n",
    "\n",
    "    # retrieve all of the vectors from the trained model\n",
    "    x  = model[model.wv.vocab]\n",
    "\n",
    "    # fit a 2D PCA model to the vectors \n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(x)\n",
    "    pyplot.scatter(result[:, 0], result[:, 1])\n",
    "    for i, word in enumerate(words):\n",
    "        pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Word Model\n",
    "\n",
    "pro: character word model does not have to worry about unknown word token\n",
    "cons: not good at capturing how the early parts of the sentence affects the later part of the sentence, also computationally expensive to train\n",
    "\n",
    "good use case for specialized vocabulary and lots of unknown words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
