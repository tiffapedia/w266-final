{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Calgary</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Champaign</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlotte</th>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleveland</th>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Las Vegas</th>\n",
       "      <td>3459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MontrÃ©al</th>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>3160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toronto</th>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text\n",
       "region          \n",
       "Calgary      156\n",
       "Champaign     73\n",
       "Charlotte    639\n",
       "Cleveland    460\n",
       "Las Vegas   3459\n",
       "Madison      189\n",
       "MontrÃ©al    302\n",
       "Phoenix     3160\n",
       "Pittsburgh   387\n",
       "Toronto     1162\n",
       "other         13"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv(x):\n",
    "    try:\n",
    "        return x.astype(np.int64)\n",
    "    except:\n",
    "        return 99\n",
    "\n",
    "df_yelp = pd.read_csv('yelp_reviews_labelled_mini.csv')\n",
    "df_yelp.dropna(inplace=True)\n",
    "df_counts = df_yelp.groupby('region').count()\n",
    "\n",
    "top_category_num = max(df_counts['text'])\n",
    "top_category_name = df_counts[df_counts['text']==max(df_counts['text'])].index[0]\n",
    "\n",
    "categories = df_counts.index.tolist()\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  If we just guessed 'Las Vegas' every time we would have accuracy of 34.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy:  If we just guessed '{}' every time we would have accuracy of {:.2f}%\"\n",
    "      .format(top_category_name, (top_category_num/df_yelp.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 7500 examples in ['Calgary' 'Champaign' 'Charlotte' 'Cleveland' 'Las Vegas' 'Madison'\n",
      " 'MontrÃ©al' 'Phoenix' 'Pittsburgh' 'Toronto' 'other'] categories, test set has 2500 examples\n"
     ]
    }
   ],
   "source": [
    "X = df_yelp['text'].tolist()\n",
    "y = df_yelp['region'].tolist()\n",
    "#y = list(map(int, y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Training set has {} examples in {} categories, test set has {} examples\".format(len(X_train), np.unique(y_train), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23,959 unique words in the vocabulary set, averaging 71 words per example.\n",
      "   0.0030 of the entries in the matrix are non-zero.\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "print(\"There are {:,} unique words in the vocabulary set, averaging {:.0f} words per example.\"\n",
    "      .format(train_vocab.shape[1], train_vocab.nnz/train_vocab.shape[0]))\n",
    "print(\"   {:.4f} of the entries in the matrix are non-zero.\"\n",
    "     .format(train_vocab.nnz/(train_vocab.shape[1]*train_vocab.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=0.001: 0.4117, accuracy: 43.12% \n",
      "F1 score for alpha=0.01: 0.4164, accuracy: 43.64% \n",
      "F1 score for alpha=0.1: 0.4283, accuracy: 44.80% \n",
      "F1 score for alpha=0.5: 0.4425, accuracy: 47.04% \n",
      "F1 score for alpha=1.0: 0.4139, accuracy: 45.72% \n",
      "F1 score for alpha=2.0: 0.4397, accuracy: 43.68% \n",
      "F1 score for alpha=3.0: 0.4177, accuracy: 42.44% \n",
      "F1 score for alpha=5.0: 0.4539, accuracy: 40.16% \n",
      "F1 score for alpha=10.0: 0.3782, accuracy: 37.32% \n",
      "Best alpha parameter found in test results: 5.0, returns an f1 score of 0.4539 \n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "amax = [0, 0]\n",
    "# Fit a MNB model for each value of alpha\n",
    "for a in alpha_values:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, y_train)\n",
    "    mnb_predicted_labels = mnb.predict(test_vocab)\n",
    "    mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "    mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each alpha level\n",
    "    print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(a, mnb_f1, mnb_acc*100))\n",
    "    # Keep track of which alpha value results in the highest accuracy\n",
    "    if mnb_f1 > amax[1]:\n",
    "        amax = [a, mnb_f1]    \n",
    "# Print the optimal alpha value\n",
    "print(\"Best alpha parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(amax[0], amax[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=5.0: 0.3877, accuracy: 37.24% \n"
     ]
    }
   ],
   "source": [
    "# Define a bigram vocabulary\n",
    "vec_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "train_vocab_b = vec_bigram.fit_transform(X_train)\n",
    "test_vocab_b = vec_bigram.transform(X_test)\n",
    "\n",
    "# Fit vocabulary to a Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB(alpha=amax[0])\n",
    "mnb.fit(train_vocab_b, y_train)\n",
    "mnb_predicted_labels = mnb.predict(test_vocab_b)\n",
    "mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each alpha level\n",
    "print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(amax[0], mnb_f1, mnb_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=0.01: 0.3814, accuracy: 44.52% \n",
      "F1 score for C=0.1: 0.4183, accuracy: 45.72% \n",
      "F1 score for C=0.3: 0.4148, accuracy: 44.64% \n",
      "F1 score for C=0.5: 0.4144, accuracy: 44.36% \n",
      "F1 score for C=1.0: 0.4163, accuracy: 44.20% \n",
      "F1 score for C=2.0: 0.4119, accuracy: 43.52% \n",
      "Best C parameter found in test results: 0.1, returns an f1 score of 0.4183 \n"
     ]
    }
   ],
   "source": [
    "cmax = [0, 0]\n",
    "c_values = [0.010, 0.1000, 0.3000, 0.5000, 1.000, 2.000]\n",
    "\n",
    "# Fit a LR model for each value of C\n",
    "for c in c_values:\n",
    "    log = LogisticRegression(C=c, penalty='l2', random_state=42, solver='lbfgs', max_iter=3000, multi_class='multinomial')\n",
    "    log.fit(train_vocab, y_train)\n",
    "    log_predicted_labels = log.predict(test_vocab)\n",
    "    log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "    log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each value of C\n",
    "    print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(c, log_f1, log_acc*100))\n",
    "    # Keep track of which C value results in the highest accuracy\n",
    "    if log_f1 > cmax[1]:\n",
    "        cmax = [c, log_f1]  \n",
    "\n",
    "# Print the optimal C value\n",
    "print(\"Best C parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(cmax[0], cmax[1]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=0.1: 0.3749, accuracy: 41.64% \n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(C=cmax[0], penalty='l2', random_state=42, solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "log.fit(train_vocab_b, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab_b)\n",
    "log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each value of C\n",
    "print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(cmax[0], log_f1, log_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    rs = s.lower()\n",
    "    # Replace some separators with spaces\n",
    "    rs = re.sub('\\n|-|/|\\.', ' ', rs)\n",
    "    # Eliminate everything else that isn't a letter or number\n",
    "    rs = re.sub('[^0-9a-z ]+', '', rs)\n",
    "    # Eliminate extraneous spaces\n",
    "    rs = re.sub('\\s{2,}', ' ', rs)\n",
    "    prs = []\n",
    "    # Drop some low-value words\n",
    "    dumbwords = ['is', 'it', 'to', 'the', 'and', 'not', 'no', 'on', 'of', 'for', 'as', 'by', 'in', 'by', 'am', 'etc', \\\n",
    "                 'was', 'that', 'has', 'at', 'or', 'we', 'be', 'had']\n",
    "    for word in rs.split():\n",
    "        # Eliminate the -ing and -ly suffices\n",
    "        word = word[:-3] if word[-3:]=='ing' and len(word) > 5 else word\n",
    "        word = word[:-2] if word[-2:]=='ly' and len(word) > 5 else word\n",
    "        # Trim words to 9 characters\n",
    "        word = word[:9] if len(word) > 9 else word\n",
    "        # Eliminate single-character words\n",
    "        if len(word) > 1 and word not in dumbwords:\n",
    "            prs.append(word)\n",
    "    \n",
    "    return \" \".join(prs)\n",
    "\n",
    "proc_train_data = [better_preprocessor(x) for x in X_train]\n",
    "proc_test_data = [better_preprocessor(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed: 23,959 words with accuracy: 0.4572\n",
      "Pre-processed: 22,641 words with accuracy: 0.4576\n",
      "Improvement: 0.0004\n",
      "\n",
      "Sample wrong answer from the preprocessed set, post #558:\n",
      "unprocessed prediction: Toronto\n",
      "preprocessed prediction: Toronto\n",
      "true label: Las Vegas\n",
      "true data:  this mesa grill does live up one ny city ive eaten here two occasions dinner brunch very disappoin both times almost all dishes here were drawn with sauces think sauce should enhance dish over power only dish would recommend spicy chicken sweet potato hash their brunch menu 16 50with avocado tomato maytag blue cheese buttermil serrano dress dish very nice presented also hearty fill other good thing here their complimen mini blue corn breads sincere feel chefs this location should step up plate real refine their dishes cook skills inorder meet standard one nyc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a baseline vectorizer\n",
    "vec = CountVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Make a preprocessed vectorizer\n",
    "vec_proc = CountVectorizer(preprocessor=better_preprocessor)\n",
    "vocab_proc = vec_proc.fit_transform(X_train)\n",
    "test_vocab_proc = vec_proc.transform(X_test)\n",
    "\n",
    "# Fit and predict the baseline\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(vocab, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab)\n",
    "log_score = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Fit and predict the pre-processed set\n",
    "log_proc = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log_proc.fit(vocab_proc, y_train)\n",
    "log_proc_predicted_labels = log_proc.predict(test_vocab_proc)\n",
    "log_proc_score = metrics.accuracy_score(y_test, log_proc_predicted_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Unprocessed: {:,} words with accuracy: {:.4f}\\nPre-processed: {:,} words with accuracy: {:.4f}\"\n",
    "      .format(vocab.shape[1], log_score, vocab_proc.shape[1], log_proc_score))\n",
    "print(\"Improvement: {:.4f}\".format(log_proc_score-log_score))\n",
    "\n",
    "# Find a wrong answer and print it out for better analysis\n",
    "wrong = np.random.choice(np.where(y_test != log_proc_predicted_labels)[0].ravel())\n",
    "\n",
    "print(\"\\nSample wrong answer from the preprocessed set, post #{}:\".format(wrong))\n",
    "print(\"unprocessed prediction: {}\".format(log_predicted_labels[wrong]))\n",
    "print(\"preprocessed prediction: {}\".format(log_proc_predicted_labels[wrong]))\n",
    "print(\"true label: {}\".format(y_test[wrong]))\n",
    "print(\"true data: \",proc_test_data[wrong])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 for TfidfVectorizer: 0.44, accuracy 45.96%\n",
      "\n",
      "TOP 3 MISIDENTIFIED DOCUMENTS:\n",
      "DOCUMENT #1\n",
      "Predicted label: Phoenix (P99.5%), True label: Charlotte (P20.6%)\n",
      "R ratio: 29957.27\n",
      "Starbucks in Terminal C.\n",
      "\n",
      "$3.70 for a small (500ml) Fiji water. That's insane and I'm an idiot for paying it.\n",
      "----\n",
      "\n",
      "DOCUMENT #2\n",
      "Predicted label: Phoenix (P99.4%), True label: Champaign (P20.6%)\n",
      "R ratio: 55504.26\n",
      "I came here for lunch on Friday.  It was pretty quiet- only a couple other people eating.  I had the chicken quesadilla and what was supposed to be a carne asada taco with no onions.  Prices were fairly low and the food came rather quickly.  First off, they messed up my order as the taco wasn't carne asada.  Then when I sent it back and it returned, the meat was carne asada, but was covered in onions.  By this point, I was over it, so I didn't bother to send it back a third time- just didn't eat it.  \n",
      "\n",
      "I was expecting more from this place because of all of the rave reviews.  To be honest, my quesadilla was really oily (on the outside!) and extremely bland. It was missing heat, spice, anything to liven up the taste.  \n",
      "\n",
      "Overall, I wasn't too impressed and doubt I'll be back. It just wasn't all that.\n",
      "----\n",
      "\n",
      "DOCUMENT #3\n",
      "Predicted label: Las Vegas (P99.9%), True label: Phoenix (P20.6%)\n",
      "R ratio: 89327.61\n",
      "Best Place on the strip ! Food was awesome. Bathrooms clean and Service was impeccable!\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a TFIDF Vectorizer and fit the training and dev vocabularies\n",
    "vec = TfidfVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Inverse vocabulary dictionary for word lookup\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "# Fit and predict a logistic regression based on the results\n",
    "lr = LogisticRegression(C=100, solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "lr.fit(vocab, y_train)\n",
    "lr_predicted_labels = lr.predict(test_vocab)\n",
    "# Calculate and print the resulting score\n",
    "lr_score = metrics.f1_score(y_test, lr_predicted_labels, average='weighted', labels=np.unique(lr_predicted_labels))\n",
    "lr_acc = metrics.accuracy_score(y_test, lr_predicted_labels)\n",
    "\n",
    "print(\"Baseline F1 for TfidfVectorizer: {:.2f}, accuracy {:.2f}%\\n\".format(lr_score, lr_acc*100))\n",
    "\n",
    "# Get the probabilities for each class prediction\n",
    "probs = lr.predict_proba(test_vocab)\n",
    "R = []\n",
    "# Run through the probabilities and calculate the R ratio as defined in the prompt, saving the value in the R list\n",
    "for x in range(0, len(probs)):\n",
    "    num = np.max(probs[x])\n",
    "    den = probs[x][np.unique(y_test).tolist().index(y_test[x])]\n",
    "    R.append(num/den)\n",
    "# Get the highest x number of R values\n",
    "top = np.argsort(np.array(R))[len(R)-3:]\n",
    "\n",
    "# Print the top misidentified documents as well as their TFIDF score and coefficients by class\n",
    "print(\"TOP {} MISIDENTIFIED DOCUMENTS:\".format(3))\n",
    "c = 1\n",
    "for i in top:\n",
    "    print(\"DOCUMENT #{}\".format(c))\n",
    "    print(\"Predicted label: {} (P{:.1f}%), True label: {} (P{:.1f}%)\"\n",
    "          .format(lr_predicted_labels[i], np.max(probs[i])*100, y_test[i], probs[1][categories.index(y_test[1])]*100))\n",
    "    print(\"R ratio: {:.2f}\".format(R[i]))\n",
    "    print(X_test[i])\n",
    "    '''\n",
    "    print(\"\\n{:10} {:>10} {:>15} {:>15} {:>15} {:>22} \".format(\"word\", \"Tfidf\", categories[0], categories[1], \\\n",
    "                                                               categories[2], categories[3]))\n",
    "    for w in np.nonzero(dev_vocab[i])[1]:\n",
    "        coefs = np.round(lr.coef_[:,w], 2).flat\n",
    "        print(\"{:10} {:10.3f} {:>15} {:>15} {:>15} {:>22}\".format(inv_vocab[w], dev_vocab[i][0,w], \\\n",
    "                                                                  coefs[0], coefs[1], coefs[2], coefs[3])\n",
    "    '''\n",
    "    print(\"----\\n\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calgary'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topwords = np.argsort(log.coef_, 1)[:, train_vocab.shape[1]-top:]\n",
    "inv_vocab[topwords[0][::-1][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calgary</th>\n",
       "      <th>Champaign</th>\n",
       "      <th>Charlotte</th>\n",
       "      <th>Cleveland</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>Madison</th>\n",
       "      <th>MontrÃ©al</th>\n",
       "      <th>Phoenix</th>\n",
       "      <th>Pittsburgh</th>\n",
       "      <th>Toronto</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calgary</td>\n",
       "      <td>champaign</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>vegas</td>\n",
       "      <td>madison</td>\n",
       "      <td>montreal</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>toronto</td>\n",
       "      <td>il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pub</td>\n",
       "      <td>ever</td>\n",
       "      <td>uptown</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>strip</td>\n",
       "      <td>old</td>\n",
       "      <td>poutine</td>\n",
       "      <td>scottsdale</td>\n",
       "      <td>upstairs</td>\n",
       "      <td>favourite</td>\n",
       "      <td>again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>found</td>\n",
       "      <td>too</td>\n",
       "      <td>craft</td>\n",
       "      <td>burgers</td>\n",
       "      <td>casino</td>\n",
       "      <td>always</td>\n",
       "      <td>le</td>\n",
       "      <td>arizona</td>\n",
       "      <td>times</td>\n",
       "      <td>flavour</td>\n",
       "      <td>poutine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>service</td>\n",
       "      <td>pizza</td>\n",
       "      <td>matthews</td>\n",
       "      <td>drive</td>\n",
       "      <td>henderson</td>\n",
       "      <td>order</td>\n",
       "      <td>est</td>\n",
       "      <td>valley</td>\n",
       "      <td>recommend</td>\n",
       "      <td>rude</td>\n",
       "      <td>haddock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free</td>\n",
       "      <td>selection</td>\n",
       "      <td>wax</td>\n",
       "      <td>ohio</td>\n",
       "      <td>summerlin</td>\n",
       "      <td>way</td>\n",
       "      <td>terrace</td>\n",
       "      <td>az</td>\n",
       "      <td>addition</td>\n",
       "      <td>roti</td>\n",
       "      <td>das</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>massage</td>\n",
       "      <td>vet</td>\n",
       "      <td>ready</td>\n",
       "      <td>parking</td>\n",
       "      <td>show</td>\n",
       "      <td>helpful</td>\n",
       "      <td>cheap</td>\n",
       "      <td>tempe</td>\n",
       "      <td>brunch</td>\n",
       "      <td>latte</td>\n",
       "      <td>uns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wine</td>\n",
       "      <td>any</td>\n",
       "      <td>center</td>\n",
       "      <td>days</td>\n",
       "      <td>las</td>\n",
       "      <td>junk</td>\n",
       "      <td>de</td>\n",
       "      <td>pool</td>\n",
       "      <td>party</td>\n",
       "      <td>soft</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kind</td>\n",
       "      <td>things</td>\n",
       "      <td>puppies</td>\n",
       "      <td>appetizer</td>\n",
       "      <td>greeted</td>\n",
       "      <td>things</td>\n",
       "      <td>favourite</td>\n",
       "      <td>patio</td>\n",
       "      <td>down</td>\n",
       "      <td>yonge</td>\n",
       "      <td>vineyard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>list</td>\n",
       "      <td>chili</td>\n",
       "      <td>waited</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>located</td>\n",
       "      <td>tree</td>\n",
       "      <td>floor</td>\n",
       "      <td>choose</td>\n",
       "      <td>waffles</td>\n",
       "      <td>canada</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>own</td>\n",
       "      <td>shop</td>\n",
       "      <td>hibachi</td>\n",
       "      <td>egg</td>\n",
       "      <td>buffet</td>\n",
       "      <td>find</td>\n",
       "      <td>meat</td>\n",
       "      <td>thank</td>\n",
       "      <td>applebee</td>\n",
       "      <td>winter</td>\n",
       "      <td>molto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fairly</td>\n",
       "      <td>seemed</td>\n",
       "      <td>opinion</td>\n",
       "      <td>east</td>\n",
       "      <td>bellagio</td>\n",
       "      <td>your</td>\n",
       "      <td>ambiance</td>\n",
       "      <td>chandler</td>\n",
       "      <td>flavor</td>\n",
       "      <td>wings</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>door</td>\n",
       "      <td>hair</td>\n",
       "      <td>off</td>\n",
       "      <td>myself</td>\n",
       "      <td>pictures</td>\n",
       "      <td>crowded</td>\n",
       "      <td>2nd</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>told</td>\n",
       "      <td>seafood</td>\n",
       "      <td>di</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>star</td>\n",
       "      <td>frosting</td>\n",
       "      <td>bacon</td>\n",
       "      <td>chipotle</td>\n",
       "      <td>lol</td>\n",
       "      <td>hear</td>\n",
       "      <td>japanese</td>\n",
       "      <td>truck</td>\n",
       "      <td>sat</td>\n",
       "      <td>okay</td>\n",
       "      <td>con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>though</td>\n",
       "      <td>try</td>\n",
       "      <td>area</td>\n",
       "      <td>pickles</td>\n",
       "      <td>tempura</td>\n",
       "      <td>really</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>professional</td>\n",
       "      <td>ingredients</td>\n",
       "      <td>veggie</td>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time</td>\n",
       "      <td>shows</td>\n",
       "      <td>movie</td>\n",
       "      <td>any</td>\n",
       "      <td>nevada</td>\n",
       "      <td>clean</td>\n",
       "      <td>traditional</td>\n",
       "      <td>gilbert</td>\n",
       "      <td>busy</td>\n",
       "      <td>patties</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>book</td>\n",
       "      <td>sunday</td>\n",
       "      <td>grits</td>\n",
       "      <td>life</td>\n",
       "      <td>showed</td>\n",
       "      <td>impressed</td>\n",
       "      <td>best</td>\n",
       "      <td>mesa</td>\n",
       "      <td>district</td>\n",
       "      <td>client</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>soup</td>\n",
       "      <td>remember</td>\n",
       "      <td>lunch</td>\n",
       "      <td>beers</td>\n",
       "      <td>desert</td>\n",
       "      <td>capital</td>\n",
       "      <td>cash</td>\n",
       "      <td>word</td>\n",
       "      <td>filling</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>cod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>live</td>\n",
       "      <td>even</td>\n",
       "      <td>queso</td>\n",
       "      <td>chinese</td>\n",
       "      <td>tickets</td>\n",
       "      <td>curds</td>\n",
       "      <td>quite</td>\n",
       "      <td>burrito</td>\n",
       "      <td>seat</td>\n",
       "      <td>patio</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>day</td>\n",
       "      <td>bbq</td>\n",
       "      <td>serious</td>\n",
       "      <td>someone</td>\n",
       "      <td>opened</td>\n",
       "      <td>addressed</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>tasty</td>\n",
       "      <td>gta</td>\n",
       "      <td>salad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>canada</td>\n",
       "      <td>daughter</td>\n",
       "      <td>rude</td>\n",
       "      <td>enjoyed</td>\n",
       "      <td>ayce</td>\n",
       "      <td>reading</td>\n",
       "      <td>meal</td>\n",
       "      <td>warranty</td>\n",
       "      <td>bar</td>\n",
       "      <td>centre</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Calgary  Champaign  Charlotte  Cleveland  Las Vegas    Madison  \\\n",
       "0   calgary  champaign  charlotte  cleveland      vegas    madison   \n",
       "1       pub       ever     uptown  fantastic      strip        old   \n",
       "2     found        too      craft    burgers     casino     always   \n",
       "3   service      pizza   matthews      drive  henderson      order   \n",
       "4      free  selection        wax       ohio  summerlin        way   \n",
       "5   massage        vet      ready    parking       show    helpful   \n",
       "6      wine        any     center       days        las       junk   \n",
       "7      kind     things    puppies  appetizer    greeted     things   \n",
       "8      list      chili     waited  meatballs    located       tree   \n",
       "9       own       shop    hibachi        egg     buffet       find   \n",
       "10   fairly     seemed    opinion       east   bellagio       your   \n",
       "11     door       hair        off     myself   pictures    crowded   \n",
       "12     star   frosting      bacon   chipotle        lol       hear   \n",
       "13   though        try       area    pickles    tempura     really   \n",
       "14     time      shows      movie        any     nevada      clean   \n",
       "15     book     sunday      grits       life     showed  impressed   \n",
       "16     soup   remember      lunch      beers     desert    capital   \n",
       "17     live       even      queso    chinese    tickets      curds   \n",
       "18      day        bbq    serious    someone     opened  addressed   \n",
       "19   canada   daughter       rude    enjoyed       ayce    reading   \n",
       "\n",
       "      MontrÃ©al       Phoenix   Pittsburgh        Toronto     other  \n",
       "0      montreal       phoenix   pittsburgh        toronto        il  \n",
       "1       poutine    scottsdale     upstairs      favourite     again  \n",
       "2            le       arizona        times        flavour   poutine  \n",
       "3           est        valley    recommend           rude   haddock  \n",
       "4       terrace            az     addition           roti       das  \n",
       "5         cheap         tempe       brunch          latte       uns  \n",
       "6            de          pool        party           soft       had  \n",
       "7     favourite         patio         down          yonge  vineyard  \n",
       "8         floor        choose      waffles         canada       die  \n",
       "9          meat         thank     applebee         winter     molto  \n",
       "10     ambiance      chandler       flavor          wings      also  \n",
       "11          2nd   outstanding         told        seafood        di  \n",
       "12     japanese         truck          sat           okay       con  \n",
       "13   portuguese  professional  ingredients         veggie        un  \n",
       "14  traditional       gilbert         busy        patties       non  \n",
       "15         best          mesa     district         client    paired  \n",
       "16         cash          word      filling  unfortunately       cod  \n",
       "17        quite       burrito         seat          patio       und  \n",
       "18    beautiful     cancelled        tasty            gta     salad  \n",
       "19         meal      warranty          bar         centre      were  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = 20\n",
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "# Make an inverse vocabulary to look up words by index\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(train_vocab, y_train)\n",
    "# Get the words with the highest coefficients from each class\n",
    "topwords = np.argsort(log.coef_, 1)[:, train_vocab.shape[1]-top:]\n",
    "df_topwords = pd.DataFrame()\n",
    "\n",
    "for x in range(topwords.shape[0]):\n",
    "    wordlist = [inv_vocab[x] for x in topwords[x][::-1]]\n",
    "    df_topwords[categories[x]] = wordlist\n",
    "\n",
    "df_topwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
