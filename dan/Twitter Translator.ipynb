{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Input, LSTM, CuDNNLSTM, Dense, Bidirectional, BatchNormalization, Dropout, Reshape, Concatenate, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "import datetime\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"albuquerque\", \"billings\", \"calgary\", \"charlotte\", \"chicago\", \"cincinnati\", \"denver\", \"houston\", \"kansas city\",\n",
    "       \"las vegas\", \"los angeles\", \"minneapolis\", \"montreal\", \"nashville\", \"new york\", \"oklahoma city\", \"phoenix\",\n",
    "       \"pittsburgh\", \"san francisco\", \"seattle\", \"tampa\", \"toronto\", \"washington\"]\n",
    "df = pd.read_csv('data/tweets_labelled.csv', nrows=100000)\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "X2 = [\"<s> \"+x+\" <e>\" for x in X]\n",
    "X3 = [x+\" <e>\" for x in X]\n",
    "y = df['region'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  7, 10, 13, 14, 15, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "training_ratio = .75\n",
    "training_size = int(len(X)*training_ratio)\n",
    "num_classes = 23\n",
    "target_num_words = 5000\n",
    "H = 500\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "embedding_vector_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode strings\n",
    "t = text.Tokenizer(num_words=10000, lower=True, char_level=False, filters='')\n",
    "\n",
    "# Convert strings to sequences, pad them to uniform length, and divide up training and test sets\n",
    "t.fit_on_texts(X2)\n",
    "word_index = t.word_index\n",
    "V = 10002 #len(word_index)+1\n",
    "index_word = {v: k for k, v in t.word_index.items()}\n",
    "X_seq = t.texts_to_sequences(X)\n",
    "X2_seq = t.texts_to_sequences(X2)\n",
    "X3_seq = t.texts_to_sequences(X3)\n",
    "x_length = max(len(x) for x in X2_seq)\n",
    "X_padded = sequence.pad_sequences(X_seq, maxlen=x_length, padding='post')\n",
    "X2_padded = sequence.pad_sequences(X2_seq, maxlen=x_length, padding='post')\n",
    "X3_padded = sequence.pad_sequences(X3_seq, maxlen=x_length, padding='post')\n",
    "\n",
    "X_train = X_padded[:training_size]\n",
    "X2_train = X2_padded[:training_size]\n",
    "X3_train = X3_padded[:training_size]\n",
    "X_test = X_padded[training_size:]\n",
    "X2_test = X2_padded[training_size:]\n",
    "X3_test = X3_padded[training_size:]\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "# One-hot encode labels\n",
    "encoded_y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "encoded_y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "#X_train_target = to_categorical(X3_train, num_classes=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156840 unique words in this dataset, but we're only using the top 10002.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} unique words in this dataset, but we're only using the top {}.\".format(len(word_index), V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to feed batches into the model\n",
    "class OneHotBatch(Sequence):\n",
    "  def __init__(self, X_data, X2_data, X3_data, y_data, batch_size, V, num_classes):\n",
    "    self.X_data = X_data\n",
    "    self.X2_data = X2_data\n",
    "    self.X3_data = X3_data\n",
    "    self.y_data = y_data\n",
    "    self.batch_size = batch_size\n",
    "    self.V = V\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def __len__(self):\n",
    "     return int(np.ceil(len(self.X_data) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, batch_id):\n",
    "    start = batch_id * self.batch_size\n",
    "    finish = start + self.batch_size\n",
    "    X = self.X_data[start:finish]\n",
    "    X2 = self.X2_data[start:finish]\n",
    "    X3 = to_categorical(self.X3_data[start:finish], num_classes=self.V)\n",
    "    y = to_categorical(self.y_data[start:finish], num_classes=self.num_classes)\n",
    "\n",
    "    return [X, X2], [y, X3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open('data/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((V, embedding_vector_length))\n",
    "for word, i in word_index.items():\n",
    "    if i == V:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# 1Bi x 1LSTM\n",
    "# 9m params with 1000 words\n",
    "\n",
    "# define training encoder\n",
    "encoder_inputs = Input(shape=(None, ), name=\"encoder_input\")\n",
    "encoder = Embedding(V, embedding_vector_length, weights=[embedding_matrix], trainable=True, \n",
    "                    name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(CuDNNLSTM(H, return_state=True), name=\"encoder_lstm_1\")(encoder)\n",
    "state_h = Concatenate(name=\"concatenate_state_h\")([forward_h, backward_h])\n",
    "state_c = Concatenate(name=\"concatenate_state_c\")([forward_c, backward_c])\n",
    "encoder_dense = Dense(num_classes, activation='softmax', name=\"encoder_dense\")\n",
    "encoder_outputs = encoder_dense(state_h)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# define training decoder\n",
    "decoder_inputs = Input(shape=(None, ), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(V, embedding_vector_length, name=\"decoder_embedding\")\n",
    "embedded_input = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = CuDNNLSTM(H*2, return_sequences=True, return_state=True, name=\"decoder_lstm_1\")\n",
    "decoder_outputs, _, _ = decoder_lstm(embedded_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(V, activation='softmax', name=\"decoder_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Combine training inputs into a single training model\n",
    "model = Model([encoder_inputs, decoder_inputs], [encoder_outputs, decoder_outputs])\n",
    "\n",
    "# define inference encoder\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "\n",
    "# define inference decoder\n",
    "decoder_state_input_h = Input(shape=(H*2,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = Input(shape=(H*2,), name=\"decoder_state_input_c\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_input_2 = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_input_2, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 200)    2000400     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (Bidirectional)  [(None, 1000), (None 2808000     encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_state_h (Concatenat (None, 1000)         0           encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][3]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 200)    2000400     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_state_c (Concatenat (None, 1000)         0           encoder_lstm_1[0][2]             \n",
      "                                                                 encoder_lstm_1[0][4]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (CuDNNLSTM)      [(None, None, 1000), 4808000     decoder_embedding[0][0]          \n",
      "                                                                 concatenate_state_h[0][0]        \n",
      "                                                                 concatenate_state_c[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_dense (Dense)           (None, 23)           23023       concatenate_state_h[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 10002)  10012002    decoder_lstm_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 21,651,825\n",
      "Trainable params: 21,651,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "1172/1172 [==============================] - 808s 690ms/step - loss: 4.1697 - encoder_dense_loss: 2.8283 - decoder_dense_loss: 1.3414 - encoder_dense_acc: 0.1457 - decoder_dense_acc: 0.8134 - val_loss: 3.8331 - val_encoder_dense_loss: 2.7271 - val_decoder_dense_loss: 1.1060 - val_encoder_dense_acc: 0.1826 - val_decoder_dense_acc: 0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_state_h_2/concat:0' shape=(?, 1000) dtype=float32>, <tf.Tensor 'concatenate_state_c_2/concat:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 795s 678ms/step - loss: 3.4948 - encoder_dense_loss: 2.5942 - decoder_dense_loss: 0.9006 - encoder_dense_acc: 0.2200 - decoder_dense_acc: 0.8580 - val_loss: 3.4386 - val_encoder_dense_loss: 2.6264 - val_decoder_dense_loss: 0.8122 - val_encoder_dense_acc: 0.2209 - val_decoder_dense_acc: 0.8706\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 795s 678ms/step - loss: 3.0796 - encoder_dense_loss: 2.4287 - decoder_dense_loss: 0.6508 - encoder_dense_acc: 0.2667 - decoder_dense_acc: 0.8865 - val_loss: 3.3407 - val_encoder_dense_loss: 2.6859 - val_decoder_dense_loss: 0.6547 - val_encoder_dense_acc: 0.2080 - val_decoder_dense_acc: 0.8895\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 795s 679ms/step - loss: 2.7145 - encoder_dense_loss: 2.2152 - decoder_dense_loss: 0.4993 - encoder_dense_acc: 0.3264 - decoder_dense_acc: 0.9051 - val_loss: 3.4317 - val_encoder_dense_loss: 2.8444 - val_decoder_dense_loss: 0.5873 - val_encoder_dense_acc: 0.1986 - val_decoder_dense_acc: 0.8981\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 794s 678ms/step - loss: 2.3120 - encoder_dense_loss: 1.9059 - decoder_dense_loss: 0.4060 - encoder_dense_acc: 0.4168 - decoder_dense_acc: 0.9182 - val_loss: 3.6387 - val_encoder_dense_loss: 3.0768 - val_decoder_dense_loss: 0.5618 - val_encoder_dense_acc: 0.1821 - val_decoder_dense_acc: 0.9015\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 794s 678ms/step - loss: 1.8439 - encoder_dense_loss: 1.5015 - decoder_dense_loss: 0.3424 - encoder_dense_acc: 0.5430 - decoder_dense_acc: 0.9279 - val_loss: 4.1011 - val_encoder_dense_loss: 3.5419 - val_decoder_dense_loss: 0.5592 - val_encoder_dense_acc: 0.1787 - val_decoder_dense_acc: 0.9027\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 794s 677ms/step - loss: 1.3672 - encoder_dense_loss: 1.0741 - decoder_dense_loss: 0.2931 - encoder_dense_acc: 0.6807 - decoder_dense_acc: 0.9361 - val_loss: 4.6249 - val_encoder_dense_loss: 4.0576 - val_decoder_dense_loss: 0.5673 - val_encoder_dense_acc: 0.1746 - val_decoder_dense_acc: 0.9023\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 793s 677ms/step - loss: 0.9569 - encoder_dense_loss: 0.7066 - decoder_dense_loss: 0.2503 - encoder_dense_acc: 0.7987 - decoder_dense_acc: 0.9437 - val_loss: 5.1819 - val_encoder_dense_loss: 4.6036 - val_decoder_dense_loss: 0.5783 - val_encoder_dense_acc: 0.1729 - val_decoder_dense_acc: 0.9021\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 793s 677ms/step - loss: 0.6602 - encoder_dense_loss: 0.4481 - decoder_dense_loss: 0.2121 - encoder_dense_acc: 0.8809 - decoder_dense_acc: 0.9511 - val_loss: 5.8171 - val_encoder_dense_loss: 5.2296 - val_decoder_dense_loss: 0.5875 - val_encoder_dense_acc: 0.1685 - val_decoder_dense_acc: 0.9019\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 792s 676ms/step - loss: 0.4703 - encoder_dense_loss: 0.2914 - decoder_dense_loss: 0.1789 - encoder_dense_acc: 0.9280 - decoder_dense_acc: 0.9579 - val_loss: 6.3458 - val_encoder_dense_loss: 5.7474 - val_decoder_dense_loss: 0.5984 - val_encoder_dense_acc: 0.1701 - val_decoder_dense_acc: 0.9017\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 801s 683ms/step - loss: 0.3516 - encoder_dense_loss: 0.2007 - decoder_dense_loss: 0.1509 - encoder_dense_acc: 0.9532 - decoder_dense_acc: 0.9637 - val_loss: 6.7606 - val_encoder_dense_loss: 6.1468 - val_decoder_dense_loss: 0.6139 - val_encoder_dense_acc: 0.1672 - val_decoder_dense_acc: 0.9016\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 801s 683ms/step - loss: 0.2873 - encoder_dense_loss: 0.1593 - decoder_dense_loss: 0.1280 - encoder_dense_acc: 0.9647 - decoder_dense_acc: 0.9687 - val_loss: 7.0441 - val_encoder_dense_loss: 6.4180 - val_decoder_dense_loss: 0.6261 - val_encoder_dense_acc: 0.1684 - val_decoder_dense_acc: 0.9017\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 801s 684ms/step - loss: 0.2595 - encoder_dense_loss: 0.1440 - decoder_dense_loss: 0.1155 - encoder_dense_acc: 0.9673 - decoder_dense_acc: 0.9711 - val_loss: 7.2505 - val_encoder_dense_loss: 6.6106 - val_decoder_dense_loss: 0.6399 - val_encoder_dense_acc: 0.1617 - val_decoder_dense_acc: 0.9008\n",
      "Epoch 14/100\n",
      "1171/1172 [============================>.] - ETA: 0s - loss: 0.2466 - encoder_dense_loss: 0.1397 - decoder_dense_loss: 0.1069 - encoder_dense_acc: 0.9677 - decoder_dense_acc: 0.9728"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = time.time()\n",
    "\n",
    "# Generators\n",
    "train_generator = OneHotBatch(X_data=X_train, X2_data=X2_train, X3_data=X3_train, y_data=y_train, \n",
    "                              batch_size=batch_size, V=V, num_classes=num_classes)\n",
    "validation_generator = OneHotBatch(X_data=X_test, X2_data=X2_test, X3_data=X3_test, y_data=y_test, \n",
    "                              batch_size=batch_size, V=V, num_classes=num_classes)\n",
    "\n",
    "# Compile and train the model\n",
    "opt = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, clipvalue=.05)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "callbacks = [EarlyStopping(monitor='decoder_dense_loss', patience=3, min_delta=.05, restore_best_weights=True),\n",
    "             ModelCheckpoint(filepath='models/Twitter_Translator_word1k.h5', \n",
    "                             monitor='decoder_dense_loss', save_best_only=True)]\n",
    "             #TensorBoard(log_dir='./logs/Twitter_Translator_100k5k_1Bix1L_masked', histogram_freq=0, batch_size=32, write_graph=False, \n",
    "             #            write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, \n",
    "             #            embeddings_metadata=None, embeddings_data=None, update_freq='epoch')]\n",
    "\n",
    "model.fit_generator(generator=train_generator, callbacks=callbacks, epochs=100, validation_data=validation_generator)\n",
    "                    #max_queue_size=10, workers=5, use_multiprocessing=True)\n",
    "# Final evaluation of the model\n",
    "end_time = time.time()\n",
    "run_time = datetime.timedelta(seconds=end_time-start_time)\n",
    "print(\"Finished in {}\".format(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_state_h_1/concat:0' shape=(?, 1000) dtype=float32>, <tf.Tensor 'concatenate_state_c_1/concat:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Twitter_100k5k_1Bix1L'\n",
    "model.save('models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_state_input_h_1:0' shape=(?, 1000) dtype=float32>, <tf.Tensor 'decoder_state_input_c_1:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save the encoder/decoder model and weights to disk\n",
    "with open('encoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(encoder_model.to_json())\n",
    "encoder_model.save_weights('models/Twitter_100k5k_1Bix1L_encoder_weights.h5')\n",
    "\n",
    "with open('decoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(decoder_model.to_json())\n",
    "decoder_model.save_weights('models/Twitter_100k5k_1Bix1L_decoder_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "def load_model_weights(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "#model_name = 'Twitter_Translator_100k5k_1Bix1L_masked'\n",
    "model_name = 'Twitter_100k5k_1Bix1L'\n",
    "model = load_model('/models/{}.h5'.format(model_name))\n",
    "\n",
    "encoder_model = load_model_weights('encoder_model.json', 'models/Twitter_100k5k_1Bix1L_encoder_weights.h5')\n",
    "decoder_model = load_model_weights('decoder_model.json', 'models/Twitter_100k5k_1Bix1L_decoder_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"if you're looking for work in check out this #job: #hiring #careerarc\",\n",
       " 'charlotte',\n",
       " 0.13493766,\n",
       " \"if you're looking for work in check out this #job: #hiring #careerarc\",\n",
       " 'charlotte',\n",
       " 0.13493766)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the loaded model is working correctly\n",
    "predict_sequence(encoder_model, decoder_model, X_train[[2]], x_length, translate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(infenc, infdec, source, n_steps, translate=False):\n",
    "    '''\n",
    "    Given a source array, feed it through the autoencoder to predict a string - either itself in the naive case \n",
    "    where translation is turned off, or run gradient ascent to convert the source array to a target category,\n",
    "    and run that through the autoencoder to get the translated version.\n",
    "    '''\n",
    "    region_dict = {0: \"albuquerque\", 1: \"billings\", 2: \"calgary\", 3: \"charlotte\", 4: \"chicago\", 5: \"cincinnati\", 6: \"denver\", \n",
    "               7: \"houston\", 8: \"kansas city\", 9: \"las vegas\", 10: \"los angeles\", 11: \"minneapolis\", 12: \"montreal\", \n",
    "               13: \"nashville\", 14: \"new york\", 15: \"oklahoma city\", 16: \"phoenix\", 17: \"pittsburgh\", 18: \"san francisco\", \n",
    "               19: \"seattle\", 20: \"tampa\", 21: \"toronto\", 22: \"washington\"}\n",
    "    \n",
    "    source_string = \" \".join([index_word[x] for x in source[0] if x > 0])\n",
    "    # feed the source into the encoder inference model\n",
    "    encode = infenc.predict(source)\n",
    "    # make prediction of category for source sequence\n",
    "    label_prediction_probs = encode[0][0]\n",
    "    label_prediction = np.argmax(label_prediction_probs)\n",
    "    source_label_prediction = region_dict[label_prediction]\n",
    "    source_label_certainty = label_prediction_probs[label_prediction]\n",
    "    \n",
    "    # If set to translate, run gradient ascent to maximize to the target_label\n",
    "    if translate:\n",
    "        state = gradient_ascent(source, infenc, translate)\n",
    "    else:\n",
    "        state = encode[1:]\n",
    "\n",
    "    decode_sequence = decode_latent(state)\n",
    "    decode_string = decode_padded_sequence(decode_sequence[0])\n",
    "    \n",
    "    # make prediction of category for predicted response\n",
    "    decode_prediction = infenc.predict(decode_sequence)\n",
    "    label_prediction_probs = decode_prediction[0][0]\n",
    "    label_prediction = np.argmax(label_prediction_probs)\n",
    "    decode_label_prediction = region_dict[label_prediction]\n",
    "    decode_label_certainty = label_prediction_probs[label_prediction]\n",
    "    \n",
    "    return (source_string, source_label_prediction, source_label_certainty,\n",
    "            decode_string, decode_label_prediction, decode_label_certainty, decode_sequence)\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm, used to scale gradient ascent\n",
    "    #return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "    return x / K.max(x)\n",
    "\n",
    "def gradient_ascent(seq, model, target):\n",
    "    '''\n",
    "    Run gradient ascent to maximize a sequence to a target category.  Returns final state values.\n",
    "    '''\n",
    "    target_probability = .95 # You want the model to be this certain the string is in the target category\n",
    "    \n",
    "    # Identify the target model layers and tensors for input and output\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    lstm_input = layer_dict['encoder_lstm_1'].input # Use the layer that accepts the embeddings\n",
    "    model_input = model.input\n",
    "    \n",
    "    loss = K.mean(model.output[0][:, target]) # The loss value for the target category\n",
    "    states = K.identity(model.output[1:]) # The h, c values for this iteration\n",
    "    grads = K.gradients(loss, lstm_input)[0] # The gradients for the lstm_input layer w/respect to the loss\n",
    "    grads = normalize(grads) # Play with this function to scale the speed of the ascent\n",
    "    \n",
    "    # Define input/output functions\n",
    "    get_embeddings = K.function([model_input], [lstm_input])\n",
    "    run_ascent = K.function([lstm_input], [loss, grads, states])\n",
    "\n",
    "    # Input sequence to model to initiate the ascent\n",
    "    embeddings_value = get_embeddings([seq])[0]\n",
    "    \n",
    "    # Iterate through the model until loss exceeds target probability\n",
    "    while True:\n",
    "        loss_value, grads_value, states_value = run_ascent([embeddings_value])\n",
    "        if loss_value > target_probability: # Exit the ascent\n",
    "            target_shape = states_value.shape[2]\n",
    "            return [np.reshape(states_value[0][0], (1,target_shape)), np.reshape(states_value[1][0], (1,target_shape))]\n",
    "            break\n",
    "        elif loss_value <= 0.: # Some inputs can zero out the loss\n",
    "            break\n",
    "        else:\n",
    "            embeddings_value += grads_value\n",
    "\n",
    "def decode_latent(state):\n",
    "    '''\n",
    "    Given a pair of state vectors, run iteratively through the decoder to build an output sequence.\n",
    "    Returns the padded output sequence\n",
    "    '''\n",
    "    # start of sequence input\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = 1 # Start Character index\n",
    "    output = list()\n",
    "    stop = False\n",
    "    word_counter = 1\n",
    "    while not stop:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        # Exit if sampled character is end token or we've reached max length\n",
    "        decoded_char = index_word[sampled_token_index] if sampled_token_index > 0 else ''\n",
    "        if (decoded_char == '<e>' or word_counter == x_length):\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        output.append(sampled_token_index)\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states and counter\n",
    "        state = [h, c]\n",
    "        word_counter += 1\n",
    "\n",
    "    while len(output) < x_length:\n",
    "        output.append(0)\n",
    "    predicted_sequence = np.array([output])\n",
    "    return predicted_sequence\n",
    "\n",
    "def get_embeddings(seq):\n",
    "    # Given a sequence, get the word embeddings for this model\n",
    "    layer_dict = dict([(layer.name, layer) for layer in encoder_model.layers[1:]])\n",
    "\n",
    "    model_input = layer_dict['encoder_embedding'].input\n",
    "    embedding_layer_output = encoder_model.layers[1].output\n",
    "    lookup_embeddings = K.function([model_input], [embedding_layer_output])\n",
    "    embeddings = lookup_embeddings([seq])[0].flatten()\n",
    "\n",
    "    return list(embeddings)\n",
    "\n",
    "def decode_padded_sequence(seq):\n",
    "    return \" \".join([index_word[x] for x in seq if x > 0])\n",
    "\n",
    "def get_padded_sequence(sentence):\n",
    "    # Given a sentence string, return the padded sequence of index numbers according to the tokenizer\n",
    "    s_seq = [word_index[x] for x in sentence.split()]\n",
    "    s_padded = sequence.pad_sequences([s_seq], maxlen=x_length, padding='post')\n",
    "    return s_padded\n",
    "\n",
    "def translate_sentence(sentence, target):\n",
    "    # Put in a sentence and a target region, get a result\n",
    "    source = get_padded_sequence(sentence)\n",
    "    result = predict_sequence(encoder_model, decoder_model, source, x_length, target)\n",
    "    return (result[0], result[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (4.96%) if you're looking for work in check out this #job: #hiring #careerarc\n",
      "1: (5.70%) do everyone recommend looking for a time the show it as the but his games -\n",
      "2: (8.87%) why wants yourself for working at it your second been one of left .\n",
      "3: (13.15%) don’t care looking for for a country then the time in his has asking that wants a lot that more a of a few fit on @\n",
      "4: (12.15%) don’t forget yourself for or be every time he the election has been the right about the person the real person that one of them a few some one at some video from work alone games at north\n",
      "5: (18.67%) what’s twitter looking for work in it for the right time without he could pay his money he knows the real makes it knows about one of a little than some one about a few of a few around them taking work alone work from work in time. nc\n",
      "6: (3.19%) don’t forget yourself for this - if we was next year for the election about the he needs to the one has a way of the real one person who has a little about some of a lot of a lot of on a few hours at pm at work 4 pm &amp; watch 6 pm 6 time.\n",
      "7: (19.72%) let’s keep another food for if your only works your job has it gets about a who actually gets some real like a too much 2 more bit about some more bit about or a few gun from me. or wearing from me. or me. i’m fine y’all tx\n",
      "8: (70.18%) what’s twitter needs for free without check out !\n",
      "9: (99.90%) follow everyone ready - full #job in fl\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "region_dict = {0: \"albuquerque\", 1: \"billings\", 2: \"calgary\", 3: \"charlotte\", 4: \"chicago\", 5: \"cincinnati\", 6: \"denver\", \n",
    "               7: \"houston\", 8: \"kansas city\", 9: \"las vegas\", 10: \"los angeles\", 11: \"minneapolis\", 12: \"montreal\", \n",
    "               13: \"nashville\", 14: \"new york\", 15: \"oklahoma city\", 16: \"phoenix\", 17: \"pittsburgh\", 18: \"san francisco\", \n",
    "               19: \"seattle\", 20: \"tampa\", 21: \"toronto\", 22: \"washington\"}\n",
    "\n",
    "seq = X_train[[2]]\n",
    "target = 20\n",
    "\n",
    "target_probability = .95 # You want the model to be this certain the string is in the target category\n",
    "layer_dict = dict([(layer.name, layer) for layer in encoder_model.layers[1:]])\n",
    "layer_name = 'encoder_lstm_1'\n",
    "layer_input = layer_dict[layer_name].input\n",
    "input_txt = encoder_model.input\n",
    "loss = K.mean(encoder_model.output[0][:, target])\n",
    "states = K.identity(encoder_model.output[1:])\n",
    "grads = K.gradients(loss, layer_input)[0]\n",
    "grads = normalize(grads)\n",
    "initiate = K.function([input_txt], [layer_input])\n",
    "iterate = K.function([layer_input], [loss, grads])\n",
    "terminate = K.function([layer_input], [states])\n",
    "\n",
    "embedding = initiate([seq])[0]\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([embedding])\n",
    "    final_output = terminate([embedding])[0]\n",
    "    target_shape = final_output.shape[2]\n",
    "    new_state = [np.reshape(final_output[0][0], (1,target_shape)), np.reshape(final_output[1][0], (1,target_shape))]\n",
    "    print(\"{}: ({:.2f}%) {}\".format(i, loss_value*100, decode_latent(new_state)))\n",
    "\n",
    "    if loss_value > target_probability:\n",
    "        final_output = terminate([embedding])[0]\n",
    "        target_shape = final_output.shape[2]\n",
    "        print(\"All Done\")\n",
    "        break\n",
    "    elif loss_value <= 0.:\n",
    "        break\n",
    "    else:\n",
    "        embedding += grads_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Source Prediction</th>\n",
       "      <th>Source Certainty</th>\n",
       "      <th>Decoded</th>\n",
       "      <th>Decoded Prediction</th>\n",
       "      <th>Decoded Certainty</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>will never that to the he's not only to trump ...</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.164445</td>\n",
       "      <td>lmao not new never to the album has no new it ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.432241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i miss you but i got no time for that</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.160725</td>\n",
       "      <td>lmao i don’t get yo la no la lmao no lmao , lm...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.778648</td>\n",
       "      <td>0.159367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in you needed a late night of awesome and welc...</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.120146</td>\n",
       "      <td>lmao la la being la la &amp;amp; its los the ride ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.922191</td>\n",
       "      <td>0.279969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i’ve seen the of having students years in a ju...</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.100750</td>\n",
       "      <td>lmao having the la more years is having a of t...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.367551</td>\n",
       "      <td>0.558873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can i a for the fact that you use in your it m...</td>\n",
       "      <td>washington</td>\n",
       "      <td>0.105180</td>\n",
       "      <td>cant you feel for the new of my name no times ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.504507</td>\n",
       "      <td>0.460299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>you the man</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.103490</td>\n",
       "      <td>lmao you tonight @ los</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.934378</td>\n",
       "      <td>0.198028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tonight</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.129843</td>\n",
       "      <td>los pm \\r\\n tryna understand it.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.740494</td>\n",
       "      <td>0.172306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and by my forever @</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.118423</td>\n",
       "      <td>by &amp;amp; la your baby @ 😭</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.471474</td>\n",
       "      <td>0.262803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>so if and then takes an to in and win out. the...</td>\n",
       "      <td>tampa</td>\n",
       "      <td>0.106776</td>\n",
       "      <td>i’m getting through when and gets the that get...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.311067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and with a</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.125073</td>\n",
       "      <td>los new los fine</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.938229</td>\n",
       "      <td>0.229712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>top all the best</td>\n",
       "      <td>chicago</td>\n",
       "      <td>0.121839</td>\n",
       "      <td>lmao by the la !!</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.555626</td>\n",
       "      <td>0.391731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>so great to see our thanks for always having t...</td>\n",
       "      <td>toronto</td>\n",
       "      <td>0.127550</td>\n",
       "      <td>tonight having your new tonight to la !! my th...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.732963</td>\n",
       "      <td>0.322521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and i was i might have all over</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.106216</td>\n",
       "      <td>lmao at my was gonna be finally \\r\\n ... 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.449519</td>\n",
       "      <td>0.373483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lol. y’all saw me about that last</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>lmao la me already open me coming with</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.588147</td>\n",
       "      <td>0.344255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>me today home from work to watch the</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.117153</td>\n",
       "      <td>lmao me tonight with la till the rain and i’m ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.586297</td>\n",
       "      <td>0.132044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and sales has been a part of the county air fa...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.378444</td>\n",
       "      <td>being 3 single and she has a day the la day lo...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.917277</td>\n",
       "      <td>0.403527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nov - it is a time of on being of your and the...</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.238694</td>\n",
       "      <td>lmao single being full is with the album and m...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.572205</td>\n",
       "      <td>0.466920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>we don’t give a what you we can’t wait for you...</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>lmao la cant get new tonight every new my be t...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.824589</td>\n",
       "      <td>0.351394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i’m in someone the fuck me</td>\n",
       "      <td>washington</td>\n",
       "      <td>0.120210</td>\n",
       "      <td>lmao la feel the ... @ los</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.934510</td>\n",
       "      <td>0.349156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this game man</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.130108</td>\n",
       "      <td>yo la was going tonight</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.563443</td>\n",
       "      <td>0.182177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i was the friend all you stupid bitches</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.138319</td>\n",
       "      <td>lmao i was la la ... forever @ los</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.928456</td>\n",
       "      <td>0.288169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one i’d like to see someone like or do a that ...</td>\n",
       "      <td>toronto</td>\n",
       "      <td>0.111605</td>\n",
       "      <td>lmao la la la to show me now she plus no la if...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.273216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i’m lol like ain’t nobody said this shit</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.146703</td>\n",
       "      <td>lmao it’s la feel me now omg wow lmao 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.746466</td>\n",
       "      <td>0.248964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i can give some about the</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.137345</td>\n",
       "      <td>lmao la get tonight feels the beautiful 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.756194</td>\n",
       "      <td>0.370771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hey follow me and you’ll get the same dm 😊</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.114345</td>\n",
       "      <td>lmao la no music you and your music to los lmao 🤣</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.930502</td>\n",
       "      <td>0.292452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hey lil you know my pull up cuz</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.175969</td>\n",
       "      <td>lmao me ... 3 school ... me tf finally me kids...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.169377</td>\n",
       "      <td>0.128522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>everything is and of things change in the 2nd</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.117358</td>\n",
       "      <td>lmao feels is and best ride to</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.243982</td>\n",
       "      <td>0.390481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i’ve been really trying to my head around the ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.112277</td>\n",
       "      <td>lmao having tryna ever out to feel is and ther...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.307981</td>\n",
       "      <td>0.383911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i’m so over being in a can i get my own place a</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.109630</td>\n",
       "      <td>lmao being fine la in your you feel - proud</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.665182</td>\n",
       "      <td>0.446537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>soon to your</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.150564</td>\n",
       "      <td>lmao to la please forever @</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.717130</td>\n",
       "      <td>0.301976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>see his can you say</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>yo la new i am yo lmao</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.746046</td>\n",
       "      <td>0.327908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>damn my parents come home tomorrow</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.169787</td>\n",
       "      <td>lmao my friends @ los new</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.917059</td>\n",
       "      <td>0.460392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>straight fire talk about it.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.140618</td>\n",
       "      <td>los los am long lmao 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.938456</td>\n",
       "      <td>0.323878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>christmas for &amp;amp;</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.122967</td>\n",
       "      <td>christmas car and you</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.141119</td>\n",
       "      <td>0.470863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>same here i finally 2 weeks but i also had &amp;am...</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>lmao i bought a year last year i finally up an...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.444337</td>\n",
       "      <td>0.403117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>by x is my new</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.144167</td>\n",
       "      <td>by la this @ los ca</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.928891</td>\n",
       "      <td>0.389054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>😂😂😂 into for</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.118291</td>\n",
       "      <td>lmao la soon today. love</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.680807</td>\n",
       "      <td>0.219884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>i’ve no idea why i like a 😂😂😂</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.105224</td>\n",
       "      <td>lmao la feel you now me lmao @</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.711679</td>\n",
       "      <td>0.409815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rain -&amp;gt; light up -&amp;gt; -&amp;gt;</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>lmao los buy fine tonight 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>0.229404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>don’t you think that one day is not enough for...</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.114512</td>\n",
       "      <td>lmao i feel one is coming no now for that plea...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.708060</td>\n",
       "      <td>0.513221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>to for for good and a chance to see will</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.115478</td>\n",
       "      <td>lmao was up for this long is gonna los tonight...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.969901</td>\n",
       "      <td>0.296223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>don’t me then try to come around stay</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.170934</td>\n",
       "      <td>lmao stay open ca to stay \\r\\n \\r\\n \\r\\n</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.571916</td>\n",
       "      <td>0.483996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>i just voted for on the and vote</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.106519</td>\n",
       "      <td>lmao gonna get la los show open !! tonight</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.920861</td>\n",
       "      <td>0.236602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>i’m my face like</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.104902</td>\n",
       "      <td>lmao la forever ... 🤣 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.781162</td>\n",
       "      <td>0.310206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>great idea to for in and to a group of</td>\n",
       "      <td>toronto</td>\n",
       "      <td>0.140469</td>\n",
       "      <td>lmao having new to los the best being a 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.367327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>taking my time 😂</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.164534</td>\n",
       "      <td>la class me plus plus my kids or i’ll stay bc ...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.317123</td>\n",
       "      <td>0.066848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>at times w/ in san as the front another in &amp;am...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.326499</td>\n",
       "      <td>music new la am it’s the sweet was the new alb...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.678941</td>\n",
       "      <td>0.321176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>women are more to die with the don’t put anyth...</td>\n",
       "      <td>washington</td>\n",
       "      <td>0.099970</td>\n",
       "      <td>lmao women feel with la !! !! no \\r\\n \\r\\n \\r\\...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.685141</td>\n",
       "      <td>0.185579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>x &amp;amp; lil</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.288408</td>\n",
       "      <td>lmao w &amp;amp; clear you’re tryna it.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.316024</td>\n",
       "      <td>0.158439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>girl i was a and we so why aren’t you together...</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.105178</td>\n",
       "      <td>i’m at my be a beautiful and i was a beautiful...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.538948</td>\n",
       "      <td>0.317066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>nov am today hi tonight clear</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.532154</td>\n",
       "      <td>lmao la am going tonight</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.692821</td>\n",
       "      <td>0.319283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>watching the national dog show with my dog</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.112943</td>\n",
       "      <td>san the la la ca ever la bc i’m getting bc i'm...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.473142</td>\n",
       "      <td>0.119524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>i get it i get it</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.162323</td>\n",
       "      <td>ca + me every + ca feel your you feel bc she s...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.613443</td>\n",
       "      <td>0.130017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>on set it’s or - always</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.141787</td>\n",
       "      <td>2 park &amp;amp; last night it’s 3 days at 4 schoo...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.148797</td>\n",
       "      <td>0.117431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>i can’t wait to go to the</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.112127</td>\n",
       "      <td>lmao yo time to los your ca &amp;amp; \\r\\n \\r\\n</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.936743</td>\n",
       "      <td>0.322133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>well now it is</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>lmao los been</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.855094</td>\n",
       "      <td>0.273075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>no from this</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.123383</td>\n",
       "      <td>yo new los am \\r\\n</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.923803</td>\n",
       "      <td>0.236296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>this.</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.187807</td>\n",
       "      <td>lmao</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>0.166971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>nice big you are a can you make me like</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.136425</td>\n",
       "      <td>lmao straight you more is fine ... me ... ca c...</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.864734</td>\n",
       "      <td>0.322859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>happy thanksgiving to</td>\n",
       "      <td>tampa</td>\n",
       "      <td>0.153813</td>\n",
       "      <td>lmao los your 😍</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.909075</td>\n",
       "      <td>0.236760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Source Source Prediction  \\\n",
       "0   will never that to the he's not only to trump ...          new york   \n",
       "1               i miss you but i got no time for that           houston   \n",
       "2   in you needed a late night of awesome and welc...          new york   \n",
       "3   i’ve seen the of having students years in a ju...           seattle   \n",
       "4   can i a for the fact that you use in your it m...        washington   \n",
       "5                                         you the man     oklahoma city   \n",
       "6                                             tonight       los angeles   \n",
       "7                                 and by my forever @       los angeles   \n",
       "8   so if and then takes an to in and win out. the...             tampa   \n",
       "9                                          and with a           seattle   \n",
       "10                                   top all the best           chicago   \n",
       "11  so great to see our thanks for always having t...           toronto   \n",
       "12                    and i was i might have all over           seattle   \n",
       "13                  lol. y’all saw me about that last           houston   \n",
       "14               me today home from work to watch the           houston   \n",
       "15  and sales has been a part of the county air fa...     san francisco   \n",
       "16  nov - it is a time of on being of your and the...          new york   \n",
       "17  we don’t give a what you we can’t wait for you...           houston   \n",
       "18                         i’m in someone the fuck me        washington   \n",
       "19                                      this game man           houston   \n",
       "20            i was the friend all you stupid bitches           houston   \n",
       "21  one i’d like to see someone like or do a that ...           toronto   \n",
       "22           i’m lol like ain’t nobody said this shit           houston   \n",
       "23                          i can give some about the           houston   \n",
       "24         hey follow me and you’ll get the same dm 😊         nashville   \n",
       "25                    hey lil you know my pull up cuz           houston   \n",
       "26      everything is and of things change in the 2nd         nashville   \n",
       "27  i’ve been really trying to my head around the ...       los angeles   \n",
       "28    i’m so over being in a can i get my own place a        cincinnati   \n",
       "29                                       soon to your     san francisco   \n",
       "..                                                ...               ...   \n",
       "70                                see his can you say        cincinnati   \n",
       "71                 damn my parents come home tomorrow           houston   \n",
       "72                       straight fire talk about it.       los angeles   \n",
       "73                                christmas for &amp;          new york   \n",
       "74  same here i finally 2 weeks but i also had &am...         nashville   \n",
       "75                                     by x is my new       los angeles   \n",
       "76                                       😂😂😂 into for           houston   \n",
       "77                      i’ve no idea why i like a 😂😂😂           houston   \n",
       "78                    rain -&gt; light up -&gt; -&gt;           seattle   \n",
       "79  don’t you think that one day is not enough for...          new york   \n",
       "80           to for for good and a chance to see will           seattle   \n",
       "81              don’t me then try to come around stay           houston   \n",
       "82                   i just voted for on the and vote         charlotte   \n",
       "83                                   i’m my face like         charlotte   \n",
       "84             great idea to for in and to a group of           toronto   \n",
       "85                                   taking my time 😂         charlotte   \n",
       "86  at times w/ in san as the front another in &am...     san francisco   \n",
       "87  women are more to die with the don’t put anyth...        washington   \n",
       "88                                        x &amp; lil           houston   \n",
       "89  girl i was a and we so why aren’t you together...         nashville   \n",
       "90                      nov am today hi tonight clear          new york   \n",
       "91         watching the national dog show with my dog       los angeles   \n",
       "92                                  i get it i get it           houston   \n",
       "93                            on set it’s or - always        cincinnati   \n",
       "94                          i can’t wait to go to the           houston   \n",
       "95                                     well now it is          new york   \n",
       "96                                       no from this          new york   \n",
       "97                                              this.          new york   \n",
       "98            nice big you are a can you make me like        cincinnati   \n",
       "99                              happy thanksgiving to             tampa   \n",
       "\n",
       "    Source Certainty                                            Decoded  \\\n",
       "0           0.164445  lmao not new never to the album has no new it ...   \n",
       "1           0.160725  lmao i don’t get yo la no la lmao no lmao , lm...   \n",
       "2           0.120146  lmao la la being la la &amp; its los the ride ...   \n",
       "3           0.100750  lmao having the la more years is having a of t...   \n",
       "4           0.105180  cant you feel for the new of my name no times ...   \n",
       "5           0.103490                             lmao you tonight @ los   \n",
       "6           0.129843                   los pm \\r\\n tryna understand it.   \n",
       "7           0.118423                          by &amp; la your baby @ 😭   \n",
       "8           0.106776  i’m getting through when and gets the that get...   \n",
       "9           0.125073                                   los new los fine   \n",
       "10          0.121839                                  lmao by the la !!   \n",
       "11          0.127550  tonight having your new tonight to la !! my th...   \n",
       "12          0.106216         lmao at my was gonna be finally \\r\\n ... 😍   \n",
       "13          0.163276             lmao la me already open me coming with   \n",
       "14          0.117153  lmao me tonight with la till the rain and i’m ...   \n",
       "15          0.378444  being 3 single and she has a day the la day lo...   \n",
       "16          0.238694  lmao single being full is with the album and m...   \n",
       "17          0.099892  lmao la cant get new tonight every new my be t...   \n",
       "18          0.120210                         lmao la feel the ... @ los   \n",
       "19          0.130108                            yo la was going tonight   \n",
       "20          0.138319                 lmao i was la la ... forever @ los   \n",
       "21          0.111605  lmao la la la to show me now she plus no la if...   \n",
       "22          0.146703            lmao it’s la feel me now omg wow lmao 😍   \n",
       "23          0.137345          lmao la get tonight feels the beautiful 😍   \n",
       "24          0.114345  lmao la no music you and your music to los lmao 🤣   \n",
       "25          0.175969  lmao me ... 3 school ... me tf finally me kids...   \n",
       "26          0.117358                     lmao feels is and best ride to   \n",
       "27          0.112277  lmao having tryna ever out to feel is and ther...   \n",
       "28          0.109630        lmao being fine la in your you feel - proud   \n",
       "29          0.150564                        lmao to la please forever @   \n",
       "..               ...                                                ...   \n",
       "70          0.091021                             yo la new i am yo lmao   \n",
       "71          0.169787                          lmao my friends @ los new   \n",
       "72          0.140618                             los los am long lmao 😍   \n",
       "73          0.122967                              christmas car and you   \n",
       "74          0.103378  lmao i bought a year last year i finally up an...   \n",
       "75          0.144167                                by la this @ los ca   \n",
       "76          0.118291                           lmao la soon today. love   \n",
       "77          0.105224                     lmao la feel you now me lmao @   \n",
       "78          0.169950                        lmao los buy fine tonight 😍   \n",
       "79          0.114512  lmao i feel one is coming no now for that plea...   \n",
       "80          0.115478  lmao was up for this long is gonna los tonight...   \n",
       "81          0.170934           lmao stay open ca to stay \\r\\n \\r\\n \\r\\n   \n",
       "82          0.106519         lmao gonna get la los show open !! tonight   \n",
       "83          0.104902                            lmao la forever ... 🤣 😍   \n",
       "84          0.140469          lmao having new to los the best being a 😍   \n",
       "85          0.164534  la class me plus plus my kids or i’ll stay bc ...   \n",
       "86          0.326499  music new la am it’s the sweet was the new alb...   \n",
       "87          0.099970  lmao women feel with la !! !! no \\r\\n \\r\\n \\r\\...   \n",
       "88          0.288408                lmao w &amp; clear you’re tryna it.   \n",
       "89          0.105178  i’m at my be a beautiful and i was a beautiful...   \n",
       "90          0.532154                           lmao la am going tonight   \n",
       "91          0.112943  san the la la ca ever la bc i’m getting bc i'm...   \n",
       "92          0.162323  ca + me every + ca feel your you feel bc she s...   \n",
       "93          0.141787  2 park &amp; last night it’s 3 days at 4 schoo...   \n",
       "94          0.112127        lmao yo time to los your ca &amp; \\r\\n \\r\\n   \n",
       "95          0.116783                                      lmao los been   \n",
       "96          0.123383                                 yo new los am \\r\\n   \n",
       "97          0.187807                                               lmao   \n",
       "98          0.136425  lmao straight you more is fine ... me ... ca c...   \n",
       "99          0.153813                                    lmao los your 😍   \n",
       "\n",
       "   Decoded Prediction  Decoded Certainty     Score  \n",
       "0         los angeles           0.232524  0.432241  \n",
       "1         los angeles           0.778648  0.159367  \n",
       "2         los angeles           0.922191  0.279969  \n",
       "3         los angeles           0.367551  0.558873  \n",
       "4         los angeles           0.504507  0.460299  \n",
       "5         los angeles           0.934378  0.198028  \n",
       "6         los angeles           0.740494  0.172306  \n",
       "7         los angeles           0.471474  0.262803  \n",
       "8         los angeles           0.555836  0.311067  \n",
       "9         los angeles           0.938229  0.229712  \n",
       "10        los angeles           0.555626  0.391731  \n",
       "11        los angeles           0.732963  0.322521  \n",
       "12        los angeles           0.449519  0.373483  \n",
       "13        los angeles           0.588147  0.344255  \n",
       "14        los angeles           0.586297  0.132044  \n",
       "15        los angeles           0.917277  0.403527  \n",
       "16        los angeles           0.572205  0.466920  \n",
       "17        los angeles           0.824589  0.351394  \n",
       "18        los angeles           0.934510  0.349156  \n",
       "19        los angeles           0.563443  0.182177  \n",
       "20        los angeles           0.928456  0.288169  \n",
       "21        los angeles           0.919048  0.273216  \n",
       "22        los angeles           0.746466  0.248964  \n",
       "23        los angeles           0.756194  0.370771  \n",
       "24        los angeles           0.930502  0.292452  \n",
       "25        los angeles           0.169377  0.128522  \n",
       "26        los angeles           0.243982  0.390481  \n",
       "27        los angeles           0.307981  0.383911  \n",
       "28        los angeles           0.665182  0.446537  \n",
       "29        los angeles           0.717130  0.301976  \n",
       "..                ...                ...       ...  \n",
       "70        los angeles           0.746046  0.327908  \n",
       "71        los angeles           0.917059  0.460392  \n",
       "72        los angeles           0.938456  0.323878  \n",
       "73        los angeles           0.141119  0.470863  \n",
       "74        los angeles           0.444337  0.403117  \n",
       "75        los angeles           0.928891  0.389054  \n",
       "76        los angeles           0.680807  0.219884  \n",
       "77        los angeles           0.711679  0.409815  \n",
       "78        los angeles           0.915816  0.229404  \n",
       "79        los angeles           0.708060  0.513221  \n",
       "80        los angeles           0.969901  0.296223  \n",
       "81        los angeles           0.571916  0.483996  \n",
       "82        los angeles           0.920861  0.236602  \n",
       "83        los angeles           0.781162  0.310206  \n",
       "84        los angeles           0.861736  0.367327  \n",
       "85        los angeles           0.317123  0.066848  \n",
       "86        los angeles           0.678941  0.321176  \n",
       "87        los angeles           0.685141  0.185579  \n",
       "88        los angeles           0.316024  0.158439  \n",
       "89      san francisco           0.538948  0.317066  \n",
       "90        los angeles           0.692821  0.319283  \n",
       "91        los angeles           0.473142  0.119524  \n",
       "92        los angeles           0.613443  0.130017  \n",
       "93        los angeles           0.148797  0.117431  \n",
       "94        los angeles           0.936743  0.322133  \n",
       "95        los angeles           0.855094  0.273075  \n",
       "96        los angeles           0.923803  0.236296  \n",
       "97        los angeles           0.211230  0.166971  \n",
       "98        los angeles           0.864734  0.322859  \n",
       "99        los angeles           0.909075  0.236760  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate success of training to see similarity of input and output sentences\n",
    "\n",
    "decoder_results = {'Source':[], 'Source Prediction':[], 'Source Certainty':[],\n",
    "                   'Decoded':[], 'Decoded Prediction':[], 'Decoded Certainty':[], 'Score': []}\n",
    "\n",
    "score_list = []\n",
    "\n",
    "for _ in range(100):\n",
    "    target = predict_sequence(encoder_model, decoder_model, X_test[[_]], x_length, 10)\n",
    "    score = 1 - spatial.distance.cosine(get_embeddings(X_test[[_]]), get_embeddings(target[6]))\n",
    "    score_list.append(score)\n",
    "    decoder_results['Source'].append(target[0])\n",
    "    decoder_results['Source Prediction'].append(target[1])\n",
    "    decoder_results['Source Certainty'].append(target[2])\n",
    "    decoder_results['Decoded'].append(target[3])\n",
    "    decoder_results['Decoded Prediction'].append(target[4])\n",
    "    decoder_results['Decoded Certainty'].append(target[5])\n",
    "    decoder_results['Score'].append(score)\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(decoder_results)[['Source', 'Source Prediction', 'Source Certainty', \n",
    "                                         'Decoded', 'Decoded Prediction', 'Decoded Certainty',\n",
    "                                         'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28574797861278056"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
