{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "from keras.models import load_model\n",
    "model = load_model('/var/models/twitter_40k_charlevel_lstm_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    #return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "    return x / K.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_category = 5\n",
    "input_txt = model.input\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_name = 'dense_1'\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(model.output[:, target_category])\n",
    "grads = K.gradients(loss, input_txt)[0]\n",
    "grads = normalize(grads)\n",
    "iterate = K.function([input_txt], [loss, grads])\n",
    "step = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tweet = [\"i'm at cassell's burgers in los angeles, ca\"]\n",
    "num_unique_symbols = 500\n",
    "x_length = 200\n",
    "\n",
    "t = text.Tokenizer(\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=True,\n",
    "    num_words=num_unique_symbols-1,\n",
    "    oov_token='unk'\n",
    ")\n",
    "\n",
    "df = pd.read_csv('/var/data/tweets_labelled_40k.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "t.fit_on_texts(X)\n",
    "\n",
    "test_sequence = t.texts_to_sequences(input_tweet)\n",
    "test_padded = sequence.pad_sequences(test_sequence, maxlen=x_length)\n",
    "input_sequence = to_categorical(test_padded, num_classes=num_unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss value: 0.015896862372756004, predicted category: [7], certainty: [0.11233685]\n",
      "Current loss value: 0.09498601406812668, predicted category: [7], certainty: [0.9396867]\n",
      "Current loss value: 0.01995289884507656, predicted category: [5], certainty: [0.31038865]\n",
      "Current loss value: 0.3103886544704437, predicted category: [2], certainty: [0.6970527]\n",
      "Current loss value: 0.008473051711916924, predicted category: [7], certainty: [0.7314852]\n",
      "Current loss value: 0.05209602788090706, predicted category: [7], certainty: [0.11122768]\n",
      "Current loss value: 0.09674761444330215, predicted category: [19], certainty: [0.16448238]\n",
      "Current loss value: 0.09268669784069061, predicted category: [7], certainty: [0.5014479]\n",
      "Current loss value: 0.1546216905117035, predicted category: [10], certainty: [0.41589633]\n",
      "Current loss value: 0.0006312582991085947, predicted category: [7], certainty: [0.24123757]\n",
      "Current loss value: 0.1267969012260437, predicted category: [7], certainty: [0.40363368]\n",
      "Current loss value: 0.19395512342453003, predicted category: [5], certainty: [0.34160176]\n",
      "Current loss value: 0.34160175919532776, predicted category: [15], certainty: [0.32392412]\n",
      "Current loss value: 0.029382066801190376, predicted category: [7], certainty: [0.36466163]\n",
      "Current loss value: 0.13414789736270905, predicted category: [7], certainty: [0.8284443]\n",
      "Current loss value: 0.0681207999587059, predicted category: [20], certainty: [0.50125414]\n",
      "Current loss value: 0.012623137794435024, predicted category: [7], certainty: [0.7231013]\n",
      "Current loss value: 0.05956433340907097, predicted category: [7], certainty: [0.24895936]\n",
      "Current loss value: 0.11842290312051773, predicted category: [7], certainty: [0.23901217]\n",
      "Current loss value: 0.13974760472774506, predicted category: [7], certainty: [0.58815724]\n",
      "Current loss value: 0.16013288497924805, predicted category: [14], certainty: [0.15252466]\n",
      "Current loss value: 0.07396902143955231, predicted category: [7], certainty: [0.20356703]\n",
      "Current loss value: 0.1505589336156845, predicted category: [7], certainty: [0.55960387]\n",
      "Current loss value: 0.2097860723733902, predicted category: [18], certainty: [0.28174987]\n",
      "Current loss value: 0.006384714040905237, predicted category: [7], certainty: [0.15276287]\n",
      "Current loss value: 0.1487164944410324, predicted category: [7], certainty: [0.829681]\n",
      "Current loss value: 0.0599239356815815, predicted category: [10], certainty: [0.5280686]\n",
      "Current loss value: 0.0022787535563111305, predicted category: [7], certainty: [0.15266624]\n",
      "Current loss value: 0.14869581162929535, predicted category: [7], certainty: [0.82869387]\n",
      "Current loss value: 0.06047928333282471, predicted category: [18], certainty: [0.4689763]\n",
      "Current loss value: 0.006098622921854258, predicted category: [7], certainty: [0.15715821]\n",
      "Current loss value: 0.1455407440662384, predicted category: [7], certainty: [0.82897127]\n",
      "Current loss value: 0.062192678451538086, predicted category: [14], certainty: [0.99173224]\n",
      "Current loss value: 0.00016154241166077554, predicted category: [5], certainty: [0.11765108]\n",
      "Current loss value: 0.1176510825753212, predicted category: [7], certainty: [0.9206559]\n",
      "Current loss value: 0.022267356514930725, predicted category: [14], certainty: [0.9956657]\n",
      "Current loss value: 5.843466715305112e-05, predicted category: [14], certainty: [0.1363796]\n",
      "Current loss value: 0.12517137825489044, predicted category: [7], certainty: [0.89959663]\n",
      "Current loss value: 0.015111626125872135, predicted category: [7], certainty: [0.2576117]\n",
      "Current loss value: 0.09314234554767609, predicted category: [7], certainty: [0.18203613]\n",
      "Current loss value: 0.12046804279088974, predicted category: [7], certainty: [0.8494825]\n",
      "Current loss value: 0.06767697632312775, predicted category: [6], certainty: [0.70916647]\n",
      "Current loss value: 0.0019002762855961919, predicted category: [7], certainty: [0.7300718]\n",
      "Current loss value: 0.003627725876867771, predicted category: [14], certainty: [0.16453665]\n",
      "Current loss value: 0.07350336760282516, predicted category: [7], certainty: [0.7459773]\n",
      "Current loss value: 0.05867467448115349, predicted category: [7], certainty: [0.21276195]\n",
      "Current loss value: 0.12265106290578842, predicted category: [7], certainty: [0.53624046]\n",
      "Current loss value: 0.22819404304027557, predicted category: [22], certainty: [0.28409803]\n",
      "Current loss value: 0.08334361761808395, predicted category: [7], certainty: [0.21173395]\n",
      "Current loss value: 0.12352003902196884, predicted category: [5], certainty: [0.3104567]\n"
     ]
    }
   ],
   "source": [
    "output_sequence = input_sequence.copy()\n",
    "for i in range(50):\n",
    "    loss_value, grads_value = iterate([output_sequence])\n",
    "    output_sequence += grads_value * step\n",
    "    if i % 3 == 0:\n",
    "        blank = np.zeros((1, 200, 500))\n",
    "        np.put_along_axis(blank[0], np.expand_dims(np.argmax(output_sequence[0], axis=1), axis=1), 1, axis=1)\n",
    "        output_sequence = blank\n",
    "    probs = model.predict_on_batch(output_sequence)\n",
    "    cat = np.argmax(probs, axis=1)\n",
    "    top_prob = probs[0][cat]\n",
    "                    \n",
    "    print('Current loss value: {}, predicted category: {}, certainty: {}'\n",
    "          .format(loss_value, cat, top_prob))\n",
    "    if loss_value <= 0. or (cat==target_category and top_prob > .9):\n",
    "        # some filters get stuck to 0, we can skip them\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_to_text(tokenizer, embedding):\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()} # map back\n",
    "    embedding = embedding[0]\n",
    "    output = []\n",
    "    for l in range(len(embedding)):\n",
    "        if np.argmax(embedding[l]) > 0:\n",
    "            output.append(index_word[np.argmax(embedding[l])])\n",
    "        else:\n",
    "            continue\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ”—ðŸ”—i'm aâ€™â€™â€™aâ€™sell'â€™ ðŸ™„ðŸ™„ðŸ™„ðŸ™„ðŸ“ðŸ“ðŸ“kâ€™â€™â€™â€™â€™s ðŸ¾nðŸ¾â€™â€™â€™ðŸ¤”ðŸ˜ŽðŸŽ„â€ðŸ™‚\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = [embedding_to_text(t, output_sequence)]\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence = t.texts_to_sequences(decode)\n",
    "decode_padded = sequence.pad_sequences(decode_sequence, maxlen=x_length)\n",
    "decode_onehot = to_categorical(decode_padded, num_classes=num_unique_symbols)\n",
    "decode_prediction_probs = model.predict_on_batch(decode_onehot)\n",
    "np.argmax(decode_prediction_probs, axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42322648], dtype=float32)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_on_batch(output_sequence)\n",
    "probs[0][np.argmax(probs, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸ™„ðŸ™„ðŸ™„â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™ðŸ¾ðŸ¾ðŸ˜˜ðŸ˜˜ðŸ’–\\U0001f929ðŸ™ŒðŸ˜˜ðŸ˜˜ðŸ™„ðŸ™„ðŸ¾ðŸ¾ðŸ™„ðŸ‡¸ðŸ¦‹)ðŸ’«ðŸ˜ˆâ€™â€™ðŸŽƒðŸ˜˜â€™ðŸ™„byðŸ™„p. âœŒðŸ˜˜ðŸ˜¤xz,ðŸŽƒ 'â€™â€¼$ ðŸ™‚\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_to_text(t, decode_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(decode_onehot[0][199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_region(model, x_length, num_symbols, tokenizer, string):\n",
    "    decode_sequence = tokenizer.texts_to_sequences(string)\n",
    "    decode_padded = sequence.pad_sequences(decode_sequence, maxlen=x_length)\n",
    "    decode_onehot = to_categorical(decode_padded, num_classes=num_unique_symbols)\n",
    "    decode_prediction_probs = model.predict_on_batch(decode_onehot)\n",
    "    region = np.argmax(decode_prediction_probs, axis=1)[0]\n",
    "    return (region, decode_prediction_probs[0][region]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def identify_regional_substring(string, length=None):\n",
    "    target_region = predict_region(model, x_length, num_unique_symbols, t, [string])[0]\n",
    "    best = [0, '']\n",
    "    text_list = text.split()\n",
    "    length = len(text_list)-1 if length is None else length+1\n",
    "    for w in range(1, length):\n",
    "        for snap in range(len(text_list)-w+1):\n",
    "            search_string = ' '.join(text_list[snap:snap+w])\n",
    "            search_response = predict_region(model, x_length, num_unique_symbols, t, [search_string])\n",
    "            if search_response[0] == target_region and search_response[1] > best[0]:\n",
    "                best = [search_response[1], search_string]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8441567, 'pembroke pines, florida']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text = \"the most beautiful belle and our littlest pumpkin had so much fun for halloween! #chocolateoverload #trickortreat #myfirsthalloween #beautyandthebeast @ pembroke pines, florida\"\n",
    "identify_regional_substring(predict_text, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 0.13844642)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text = \"ðŸ¦‹ðŸ¦‹ðŸ¦‹=â€™â€™'e'eðŸŽ„ðŸŽ„ðŸŽ„ðŸŽ„)\"\n",
    "predict_region(model, x_length, num_unique_symbols, t, [predict_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(output_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_embedding(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = np.zeros((1, 200, 500))\n",
    "np.put_along_axis(blank[0], np.expand_dims(np.argmax(output_sequence[0], axis=1), axis=1), 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = \n",
    "np.put_along_axis(blank[0], np.expand_dims(np.argmax(output_sequence[0], axis=1), axis=1), 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel",
   "language": "python",
   "name": "daniel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
