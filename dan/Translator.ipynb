{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "from keras.models import load_model\n",
    "model = load_model('/var/models/twitter_40k_charlevel_lstm_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    #return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "    return x / K.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_category = 5\n",
    "input_txt = model.input\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_name = 'dense_1'\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(model.output[:, target_category])\n",
    "grads = K.gradients(loss, input_txt)[0]\n",
    "grads = normalize(grads)\n",
    "iterate = K.function([input_txt], [loss, grads])\n",
    "step = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tweet = [\"i'm at cassell's burgers in los angeles, ca\"]\n",
    "input_tweet = [\"i'm going to los angeles, ca to have a burger\"]\n",
    "\n",
    "num_unique_symbols = 500\n",
    "x_length = 200\n",
    "\n",
    "t = text.Tokenizer(\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=True,\n",
    "    num_words=num_unique_symbols-1,\n",
    "    oov_token='unk'\n",
    ")\n",
    "\n",
    "df = pd.read_csv('/var/data/tweets_labelled_40k.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "t.fit_on_texts(X)\n",
    "\n",
    "test_sequence = t.texts_to_sequences(input_tweet)\n",
    "test_padded = sequence.pad_sequences(test_sequence, maxlen=x_length)\n",
    "input_sequence = to_categorical(test_padded, num_classes=num_unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss value: 0.015896862372756004, predicted category: [20], certainty: [0.3520429]\n",
      "Current loss value: 0.13307678699493408, predicted category: [7], certainty: [0.6156763]\n",
      "Current loss value: 0.08834603428840637, predicted category: [7], certainty: [0.19319843]\n",
      "Current loss value: 0.1274987906217575, predicted category: [7], certainty: [0.41469833]\n",
      "Current loss value: 0.20774774253368378, predicted category: [5], certainty: [0.2806793]\n",
      "Current loss value: 0.28067928552627563, predicted category: [7], certainty: [0.33218554]\n",
      "Current loss value: 0.31690648198127747, predicted category: [5], certainty: [0.35932237]\n",
      "Current loss value: 0.35932236909866333, predicted category: [4], certainty: [0.2646271]\n",
      "Current loss value: 0.11231055855751038, predicted category: [5], certainty: [0.33198428]\n",
      "Current loss value: 0.331984281539917, predicted category: [5], certainty: [0.4844394]\n",
      "Current loss value: 0.4844394028186798, predicted category: [5], certainty: [0.5425707]\n",
      "Current loss value: 0.5425707101821899, predicted category: [5], certainty: [0.65615565]\n",
      "Current loss value: 0.6561556458473206, predicted category: [5], certainty: [0.69937104]\n",
      "Current loss value: 0.6993710398674011, predicted category: [5], certainty: [0.646359]\n",
      "Current loss value: 0.6463590264320374, predicted category: [5], certainty: [0.74401814]\n",
      "Current loss value: 0.7440181374549866, predicted category: [5], certainty: [0.68378973]\n",
      "Current loss value: 0.6837897300720215, predicted category: [5], certainty: [0.593287]\n",
      "Current loss value: 0.5932869911193848, predicted category: [5], certainty: [0.8380286]\n",
      "Current loss value: 0.838028609752655, predicted category: [5], certainty: [0.84527713]\n",
      "Current loss value: 0.8452771306037903, predicted category: [5], certainty: [0.8988584]\n",
      "Current loss value: 0.8988584280014038, predicted category: [5], certainty: [0.93679065]\n"
     ]
    }
   ],
   "source": [
    "output_sequence = input_sequence.copy()\n",
    "for i in range(50):\n",
    "    loss_value, grads_value = iterate([output_sequence])\n",
    "    output_sequence += grads_value * step\n",
    "    probs = model.predict_on_batch(output_sequence)\n",
    "    cat = np.argmax(probs, axis=1)\n",
    "    top_prob = probs[0][cat]\n",
    "                    \n",
    "    print('Current loss value: {}, predicted category: {}, certainty: {}'\n",
    "          .format(loss_value, cat, top_prob))\n",
    "    if loss_value <= 0. or (cat==target_category and top_prob > .9):\n",
    "        # some filters get stuck to 0, we can skip them\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_to_text(tokenizer, embedding):\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()} # map back\n",
    "    embedding = embedding[0]\n",
    "    output = []\n",
    "    for l in range(len(embedding)):\n",
    "        if np.argmax(embedding[l]) > 0:\n",
    "            output.append(index_word[np.argmax(embedding[l])])\n",
    "        else:\n",
    "            continue\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"%%ðŸ¦ƒðŸ¦ƒi'm at cassell's burgersâ€™iâ€™â€™â€™â€™s â€™â€™ge(eðŸŽ„ðŸŽ„ðŸŽ„2ðŸ»\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = [embedding_to_text(t, output_sequence)]\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence = t.texts_to_sequences(decode)\n",
    "decode_padded = sequence.pad_sequences(decode_sequence, maxlen=x_length)\n",
    "decode_onehot = to_categorical(decode_padded, num_classes=num_unique_symbols)\n",
    "decode_prediction_probs = model.predict_on_batch(decode_onehot)\n",
    "np.argmax(decode_prediction_probs, axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42322648], dtype=float32)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_on_batch(output_sequence)\n",
    "probs[0][np.argmax(probs, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸ™„ðŸ™„ðŸ™„â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™â€™ðŸ¾ðŸ¾ðŸ˜˜ðŸ˜˜ðŸ’–\\U0001f929ðŸ™ŒðŸ˜˜ðŸ˜˜ðŸ™„ðŸ™„ðŸ¾ðŸ¾ðŸ™„ðŸ‡¸ðŸ¦‹)ðŸ’«ðŸ˜ˆâ€™â€™ðŸŽƒðŸ˜˜â€™ðŸ™„byðŸ™„p. âœŒðŸ˜˜ðŸ˜¤xz,ðŸŽƒ 'â€™â€¼$ ðŸ™‚\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_to_text(t, decode_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 0.35342124)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_region(model, x_length, num_unique_symbols, t, input_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_region(model, x_length, num_symbols, tokenizer, string):\n",
    "    decode_sequence = tokenizer.texts_to_sequences(string)\n",
    "    decode_padded = sequence.pad_sequences(decode_sequence, maxlen=x_length)\n",
    "    decode_onehot = to_categorical(decode_padded, num_classes=num_unique_symbols)\n",
    "    decode_prediction_probs = model.predict_on_batch(decode_onehot)\n",
    "    region = np.argmax(decode_prediction_probs, axis=1)[0]\n",
    "    return (region, decode_prediction_probs[0][region]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def identify_regional_substring(string, length=None):\n",
    "    target_region = predict_region(model, x_length, num_unique_symbols, t, [string])[0]\n",
    "    best = [0, '']\n",
    "    text_list = text.split()\n",
    "    length = len(text_list)-1 if length is None else length+1\n",
    "    for w in range(1, length):\n",
    "        for snap in range(len(text_list)-w+1):\n",
    "            search_string = ' '.join(text_list[snap:snap+w])\n",
    "            search_response = predict_region(model, x_length, num_unique_symbols, t, [search_string])\n",
    "            if search_response[0] == target_region and search_response[1] > best[0]:\n",
    "                best = [search_response[1], search_string]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8441567, 'pembroke pines, florida']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text = \"the most beautiful belle and our littlest pumpkin had so much fun for halloween! #chocolateoverload #trickortreat #myfirsthalloween #beautyandthebeast @ pembroke pines, florida\"\n",
    "identify_regional_substring(input, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 0.13844642)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text = \"ðŸ¦‹ðŸ¦‹ðŸ¦‹=â€™â€™'e'eðŸŽ„ðŸŽ„ðŸŽ„ðŸŽ„)\"\n",
    "predict_region(model, x_length, num_unique_symbols, t, [predict_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(output_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_embedding(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = np.zeros((1, 200, 500))\n",
    "np.put_along_axis(blank[0], np.expand_dims(np.argmax(output_sequence[0], axis=1), axis=1), 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = \n",
    "np.put_along_axis(blank[0], np.expand_dims(np.argmax(output_sequence[0], axis=1), axis=1), 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel",
   "language": "python",
   "name": "daniel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
