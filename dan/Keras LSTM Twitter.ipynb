{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "import datetime\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "region      \n",
       "0        517\n",
       "1        176\n",
       "2         44\n",
       "3       2387\n",
       "4       2202\n",
       "5       1726\n",
       "6        624\n",
       "7       3760\n",
       "8       1034\n",
       "9        699\n",
       "10      3606\n",
       "11       723\n",
       "12       509\n",
       "13      2587\n",
       "14      5221\n",
       "15      1959\n",
       "16       761\n",
       "17      1441\n",
       "18      2018\n",
       "19      1513\n",
       "20      2622\n",
       "21      1510\n",
       "22      2361"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/var/data/tweets_labelled_40k.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "y = df['region'].tolist()\n",
    "df_counts = df.groupby('region').count()\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "V = 50000\n",
    "x_length = 50\n",
    "training_ratio = .75\n",
    "training_size = int(len(X)*training_ratio)\n",
    "num_classes = 23\n",
    "embedding_vector_length = 200\n",
    "num_layers = 2\n",
    "H = 200\n",
    "epochs = 100\n",
    "optimizer = 'rmsprop'\n",
    "batch_size = 128\n",
    "learning_rate = .001\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('Twitter 40k word-level')\n",
    "mlflow.start_run()\n",
    "mlflow.log_param('learning_rate', learning_rate)\n",
    "mlflow.log_param('vocabulary', V)\n",
    "mlflow.log_param('number_of_layers', num_layers)\n",
    "mlflow.log_param('x_length', x_length)\n",
    "mlflow.log_param('embedding_vector', embedding_vector_length)\n",
    "mlflow.log_param('H', H)\n",
    "mlflow.log_param('optimizer', optimizer)\n",
    "mlflow.log_param('dropout', dropout)\n",
    "mlflow.log_param('epochs', epochs)\n",
    "mlflow.log_param('batch_size', batch_size)\n",
    "mlflow.log_param('train_size', training_size)\n",
    "mlflow.log_param('test_size', len(y)-training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 30000 examples, test set has 10000 examples\n"
     ]
    }
   ],
   "source": [
    "# Convert text to integer indices, separate test and training sets\n",
    "t = text.Tokenizer(num_words=V, lower=True)\n",
    "t.fit_on_texts(X)\n",
    "X_seq = t.texts_to_sequences(X)\n",
    "X_pad = sequence.pad_sequences(X_seq, maxlen=x_length)\n",
    "X_train = X_pad[:training_size]\n",
    "X_test = X_pad[training_size:]\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "one_hot_y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "one_hot_y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "print(\"Training set has {} examples, test set has {} examples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 200)           10000000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 10,646,223\n",
      "Trainable params: 10,646,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 392s 13ms/step - loss: 2.8566 - acc: 0.1355 - val_loss: 2.8368 - val_acc: 0.1433\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 406s 14ms/step - loss: 2.6151 - acc: 0.2124 - val_loss: 2.8259 - val_acc: 0.1537\n",
      "Epoch 3/100\n",
      "16000/30000 [===============>..............] - ETA: 2:52 - loss: 2.3129 - acc: 0.3074"
     ]
    }
   ],
   "source": [
    "# Build and run the model\n",
    "opt = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "start_time = time.time()\n",
    "model = Sequential()\n",
    "model.add(Embedding(V, embedding_vector_length, input_length=x_length))\n",
    "if num_layers > 1:\n",
    "    for m in range(num_layers-1):\n",
    "        model.add(LSTM(H, return_sequences=True))\n",
    "    model.add(LSTM(H))\n",
    "else:\n",
    "    model.add(LSTM(H))\n",
    "\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='/var/models/twitter_40k_wordlevel_lstm_chk.h5', monitor='val_loss', save_best_only=True)]\n",
    "model.fit(X_train, one_hot_y_train, epochs=epochs, callbacks=callbacks, batch_size=batch_size,\n",
    "          validation_data=(X_test, one_hot_y_test))\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, one_hot_y_test, verbose=0)\n",
    "end_time = time.time()\n",
    "run_time = datetime.timedelta(seconds=end_time-start_time)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'twitter_40k_wordlevel_lstm'\n",
    "mlflow.log_param('model_name', model_name)\n",
    "mlflow.log_param('notes', 'None')\n",
    "mlflow.log_param('run_time', run_time)\n",
    "mlflow.log_metric('accuracy', scores[1]*100)\n",
    "mlflow.end_run()\n",
    "model.save('/var/models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  7, 14,  7, 14,  7, 14, 14,  7,  7,  7, 14, 14, 14,  7,  7, 14,\n",
       "        7, 14,  7, 14,  7,  7,  7, 14,  7,  7,  7,  7,  7, 10,  7,  7,  7,\n",
       "        7, 14,  7,  7, 14,  7,  7, 14,  7, 14, 14, 14, 14,  7,  7,  7, 14,\n",
       "       14,  7, 14,  7,  7, 14,  7, 14, 14, 14,  7, 14, 14, 14,  7, 14, 14,\n",
       "       14, 14,  7, 14,  7, 14, 14, 14,  7, 14,  7,  7, 14, 14,  7, 14,  7,\n",
       "       14,  7, 14,  7, 14, 14,  7, 14,  7, 14,  7, 14, 14, 14,  7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "test = [\"Man I love a good bowl of borscht\",\n",
    "       \"Did anyone see the Packers game last night?\",\n",
    "       \"Did anyone see the Raiders game last night?\",\n",
    "       \"Football is da bomb!\",\n",
    "       \"Nothing like a poutine and some beer with the game\",\n",
    "       \"The Bulls suck this year\",\n",
    "       \"Bitch better have my money\",\n",
    "       \"Dude that's hella tight\",\n",
    "       \"The metro/non-metro split nationalized: it is, at this point essentially the politics of every state. The metro areas in Texas moved almost as sharply away from the Trump-led GOP as they did in PA, NJ or MN. Trump only won 13/100 largest US counties and 1/2 of those moved D\",\n",
    "       \"The people who are going to pay for Jeff Bezoâ€™s helipad in NYC\"]\n",
    "\n",
    "#\n",
    "Xt = random.sample(X[training_size:], 100)\n",
    "\n",
    "t_test = text.Tokenizer(num_words=V, lower=True)\n",
    "\n",
    "t_test.fit_on_texts(Xt)\n",
    "sequences = t_test.texts_to_sequences(Xt)\n",
    "test_padded = sequence.pad_sequences(sequences, maxlen=pad_size)\n",
    "predictions = model.predict_on_batch(test_padded)\n",
    "np.argmax(predictions, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel",
   "language": "python",
   "name": "daniel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
