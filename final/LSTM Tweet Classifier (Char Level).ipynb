{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import mlflow\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "import datetime\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "region      \n",
       "0        517\n",
       "1        176\n",
       "2         44\n",
       "3       2387\n",
       "4       2202\n",
       "5       1726\n",
       "6        624\n",
       "7       3760\n",
       "8       1034\n",
       "9        699\n",
       "10      3606\n",
       "11       723\n",
       "12       509\n",
       "13      2587\n",
       "14      5221\n",
       "15      1959\n",
       "16       761\n",
       "17      1441\n",
       "18      2018\n",
       "19      1513\n",
       "20      2622\n",
       "21      1510\n",
       "22      2361"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/var/data/tweets_labelled_40k.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "y = df['region'].tolist()\n",
    "df_counts = df.groupby('region').count()\n",
    "top_category_num = max(df_counts['text'])\n",
    "top_category_name = df_counts[df_counts['text']==max(df_counts['text'])].index[0]\n",
    "categories = df_counts.index.tolist()\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  If we just guessed '14' every time we would have accuracy of 13.05%\n"
     ]
    }
   ],
   "source": [
    "# Dumb Baseline\n",
    "print(\"Baseline accuracy:  If we just guessed '{}' every time we would have accuracy of {:.2f}%\"\n",
    "      .format(top_category_name, (top_category_num/df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.619408"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get average length of each tweet\n",
    "df['text'].apply(lambda x:len(x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "x_length = 200\n",
    "training_ratio = .75\n",
    "training_size = int(len(X)*training_ratio)\n",
    "num_classes = 23\n",
    "embedding_vector_length = 100\n",
    "num_unique_symbols = 500\n",
    "num_layers = 3\n",
    "H = 200\n",
    "epochs = 100\n",
    "optimizer = 'rmsprop'\n",
    "batch_size = 128\n",
    "learning_rate = .0001\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ML Flow parameters and start the run\n",
    "mlflow.set_experiment('Twitter 40k v2')\n",
    "mlflow.start_run()\n",
    "mlflow.log_param('learning_rate', learning_rate)\n",
    "mlflow.log_param('num_unique_symbols', num_unique_symbols)\n",
    "mlflow.log_param('number_of_layers', num_layers)\n",
    "mlflow.log_param('x_length', x_length)\n",
    "mlflow.log_param('embedding_vector', embedding_vector_length)\n",
    "mlflow.log_param('H', H)\n",
    "mlflow.log_param('optimizer', optimizer)\n",
    "mlflow.log_param('dropout', dropout)\n",
    "mlflow.log_param('epochs', epochs)\n",
    "mlflow.log_param('batch_size', batch_size)\n",
    "mlflow.log_param('train_size', training_size)\n",
    "mlflow.log_param('test_size', len(y)-training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 30000 examples, test set has 10000 examples\n"
     ]
    }
   ],
   "source": [
    "t = text.Tokenizer(\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=True,\n",
    "    num_words=num_unique_symbols-1,\n",
    "    oov_token='unk'\n",
    ")\n",
    "\n",
    "t.fit_on_texts(X)\n",
    "X_seq = t.texts_to_sequences(X)\n",
    "X_padded = sequence.pad_sequences(X_seq, maxlen=x_length)\n",
    "X_train = X_padded[:training_size]\n",
    "X_test = X_padded[training_size:]\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "one_hot_y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "one_hot_y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "#one_hot_x_train = to_categorical(X_train, num_classes=num_unique_symbols)\n",
    "\n",
    "print(\"Training set has {} examples, test set has {} examples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class OneHotBatch(Sequence):\n",
    "  def __init__(self, X_data, y_data, batch_size, num_chars, num_classes):\n",
    "    self.X_data = X_data\n",
    "    self.y_data = y_data\n",
    "    self.batch_size = batch_size\n",
    "    self.num_chars = num_chars\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def __len__(self):\n",
    "     return int(np.ceil(len(self.X_data) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, batch_id):\n",
    "    start = batch_id * self.batch_size\n",
    "    finish = start + self.batch_size\n",
    "    X = to_categorical(self.X_data[start:finish], num_classes=self.num_chars)\n",
    "    y = to_categorical(self.y_data[start:finish], num_classes=self.num_classes)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200, 200)          560800    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 200, 200)          320800    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 1,207,023\n",
      "Trainable params: 1,207,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "234/235 [============================>.] - ETA: 2s - loss: 2.8954 - acc: 0.1258Epoch 1/100\n",
      "Epoch 1/100\n",
      "235/235 [==============================] - 596s 3s/step - loss: 2.8952 - acc: 0.1260 - val_loss: 2.9169 - val_acc: 0.0969\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 595s 3s/step - loss: 2.8740 - acc: 0.1291 - val_loss: 2.8868 - val_acc: 0.1123\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 597s 3s/step - loss: 2.8642 - acc: 0.1340 - val_loss: 2.8746 - val_acc: 0.1332\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 589s 3s/step - loss: 2.8558 - acc: 0.1372 - val_loss: 2.8696 - val_acc: 0.1356\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 588s 3s/step - loss: 2.8419 - acc: 0.1381 - val_loss: 2.8647 - val_acc: 0.1408\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 585s 2s/step - loss: 2.8289 - acc: 0.1437 - val_loss: 2.8586 - val_acc: 0.1429\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 587s 2s/step - loss: 2.8158 - acc: 0.1473 - val_loss: 2.8562 - val_acc: 0.1436\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 588s 3s/step - loss: 2.7985 - acc: 0.1536 - val_loss: 2.8469 - val_acc: 0.1469\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 586s 2s/step - loss: 2.7808 - acc: 0.1584 - val_loss: 2.8526 - val_acc: 0.1470\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 586s 2s/step - loss: 2.7759 - acc: 0.1627 - val_loss: 2.8279 - val_acc: 0.1490\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 587s 2s/step - loss: 2.7390 - acc: 0.1725 - val_loss: 2.8400 - val_acc: 0.1449\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 587s 2s/step - loss: 2.7128 - acc: 0.1798 - val_loss: 2.8299 - val_acc: 0.1537\n",
      "79/79 [==============================] - 69s 877ms/step\n",
      "Accuracy: 15.37%\n"
     ]
    }
   ],
   "source": [
    "# Generators\n",
    "train_generator = OneHotBatch(X_train, y_train, batch_size=batch_size, num_chars=num_unique_symbols, num_classes=num_classes)\n",
    "validation_generator = OneHotBatch(X_test, y_test, batch_size=batch_size, num_chars=num_unique_symbols, num_classes=num_classes)\n",
    "\n",
    "# Build and run the model using an Embedding\n",
    "start_time = time.time()\n",
    "model = Sequential()\n",
    "if num_layers > 1:\n",
    "    model.add(LSTM(H, return_sequences=True, input_shape=(x_length, num_unique_symbols)))\n",
    "    for m in range(1, num_layers-1):\n",
    "        model.add(LSTM(H, return_sequences=True))\n",
    "    model.add(LSTM(H))\n",
    "else:\n",
    "    model.add(LSTM(H, input_shape=(x_length, num_unique_symbols)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='/var/models/twitter_40k_charlevel_lstm_onehot_chk.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "history = model.fit_generator(generator=train_generator, epochs=epochs, callbacks=callbacks, \n",
    "                              validation_data=validation_generator, max_queue_size=10,\n",
    "                              workers=5, use_multiprocessing=True)\n",
    "# Final evaluation of the model\n",
    "end_time = time.time()\n",
    "run_time = datetime.timedelta(seconds=end_time-start_time)\n",
    "scores = model.evaluate_generator(generator=validation_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'twitter_40k_charlevel_lstm_onehot_3layer'\n",
    "mlflow.log_param('model_name', model_name)\n",
    "mlflow.log_param('notes', 'No multiprocessing or shuffle')\n",
    "mlflow.log_param('run_time', run_time)\n",
    "mlflow.log_metric('accuracy', scores[1]*100)\n",
    "mlflow.end_run()\n",
    "model.save('/var/models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "from keras.models import load_model\n",
    "model = load_model('/var/models/twitter_40k_charlevel_lstm_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweet = [\"i'm at cassell’s burgers in los angeles, ca\"]\n",
    "\n",
    "test_sequence = t.texts_to_sequences(sample_tweet)\n",
    "test_padded = sequence.pad_sequences(test_sequence, maxlen=x_length)\n",
    "test_onehot = to_categorical(test_padded, num_classes=num_unique_symbols)\n",
    "test_prediction_probs = model.predict_on_batch(test_onehot)\n",
    "np.argmax(test_prediction_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-541c5277c941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_padded' is not defined"
     ]
    }
   ],
   "source": [
    "test_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tweets = X[training_size:]\n",
    "\n",
    "Xt = X_test\n",
    "Xt_onehot = to_categorical(Xt, num_classes=num_unique_symbols)\n",
    "prediction_probs = model.predict_on_batch(Xt_onehot)\n",
    "predictions = np.argmax(prediction_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_regions = np.unique(predictions).tolist()\n",
    "predicted_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm at cassell’s burgers in los angeles, ca\n"
     ]
    }
   ],
   "source": [
    "def sequence_to_text(tokenizer, array):\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()} # map back\n",
    "    return_tweet = []\n",
    "    for i in array:\n",
    "        if i != 0:\n",
    "            return_tweet.append(index_word[i])\n",
    "    return ''.join(return_tweet)\n",
    "\n",
    "print(sequence_to_text(t, test_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[training_size:training_size+100][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each predicted region, find the tweet that the model is MOST confident belongs\n",
    "regions = [\"albuquerque\", \"billings\", \"calgary\", \"charlotte\", \"chicago\", \"cincinnati\", \"denver\", \"houston\", \"kansas city\",\n",
    "           \"las vegas\", \"los angeles\", \"minneapolis\", \"montreal\", \"nashville\", \"new york\", \"oklahoma city\", \"phoenix\",\n",
    "           \"pittsburgh\", \"san francisco\", \"seattle\", \"tampa\", \"toronto\", \"washington\"]\n",
    "best_tweets = dict()\n",
    "\n",
    "for region in predicted_regions:\n",
    "    best_tweets[regions[region]] = {'tweet': '', 'prob': 0, 'index': 0}\n",
    "\n",
    "for i in range(len(prediction_probs)):\n",
    "    top_region_int = np.argmax(prediction_probs[i])\n",
    "    top_region = regions[top_region_int]\n",
    "    top_score = prediction_probs[i][top_region_int]\n",
    "    if top_score > best_tweets[top_region]['prob']:\n",
    "        best_tweets[top_region]['prob'] = round(100*top_score, 2)\n",
    "        best_tweets[top_region]['tweet'] = sequence_to_text(t, Xt[i])\n",
    "        best_tweets[top_region]['index'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prob</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>albuquerque</th>\n",
       "      <td>126</td>\n",
       "      <td>87.64</td>\n",
       "      <td>this #job might be a great fit for you: barista/café server - temporary -  #barista #lascruces, nm #hiring #careerarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>billings</th>\n",
       "      <td>257</td>\n",
       "      <td>52.42</td>\n",
       "      <td>we're #hiring! click to apply: travel labor and delivery registered nurse -  #nursing #polson, mt #job #jobs #careerarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charlotte</th>\n",
       "      <td>47</td>\n",
       "      <td>13.23</td>\n",
       "      <td>i can do better, i gotta stick to the plan 💯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicago</th>\n",
       "      <td>0</td>\n",
       "      <td>14.92</td>\n",
       "      <td>i'm sure you'll think this is funny. hilarious. i am still dying here laughing. and no offense to stevie wonder; he would laugh... #mirth #chuckle #smile #hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cincinnati</th>\n",
       "      <td>696</td>\n",
       "      <td>7.4</td>\n",
       "      <td>brockton goes up 2-1 with a 25-21 win in the third set. rocketeers need to win then next set to keep their season alive. #hockomock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denver</th>\n",
       "      <td>40</td>\n",
       "      <td>98.07</td>\n",
       "      <td>see our latest #fortlupton, co #job and click to apply: contract pharmacist -  #pharmaceutical #hiring #careerarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>houston</th>\n",
       "      <td>1</td>\n",
       "      <td>11.83</td>\n",
       "      <td>coumting down the days till i see my brother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas city</th>\n",
       "      <td>41</td>\n",
       "      <td>28.48</td>\n",
       "      <td>this #job might be a great fit for you: merchandiser intern -  #omaha, ne #hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>las vegas</th>\n",
       "      <td>23</td>\n",
       "      <td>13.8</td>\n",
       "      <td>students using #teamwork building houses out of cards!  #workready2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>los angeles</th>\n",
       "      <td>4</td>\n",
       "      <td>84.47</td>\n",
       "      <td>i'm at cassell’s burgers in los angeles, ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minneapolis</th>\n",
       "      <td>3998</td>\n",
       "      <td>21.53</td>\n",
       "      <td>stuck at the airport and ready to lose my mind!  roger stone emails, trump is a racist and the caravan invasion isn't real. 😡😡😡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>montreal</th>\n",
       "      <td>253</td>\n",
       "      <td>18.58</td>\n",
       "      <td>seahawks fumble in own end zone and recover but that still gives hartsville points, 7-0 red foxes ,5:23 1q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nashville</th>\n",
       "      <td>50</td>\n",
       "      <td>22.76</td>\n",
       "      <td>hmu if y’all need plans for tomorrow night👀‼️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york</th>\n",
       "      <td>2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>bruh. tekashi and brother nature veneers are trash.  should have gone to gucci’s and young thug’s dentist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oklahoma city</th>\n",
       "      <td>10</td>\n",
       "      <td>15.69</td>\n",
       "      <td>december baby like you 😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phoenix</th>\n",
       "      <td>71</td>\n",
       "      <td>93.13</td>\n",
       "      <td>how is this even a thing? why?? @ sierra vista, arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pittsburgh</th>\n",
       "      <td>282</td>\n",
       "      <td>28.89</td>\n",
       "      <td>another unemployed racist, 😬😢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san francisco</th>\n",
       "      <td>46</td>\n",
       "      <td>73.97</td>\n",
       "      <td>this #job might be a great fit for you: speech-language pathologist -  #slp #slpeeps #vacaville, ca #hiring #careerarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>98</td>\n",
       "      <td>98.44</td>\n",
       "      <td>see our latest #kennewick, wa #job and click to apply: mortgage consultant (safe) -  #sales #hiring #careerarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tampa</th>\n",
       "      <td>19</td>\n",
       "      <td>23.41</td>\n",
       "      <td>love the article!!! 👍👏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toronto</th>\n",
       "      <td>567</td>\n",
       "      <td>98.94</td>\n",
       "      <td>goodbye, until next time.... @ toronto, ontario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington</th>\n",
       "      <td>32</td>\n",
       "      <td>65.96</td>\n",
       "      <td>i'm at  in washington, dc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index   prob  \\\n",
       "albuquerque     126  87.64   \n",
       "billings        257  52.42   \n",
       "charlotte        47  13.23   \n",
       "chicago           0  14.92   \n",
       "cincinnati      696    7.4   \n",
       "denver           40  98.07   \n",
       "houston           1  11.83   \n",
       "kansas city      41  28.48   \n",
       "las vegas        23   13.8   \n",
       "los angeles       4  84.47   \n",
       "minneapolis    3998  21.53   \n",
       "montreal        253  18.58   \n",
       "nashville        50  22.76   \n",
       "new york          2   11.9   \n",
       "oklahoma city    10  15.69   \n",
       "phoenix          71  93.13   \n",
       "pittsburgh      282  28.89   \n",
       "san francisco    46  73.97   \n",
       "seattle          98  98.44   \n",
       "tampa            19  23.41   \n",
       "toronto         567  98.94   \n",
       "washington       32  65.96   \n",
       "\n",
       "                                                                                                                                                                               tweet  \n",
       "albuquerque                                                    this #job might be a great fit for you: barista/café server - temporary -  #barista #lascruces, nm #hiring #careerarc  \n",
       "billings                                                     we're #hiring! click to apply: travel labor and delivery registered nurse -  #nursing #polson, mt #job #jobs #careerarc  \n",
       "charlotte                                                                                                                               i can do better, i gotta stick to the plan 💯  \n",
       "chicago        i'm sure you'll think this is funny. hilarious. i am still dying here laughing. and no offense to stevie wonder; he would laugh... #mirth #chuckle #smile #hilarious   \n",
       "cincinnati                                       brockton goes up 2-1 with a 25-21 win in the third set. rocketeers need to win then next set to keep their season alive. #hockomock  \n",
       "denver                                                             see our latest #fortlupton, co #job and click to apply: contract pharmacist -  #pharmaceutical #hiring #careerarc  \n",
       "houston                                                                                                                                 coumting down the days till i see my brother  \n",
       "kansas city                                                                                        this #job might be a great fit for you: merchandiser intern -  #omaha, ne #hiring  \n",
       "las vegas                                                                                                   students using #teamwork building houses out of cards!  #workready2018    \n",
       "los angeles                                                                                                                             i'm at cassell’s burgers in los angeles, ca   \n",
       "minneapolis                                         stuck at the airport and ready to lose my mind!  roger stone emails, trump is a racist and the caravan invasion isn't real. 😡😡😡   \n",
       "montreal                                                                  seahawks fumble in own end zone and recover but that still gives hartsville points, 7-0 red foxes ,5:23 1q  \n",
       "nashville                                                                                                                              hmu if y’all need plans for tomorrow night👀‼️  \n",
       "new york                                                                  bruh. tekashi and brother nature veneers are trash.  should have gone to gucci’s and young thug’s dentist.  \n",
       "oklahoma city                                                                                                                                               december baby like you 😍  \n",
       "phoenix                                                                                                                     how is this even a thing? why?? @ sierra vista, arizona   \n",
       "pittsburgh                                                                                                                                             another unemployed racist, 😬😢  \n",
       "san francisco                                                 this #job might be a great fit for you: speech-language pathologist -  #slp #slpeeps #vacaville, ca #hiring #careerarc  \n",
       "seattle                                                               see our latest #kennewick, wa #job and click to apply: mortgage consultant (safe) -  #sales #hiring #careerarc  \n",
       "tampa                                                                                                                                                         love the article!!! 👍👏  \n",
       "toronto                                                                                                                             goodbye, until next time.... @ toronto, ontario   \n",
       "washington                                                                                                                                                i'm at  in washington, dc   "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df_toptweets = pd.DataFrame.from_dict(best_tweets).T\n",
    "df_toptweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_mapping = {\n",
    "    \"albuquerque\":0,\n",
    "    \"billings\":1,\n",
    "    \"calgary\":2,\n",
    "    \"charlotte\":3,\n",
    "    \"chicago\":4,\n",
    "    \"cincinnati\":5,\n",
    "    \"denver\":6,\n",
    "    \"houston\":7,\n",
    "    \"kansas city\":8,\n",
    "    \"las vegas\":9,\n",
    "    \"los angeles\":10,\n",
    "    \"minneapolis\":11,\n",
    "    \"montreal\":12,\n",
    "    \"nashville\":13,\n",
    "    \"new york\":14,\n",
    "    \"oklahoma city\":15,\n",
    "    \"phoenix\":16,\n",
    "    \"pittsburgh\":17,\n",
    "    \"san francisco\":18,\n",
    "    \"seattle\":19,\n",
    "    \"tampa\":20,\n",
    "    \"toronto\":21,\n",
    "    \"washington\":22\n",
    "}\n",
    "\n",
    "regions = [\"albuquerque\", \"billings\", \"calgary\", \"charlotte\", \"chicago\", \"cincinnati\", \"denver\", \"houston\", \"kansas city\",\n",
    "           \"las vegas\", \"los angeles\", \"minneapolis\", \"montreal\", \"nashville\", \"new york\", \"oklahoma city\", \"phoenix\",\n",
    "           \"pittsburgh\", \"san francisco\", \"seattle\", \"tampa\", \"toronto\", \"washington\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
