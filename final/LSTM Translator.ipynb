{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Input, LSTM, CuDNNLSTM, Dense, Bidirectional, BatchNormalization, Dropout, Reshape, Concatenate, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"albuquerque\", \"billings\", \"calgary\", \"charlotte\", \"chicago\", \"cincinnati\", \"denver\", \"houston\", \"kansas city\",\n",
    "       \"las vegas\", \"los angeles\", \"minneapolis\", \"montreal\", \"nashville\", \"new york\", \"oklahoma city\", \"phoenix\",\n",
    "       \"pittsburgh\", \"san francisco\", \"seattle\", \"tampa\", \"toronto\", \"washington\"]\n",
    "df = pd.read_csv('data/tweets_labelled_balanced.csv', nrows=300000)\n",
    "df.dropna(inplace=True)\n",
    "df.region = df.region.astype(int)\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())\n",
    "X = df['text'].tolist()\n",
    "X2 = [\"<s> \"+x+\" <e>\" for x in X]\n",
    "X3 = [x+\" <e>\" for x in X]\n",
    "y = df['region'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  7, 10, 13, 14, 15, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "training_ratio = .75\n",
    "training_size = int(len(X)*training_ratio)\n",
    "num_classes = 23\n",
    "target_num_words = 10000\n",
    "H = 500\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "embedding_vector_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode strings\n",
    "t = text.Tokenizer(num_words=target_num_words, lower=True, char_level=False, filters='')\n",
    "\n",
    "# Convert strings to sequences, pad them to uniform length, and divide up training and test sets\n",
    "t.fit_on_texts(X2)\n",
    "word_index = t.word_index\n",
    "V = target_num_words + 2 #len(word_index)+1\n",
    "index_word = {v: k for k, v in t.word_index.items()}\n",
    "X_seq = t.texts_to_sequences(X)\n",
    "X2_seq = t.texts_to_sequences(X2)\n",
    "X3_seq = t.texts_to_sequences(X3)\n",
    "x_length = max(len(x) for x in X2_seq)\n",
    "X_padded = sequence.pad_sequences(X_seq, maxlen=x_length, padding='post')\n",
    "X2_padded = sequence.pad_sequences(X2_seq, maxlen=x_length, padding='post')\n",
    "X3_padded = sequence.pad_sequences(X3_seq, maxlen=x_length, padding='post')\n",
    "\n",
    "X_train = X_padded[:training_size]\n",
    "X2_train = X2_padded[:training_size]\n",
    "X3_train = X3_padded[:training_size]\n",
    "X_test = X_padded[training_size:]\n",
    "X2_test = X2_padded[training_size:]\n",
    "X3_test = X3_padded[training_size:]\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "# One-hot encode labels\n",
    "encoded_y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "encoded_y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "#X_train_target = to_categorical(X3_train, num_classes=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 802015 unique words in this dataset, but we're only using the top 10002.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} unique words in this dataset, but we're only using the top {}.\".format(len(word_index), V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Classifier Model for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Classifier Model and set its variables\n",
    "\n",
    "def load_model_weights(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "cls_model = load_model_weights('models/Classifier_full_balanced_1Bi1L.json', \n",
    "                               'models/Classifier_full_balanced_1Bi1L_rms_weights.h5')\n",
    "\n",
    "cls_df = pd.read_csv('data/tweets_labelled_balanced.csv')\n",
    "cls_df.dropna(inplace=True)\n",
    "cls_df.region = cls_df.region.astype(int)\n",
    "cls_X = cls_df['text'].tolist()\n",
    "cls_t = text.Tokenizer(num_words=10000, lower=True)\n",
    "cls_t.fit_on_texts(cls_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 10, 19, 18])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test classifier model is loaded and working properly\n",
    "\n",
    "def classify(tweets, target=False):\n",
    "    '''\n",
    "    Given a list of strings, return the predicted region for each string.  If a target is provided,\n",
    "    return the probability that each string is from the target region.\n",
    "    '''\n",
    "    test_sequence = cls_t.texts_to_sequences(tweets)\n",
    "    test_padded = sequence.pad_sequences(test_sequence, maxlen=50)\n",
    "    test_prediction_probs = cls_model.predict_on_batch(test_padded)\n",
    "    if target:\n",
    "        return test_prediction_probs[:,target]\n",
    "    else:\n",
    "        return np.argmax(test_prediction_probs, axis=1)\n",
    "\n",
    "    \n",
    "tweet_list = [\"if you're looking for work in va, check out this #job: #hiring #careerarc\", \n",
    "              \"i'm at cassellâ€™s burgers in los angeles, ca\",\n",
    "              \"go seahawks!\",\n",
    "              \"dude i have a startup.  want to invest in electric scooters?\"]\n",
    "\n",
    "classify(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to feed batches into the model\n",
    "class OneHotBatch(Sequence):\n",
    "  def __init__(self, X_data, X2_data, X3_data, y_data, batch_size, V, num_classes):\n",
    "    self.X_data = X_data\n",
    "    self.X2_data = X2_data\n",
    "    self.X3_data = X3_data\n",
    "    self.y_data = y_data\n",
    "    self.batch_size = batch_size\n",
    "    self.V = V\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def __len__(self):\n",
    "     return int(np.ceil(len(self.X_data) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, batch_id):\n",
    "    start = batch_id * self.batch_size\n",
    "    finish = start + self.batch_size\n",
    "    X = self.X_data[start:finish]\n",
    "    X2 = self.X2_data[start:finish]\n",
    "    X3 = to_categorical(self.X3_data[start:finish], num_classes=self.V)\n",
    "    y = to_categorical(self.y_data[start:finish], num_classes=self.num_classes)\n",
    "\n",
    "    return [X, X2], [y, X3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open('data/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((V, embedding_vector_length))\n",
    "for word, i in word_index.items():\n",
    "    if i == V:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# 1Bi x 1LSTM\n",
    "# 9m params with 1000 words\n",
    "\n",
    "# define training encoder\n",
    "encoder_inputs = Input(shape=(None, ), name=\"encoder_input\")\n",
    "encoder = Embedding(V, embedding_vector_length, weights=[embedding_matrix], trainable=True, \n",
    "                    name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(CuDNNLSTM(H, return_state=True), \n",
    "                                                                              name=\"encoder_lstm_1\")(encoder)\n",
    "#encoder_dropout = Dropout(0.2)\n",
    "encoder_dense = Dense(num_classes, activation='softmax', name=\"encoder_dense\")\n",
    "#encoder_outputs = encoder_dropout(encoder_outputs)\n",
    "encoder_outputs = encoder_dense(encoder_outputs)\n",
    "encoder_fstates = [forward_h, forward_c]\n",
    "encoder_bstates = [backward_h, backward_c]\n",
    "\n",
    "\n",
    "# define training decoder\n",
    "decoder_inputs = Input(shape=(None, ), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(V, embedding_vector_length, name=\"decoder_embedding\")\n",
    "embedded_input = decoder_embedding(decoder_inputs)\n",
    "decoder_flstm = CuDNNLSTM(H, return_sequences=True, return_state=True, name=\"decoder_flstm_1\")\n",
    "decoder_blstm = CuDNNLSTM(H, return_sequences=True, return_state=True, name=\"decoder_blstm_1\")\n",
    "decoder_foutputs, _, _ = decoder_flstm(embedded_input, initial_state=encoder_fstates)\n",
    "decoder_boutputs, _, _ = decoder_blstm(embedded_input, initial_state=encoder_bstates)\n",
    "decoder_outputs = Concatenate(name=\"concatenate_decoder_outputs_1\")([decoder_foutputs, decoder_boutputs])\n",
    "decoder_dense = Dense(V, activation='softmax', name=\"decoder_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Combine training inputs into a single training model\n",
    "model = Model([encoder_inputs, decoder_inputs], [encoder_outputs, decoder_outputs])\n",
    "\n",
    "# define inference encoder\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_fstates + encoder_bstates)\n",
    "\n",
    "# define inference decoder\n",
    "decoder_state_input_fh = Input(shape=(H,), name=\"decoder_state_input_fh\")\n",
    "decoder_state_input_fc = Input(shape=(H,), name=\"decoder_state_input_fc\")\n",
    "decoder_state_input_bh = Input(shape=(H,), name=\"decoder_state_input_bh\")\n",
    "decoder_state_input_bc = Input(shape=(H,), name=\"decoder_state_input_bc\")\n",
    "decoder_fstates_inputs = [decoder_state_input_fh, decoder_state_input_fc]\n",
    "decoder_bstates_inputs = [decoder_state_input_bh, decoder_state_input_bc]\n",
    "\n",
    "decoder_input_f2 = decoder_embedding(decoder_inputs)\n",
    "decoder_input_b2 = decoder_embedding(decoder_inputs)\n",
    "\n",
    "decoder_foutputs, state_fh, state_fc = decoder_flstm(decoder_input_f2, initial_state=decoder_fstates_inputs)\n",
    "decoder_boutputs, state_bh, state_bc = decoder_blstm(decoder_input_b2, initial_state=decoder_bstates_inputs)\n",
    "\n",
    "decoder_fstates = [state_fh, state_fc]\n",
    "decoder_bstates = [state_bh, state_bc]\n",
    "decoder_outputs = Concatenate(name=\"concatenate_decoder_outputs_2\")([decoder_foutputs, decoder_boutputs])\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_fstates_inputs + decoder_bstates_inputs, \n",
    "                      [decoder_outputs] + decoder_fstates + decoder_bstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 200)    2000400     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (Bidirectional)  [(None, 1000), (None 2808000     encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 200)    2000400     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_flstm_1 (CuDNNLSTM)     [(None, None, 500),  1404000     decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_blstm_1 (CuDNNLSTM)     [(None, None, 500),  1404000     decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm_1[0][3]             \n",
      "                                                                 encoder_lstm_1[0][4]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_decoder_outputs_1 ( (None, None, 1000)   0           decoder_flstm_1[0][0]            \n",
      "                                                                 decoder_blstm_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_dense (Dense)           (None, 23)           23023       encoder_lstm_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 10002)  10012002    concatenate_decoder_outputs_1[0][\n",
      "==================================================================================================\n",
      "Total params: 19,651,825\n",
      "Trainable params: 19,651,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/12\n",
      "3516/3516 [==============================] - 2608s 742ms/step - loss: 27.9653 - encoder_dense_loss: 2.6101 - decoder_dense_loss: 0.8452 - encoder_dense_acc: 0.0889 - decoder_dense_acc: 0.8761 - val_loss: 18.4551 - val_encoder_dense_loss: 2.5738 - val_decoder_dense_loss: 0.5294 - val_encoder_dense_acc: 0.1087 - val_decoder_dense_acc: 0.9132\n",
      "Epoch 2/12\n",
      "3516/3516 [==============================] - 2603s 740ms/step - loss: 14.1235 - encoder_dense_loss: 2.5361 - decoder_dense_loss: 0.3862 - encoder_dense_acc: 0.1171 - decoder_dense_acc: 0.9317 - val_loss: 11.3753 - val_encoder_dense_loss: 2.5017 - val_decoder_dense_loss: 0.2958 - val_encoder_dense_acc: 0.1304 - val_decoder_dense_acc: 0.9451\n",
      "Epoch 3/12\n",
      "3516/3516 [==============================] - 2604s 741ms/step - loss: 8.7904 - encoder_dense_loss: 2.4799 - decoder_dense_loss: 0.2104 - encoder_dense_acc: 0.1381 - decoder_dense_acc: 0.9575 - val_loss: 8.4418 - val_encoder_dense_loss: 2.4900 - val_decoder_dense_loss: 0.1984 - val_encoder_dense_acc: 0.1319 - val_decoder_dense_acc: 0.9600\n",
      "Epoch 4/12\n",
      "3516/3516 [==============================] - 2600s 739ms/step - loss: 6.2586 - encoder_dense_loss: 2.4349 - decoder_dense_loss: 0.1275 - encoder_dense_acc: 0.1568 - decoder_dense_acc: 0.9718 - val_loss: 7.1729 - val_encoder_dense_loss: 2.4413 - val_decoder_dense_loss: 0.1577 - val_encoder_dense_acc: 0.1531 - val_decoder_dense_acc: 0.9664\n",
      "Epoch 5/12\n",
      "3516/3516 [==============================] - 2609s 742ms/step - loss: 4.8722 - encoder_dense_loss: 2.3900 - decoder_dense_loss: 0.0827 - encoder_dense_acc: 0.1755 - decoder_dense_acc: 0.9806 - val_loss: 6.6784 - val_encoder_dense_loss: 2.4298 - val_decoder_dense_loss: 0.1416 - val_encoder_dense_acc: 0.1572 - val_decoder_dense_acc: 0.9694\n",
      "Epoch 6/12\n",
      "3516/3516 [==============================] - 2610s 742ms/step - loss: 4.0593 - encoder_dense_loss: 2.3566 - decoder_dense_loss: 0.0568 - encoder_dense_acc: 0.1899 - decoder_dense_acc: 0.9862 - val_loss: 6.4960 - val_encoder_dense_loss: 2.4322 - val_decoder_dense_loss: 0.1355 - val_encoder_dense_acc: 0.1649 - val_decoder_dense_acc: 0.9709\n",
      "Epoch 7/12\n",
      "3516/3516 [==============================] - 2615s 744ms/step - loss: 3.5598 - encoder_dense_loss: 2.3264 - decoder_dense_loss: 0.0411 - encoder_dense_acc: 0.2013 - decoder_dense_acc: 0.9897 - val_loss: 6.5425 - val_encoder_dense_loss: 2.4292 - val_decoder_dense_loss: 0.1371 - val_encoder_dense_acc: 0.1674 - val_decoder_dense_acc: 0.9709\n",
      "Epoch 8/12\n",
      "3516/3516 [==============================] - 2614s 743ms/step - loss: 3.2339 - encoder_dense_loss: 2.2898 - decoder_dense_loss: 0.0315 - encoder_dense_acc: 0.2169 - decoder_dense_acc: 0.9919 - val_loss: 6.5454 - val_encoder_dense_loss: 2.4609 - val_decoder_dense_loss: 0.1361 - val_encoder_dense_acc: 0.1672 - val_decoder_dense_acc: 0.9718\n",
      "Finished in 5:47:45.506916\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = time.time()\n",
    "\n",
    "# Generators\n",
    "train_generator = OneHotBatch(X_data=X_train, X2_data=X2_train, X3_data=X3_train, y_data=y_train, \n",
    "                              batch_size=batch_size, V=V, num_classes=num_classes)\n",
    "validation_generator = OneHotBatch(X_data=X_test, X2_data=X2_test, X3_data=X3_test, y_data=y_test, \n",
    "                              batch_size=batch_size, V=V, num_classes=num_classes)\n",
    "\n",
    "# Compile and train the model\n",
    "opt = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, clipvalue=.05)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'], loss_weights=[1.0,30.0])\n",
    "model.summary()\n",
    "callbacks = [EarlyStopping(monitor='val_decoder_dense_loss', patience=3, min_delta=.01, restore_best_weights=True),\n",
    "             TensorBoard(log_dir='./logs/Twitter_300k10k_1BLx2L_nodrop', histogram_freq=0, batch_size=32, write_graph=False, \n",
    "                         write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, \n",
    "                         embeddings_metadata=None, embeddings_data=None, update_freq='epoch')]\n",
    "\n",
    "model.fit_generator(generator=train_generator, callbacks=callbacks, epochs=12, validation_data=validation_generator)\n",
    "\n",
    "# Final evaluation of the model\n",
    "end_time = time.time()\n",
    "run_time = datetime.timedelta(seconds=end_time-start_time)\n",
    "print(\"Finished in {}\".format(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_flstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_1/strided_slice:0' shape=(?, 500) dtype=float32>, <tf.Tensor 'encoder_lstm_1/strided_slice_1:0' shape=(?, 500) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_blstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_1/strided_slice_3:0' shape=(?, 500) dtype=float32>, <tf.Tensor 'encoder_lstm_1/strided_slice_4:0' shape=(?, 500) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_flstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_state_input_fh:0' shape=(?, 500) dtype=float32>, <tf.Tensor 'decoder_state_input_fc:0' shape=(?, 500) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dolmstead/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_blstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_state_input_bh:0' shape=(?, 500) dtype=float32>, <tf.Tensor 'decoder_state_input_bc:0' shape=(?, 500) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save the encoder/decoder model and weights to disk\n",
    "model_name = 'Twitter_300k10k_1BLx2L_nodrop_8e'\n",
    "\n",
    "model_weights = model_name+\"_weights\"\n",
    "encoder_name = model_name+\"_encoder\"\n",
    "decoder_name = model_name+\"_decoder\"\n",
    "encoder_weights = encoder_name+\"_weights\"\n",
    "decoder_weights = decoder_name+\"_weights\"\n",
    "\n",
    "with open(\"models/{}.json\".format(model_name), 'w', encoding='utf8') as f:\n",
    "    f.write(model.to_json())\n",
    "model.save_weights(\"models/{}.h5\".format(model_weights))\n",
    "\n",
    "with open(\"models/{}.json\".format(encoder_name), 'w', encoding='utf8') as f:\n",
    "    f.write(encoder_model.to_json())\n",
    "encoder_model.save_weights(\"models/{}.h5\".format(encoder_weights))\n",
    "\n",
    "with open(\"models/{}.json\".format(decoder_name), 'w', encoding='utf8') as f:\n",
    "    f.write(decoder_model.to_json())\n",
    "decoder_model.save_weights(\"models/{}.h5\".format(decoder_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "def load_model_weights(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "#model_name = 'Twitter_Translator_100k5k_1Bix1L_masked'\n",
    "model_name = 'Twitter_100k5k_1Bix1L_rms'\n",
    "model = load_model('models/{}.h5'.format(model_name))\n",
    "\n",
    "encoder_model = load_model_weights('encoder_model.json', 'models/Twitter_100k5k_1Bix1L_rms_encoder_weights.h5')\n",
    "decoder_model = load_model_weights('decoder_model.json', 'models/Twitter_100k5k_1Bix1L_rms_decoder_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('see our latest #seattle, wa #job and click to apply: software quality engineer -  #delljobs #qa #hiring #careerarc',\n",
       " 'see our latest #sanfrancisco, #job now: click and to software software to be id - #indianapolis, #hiring #careerarc')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Test that the loaded model is working correctly\n",
    "translate(X[20], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fidelity(source):\n",
    "    return (decode_padded_sequence(source), decode_padded_sequence(predict_sequence(source)[0]))\n",
    "\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    return 1 - spatial.distance.cosine(emb1, emb2)\n",
    "\n",
    "def check_fidelity_batch(source_array, num):\n",
    "    translator_results = {'Source':[], 'Decoded':[], 'Fidelity': []}\n",
    "    fidelities = []\n",
    "    for _ in range(num):\n",
    "        source_embeddings = get_embeddings(source_array[_])\n",
    "        translation = predict_sequence(source_array[_], translate=False)\n",
    "        translation_embeddings = get_embeddings(translation)\n",
    "        fidelity = cosine_similarity(source_embeddings, translation_embeddings)\n",
    "        translator_results['Source'].append(decode_padded_sequence(source_array[_]))\n",
    "        translator_results['Decoded'].append(decode_padded_sequence(translation))\n",
    "        translator_results['Fidelity'].append(fidelity)\n",
    "        fidelities.append(fidelity)\n",
    "    df_fidbatch = pd.DataFrame.from_dict(translator_results)[['Source', 'Decoded', 'Fidelity']]\n",
    "    fidscore = np.mean(fidelities)\n",
    "    return (df_fidbatch, fidscore)\n",
    "\n",
    "def translate(source, target, probs=False):\n",
    "    return_string = False\n",
    "    if type(source) == str:\n",
    "        source_sentence = source\n",
    "        source_sequence = t.texts_to_sequences([source])\n",
    "        source_padded = sequence.pad_sequences(source_sequence, maxlen=x_length, padding='post')\n",
    "        return_string = True\n",
    "    else:\n",
    "        source_sentence = decode_padded_sequence(source)\n",
    "        source_padded = source\n",
    "    if probs:\n",
    "        source_sentence = source_sentence\n",
    "        source_prediction = get_nb_prediction(source_sentence)\n",
    "        source_probstring = \"({}, {:.2f}%)\".format(region_dict[source_prediction[0][0]], source_prediction[1][0]*100)\n",
    "        target_sentence = decode_padded_sequence(predict_sequence(source_padded, translate=target))\n",
    "        target_prediction = get_nb_prediction(target_sentence)\n",
    "        target_probstring = \"({}, {:.2f}%)\".format(region_dict[target_prediction[0][0]], target_prediction[1][0]*100)\n",
    "        return  (source_sentence, source_probstring, target_sentence, target_probstring)\n",
    "    else:\n",
    "        if return_string:\n",
    "            return (source_sentence, decode_padded_sequence(predict_sequence(source_padded, translate=target)))\n",
    "        else:\n",
    "            return (source_padded, predict_sequence(source_padded, translate=target))\n",
    "\n",
    "def translate_batch(source_array, target, num, probs=False):\n",
    "    translator_results = {'Source':[], 'Source_Prob': [], 'Translation':[], 'Translation_Prob':[]}\n",
    "    for _ in range(num):\n",
    "        translation = translate(source_array[_], target, False)\n",
    "        translator_results['Source'].append(translation[0])\n",
    "        translator_results['Translation'].append(translation[1])\n",
    "    if probs:\n",
    "        source_probs = get_nb_prediction(translator_results['Source'])\n",
    "        trans_probs = get_nb_prediction(translator_results['Translation'])\n",
    "        for x in range(len(translator_results['Source'])):\n",
    "            translator_results['Source_Prob'].append(\"{}, {:.2f}%\".format(region_dict[source_probs[0][x]], \n",
    "                                                                            source_probs[1][x]*100))\n",
    "            translator_results['Translation_Prob'].append(\"{}, {:.2f}%\".format(region_dict[trans_probs[0][x]], \n",
    "                                                                            trans_probs[1][x]*100))       \n",
    "        return pd.DataFrame.from_dict(translator_results)[['Source', 'Source_Prob', 'Translation', 'Translation_Prob']]\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(translator_results)[['Source', 'Translation']]\n",
    "\n",
    "def predict_sequence(source, translate=False, beam=False):\n",
    "    '''\n",
    "    Given a source array, feed it through the autoencoder to predict a string - either itself in the naive case \n",
    "    where translation is turned off, or run gradient ascent to convert the source array to a target category,\n",
    "    and run that through the autoencoder to get the translated version.\n",
    "    '''\n",
    "    source = source.reshape(1, x_length)\n",
    "    # feed the source into the encoder inference model\n",
    "    encode = encoder_model.predict(source)\n",
    "    \n",
    "    # If set to translate, run gradient ascent to maximize to the target_label\n",
    "    if translate:\n",
    "        fstate, bstate = gradient_ascent(source, encoder_model, translate)\n",
    "        decode_sequence = decode_latent(fstate, bstate, source=source, target_cat=translate, beam=beam)\n",
    "    else:\n",
    "        fstate = [encode[1:][0], encode[1:][1]]\n",
    "        bstate = [encode[1:][2], encode[1:][3]]\n",
    "        decode_sequence = decode_latent(fstate, bstate)\n",
    "    \n",
    "    return decode_sequence\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm, used to scale gradient ascent\n",
    "    # return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "    return x / K.max(x)\n",
    "\n",
    "def gradient_ascent(seq, model, target, show_steps=False, step_rate=.5, target_prob=.9):\n",
    "    '''\n",
    "    Run gradient ascent to maximize a sequence to a target category.  Returns final state values.\n",
    "    '''\n",
    "    target_probability = target_prob # You want the model to be this certain the string is in the target category\n",
    "    \n",
    "    # Identify the target model layers and tensors for input and output\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    lstm_input = layer_dict['encoder_lstm_1'].input # Use the layer that accepts the embeddings\n",
    "    model_input = model.input\n",
    "    \n",
    "    loss = K.mean(model.output[0][:, target]) # The loss value for the target category\n",
    "    states = K.identity(model.output[1:]) # The h, c values for this iteration\n",
    "    grads = K.gradients(loss, lstm_input)[0] # The gradients for the lstm_input layer w/respect to the loss\n",
    "    grads = normalize(grads) # Play with this function to scale the speed of the ascent\n",
    "    \n",
    "    # Define input/output functions\n",
    "    get_embeddings = K.function([model_input], [lstm_input])\n",
    "    run_ascent = K.function([lstm_input], [loss, grads, states])\n",
    "\n",
    "    # Input sequence to model to initiate the ascent\n",
    "    embeddings_value = get_embeddings([seq])[0]\n",
    "    \n",
    "    # Iterate through the model until loss exceeds target probability\n",
    "    counter = 0\n",
    "    while counter < 20:\n",
    "        loss_value, grads_value, states_value = run_ascent([embeddings_value])\n",
    "        target_shape = states_value.shape[2]\n",
    "        fstate = [np.reshape(states_value[0][0], (1,target_shape)), np.reshape(states_value[1][0], (1,target_shape))]\n",
    "        bstate = [np.reshape(states_value[2][0], (1,target_shape)), np.reshape(states_value[3][0], (1,target_shape))]        \n",
    "        if show_steps:\n",
    "            print(\"{}. ({:.2f}) {}\".format(counter, loss_value, decode_padded_sequence(decode_latent(fstate, bstate))))\n",
    "        return_value = (fstate, bstate)\n",
    "        if loss_value > target_probability and not show_steps: # Exit the ascent\n",
    "            return return_value if not show_steps else \"Complete\"\n",
    "            break\n",
    "        elif loss_value <= 0.: # Some inputs can zero out the loss\n",
    "            break\n",
    "        else:\n",
    "            grads_value = grads_value * step_rate\n",
    "            embeddings_value += grads_value\n",
    "            counter += 1\n",
    "\n",
    "def decode_latent(fstate, bstate, source=None, target_cat=None, beam=False):\n",
    "    '''\n",
    "    Given a pair of state vectors, run iteratively through the decoder to build an output sequence.\n",
    "    Returns the padded output sequence of the most likely sentence, unless 'beam_search' is set\n",
    "    to True, in which case it returns the most probably sequence and an array of the top 5 most\n",
    "    likely sequences as a tuple.\n",
    "    '''\n",
    "    # start of sequence input\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = 1 # Start Character index\n",
    "    output_seq = list()\n",
    "    output_array = np.ones((5,1), dtype=int)\n",
    "    stop = False\n",
    "    word_counter = 1\n",
    "    while not stop:\n",
    "        output_tokens, fh, fc, bh, bc = decoder_model.predict([target_seq] + fstate + bstate)\n",
    "        output_tokens = output_tokens[0, -1, :]\n",
    "\n",
    "        # Sample the top n tokens\n",
    "        top_tokens = np.argpartition(output_tokens, -5)[-5:]\n",
    "        top_tokens = top_tokens[np.argsort(output_tokens[top_tokens])][::-1]\n",
    "        top_token = top_tokens[0]\n",
    "\n",
    "        # Exit if sampled character is end token or we've reached max length\n",
    "        decoded_char = index_word[top_token] if top_token > 0 else ''\n",
    "        if (decoded_char == '<e>' or word_counter + 1 == x_length):\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        # If we haven't reached the end of the sentence...\n",
    "        output_seq.append(top_token)\n",
    "        top_tokens = np.expand_dims(top_tokens, axis=1)\n",
    "        if word_counter > 1:\n",
    "            output_array = np.concatenate((output_array, top_tokens), axis=1)\n",
    "        else:\n",
    "            output_array = top_tokens\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = top_token\n",
    "\n",
    "        # Update states and counter\n",
    "        fstate = [fh, fc]\n",
    "        bstate = [bh, bc]\n",
    "        word_counter += 1\n",
    "    \n",
    "    if beam:\n",
    "        beam_array = beam_search(source, output_array, target_cat, tag_weight=0)\n",
    "\n",
    "    while len(output_seq) < x_length:\n",
    "        output_seq.append(0)\n",
    "    return (np.array([output_seq]), output_array) if beam else np.array([output_seq])\n",
    "\n",
    "\n",
    "def beam_search(source, dec_array, target_cat, tag_weight=0):\n",
    "    '''\n",
    "    Given a source sequence and a an array of the top-5 potential words for each position from the decoder,\n",
    "    optimize the potential words into the best five sentences according to both target region score\n",
    "    and part-of-speech tag similarity.  'tag_weight' is set between -1 (full weight to regional score) to\n",
    "    1 (full weight to tag similarity--NOT IMPLEMENTED).\n",
    "    '''\n",
    "    target_tags = pos_tagger([decode_padded_sequence(source[0])])\n",
    "    buffer = dec_array[:,0].reshape(5,1)\n",
    "    for i in range(1, dec_array.shape[1]):\n",
    "        candidates = None # build the list of 25 new candidate sentences\n",
    "        for j in dec_array[:,i]: # iterate through the next top 5 words\n",
    "            for k in buffer: # for each of the top 5 previous sentences\n",
    "                candidate = k.copy()\n",
    "                candidate = np.append(candidate, j)\n",
    "                if candidates is not None:\n",
    "                    candidates = np.vstack([candidates, candidate])\n",
    "                else:\n",
    "                    candidates = candidate\n",
    "\n",
    "        # From the 25 candidates, narrow down to the top 5\n",
    "        sentences = []\n",
    "        for c in candidates:\n",
    "            sentences.append(decode_padded_sequence(c))\n",
    "        tag_score = match_tags(sentences, target_tags[0][:i])\n",
    "        cat_score = get_nb_prediction(sentences, target_cat)\n",
    "        sen_score = (tag_score + cat_score) / 2\n",
    "        top_sequences = np.argpartition(sen_score, -5)[-5:]\n",
    "        top_sequences = top_sequences[np.argsort(sen_score[top_sequences])][::-1]\n",
    "        top_5_sequences = candidates[top_sequences]\n",
    "        buffer = top_5_sequences\n",
    "    return buffer\n",
    "\n",
    "def pos_tagger(tweets):\n",
    "    ''' Fetch the Part-of-Speech tags for this text from the CMU Tweet Tagger.'''\n",
    "    tags = []\n",
    "    p = Popen('java -XX:ParallelGCThreads=2 -Xmx500m -jar ~/ark-tweet-nlp/ark-tweet-nlp-0.3.2.jar --no-confidence', \n",
    "                   stdin=PIPE, shell=True, stderr=PIPE, stdout=PIPE)\n",
    "    stdout_data = p.communicate(input='\\n'.join(tweets).encode())[0].decode(\"utf-8\")\n",
    "    stdout_data = stdout_data.split('\\n')\n",
    "    p.kill()\n",
    "    for response in stdout_data[:-1]:\n",
    "        tag_string = re.sub(' ', '', response.split('\\t')[1])\n",
    "        tags.append(tag_string)\n",
    "    return tags\n",
    "\n",
    "def get_embeddings(seq):\n",
    "    ''' Given a sequence, get the word embeddings for this model'''\n",
    "    layer_dict = dict([(layer.name, layer) for layer in encoder_model.layers[1:]])\n",
    "\n",
    "    model_input = layer_dict['encoder_embedding'].input\n",
    "    embedding_layer_output = encoder_model.layers[1].output\n",
    "    lookup_embeddings = K.function([model_input], [embedding_layer_output])\n",
    "    embeddings = lookup_embeddings([seq])[0].flatten()\n",
    "\n",
    "    return list(embeddings)\n",
    "\n",
    "def decode_padded_sequence(seq):\n",
    "    seq = seq.flatten()\n",
    "    return \" \".join([index_word[x] for x in seq if x > 0])\n",
    "\n",
    "def get_padded_sequence(sentence):\n",
    "    # Given a sentence string, return the padded sequence of index numbers according to the tokenizer\n",
    "    s_seq = [word_index[x] for x in sentence.split()]\n",
    "    s_padded = sequence.pad_sequences([s_seq], maxlen=x_length, padding='post')\n",
    "    return s_padded\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).quick_ratio()\n",
    "\n",
    "def match_tags(taglist, target):\n",
    "    tags = pos_tagger(taglist)\n",
    "    return np.array([similar(x, target) for x in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(infenc, infdec, source, n_steps, translate=False):\n",
    "    '''\n",
    "    Given a source array, feed it through the autoencoder to predict a string - either itself in the naive case \n",
    "    where translation is turned off, or run gradient ascent to convert the source array to a target category,\n",
    "    and run that through the autoencoder to get the translated version.\n",
    "    '''\n",
    "    region_dict = {0: \"albuquerque\", 1: \"billings\", 2: \"calgary\", 3: \"charlotte\", 4: \"chicago\", 5: \"cincinnati\", 6: \"denver\", \n",
    "               7: \"houston\", 8: \"kansas city\", 9: \"las vegas\", 10: \"los angeles\", 11: \"minneapolis\", 12: \"montreal\", \n",
    "               13: \"nashville\", 14: \"new york\", 15: \"oklahoma city\", 16: \"phoenix\", 17: \"pittsburgh\", 18: \"san francisco\", \n",
    "               19: \"seattle\", 20: \"tampa\", 21: \"toronto\", 22: \"washington\"}\n",
    "    \n",
    "    source_string = \" \".join([index_word[x] for x in source[0] if x > 0])\n",
    "    # feed the source into the encoder inference model\n",
    "    encode = infenc.predict(source)\n",
    "    # make prediction of category for source sequence\n",
    "    label_prediction_probs = encode[0][0]\n",
    "    label_prediction = np.argmax(label_prediction_probs)\n",
    "    source_label_prediction = region_dict[label_prediction]\n",
    "    source_label_certainty = label_prediction_probs[label_prediction]\n",
    "    \n",
    "    # If set to translate, run gradient ascent to maximize to the target_label\n",
    "    if translate:\n",
    "        fstate, bstate = gradient_ascent(source, infenc, translate)\n",
    "    else:\n",
    "        fstate = [encode[1:][0], encode[1:][1]]\n",
    "        bstate = [encode[1:][2], encode[1:][3]]\n",
    "\n",
    "    decode_sequence = decode_latent(fstate, bstate)\n",
    "    decode_string = decode_padded_sequence(decode_sequence[0])\n",
    "    \n",
    "    # make prediction of category for predicted response\n",
    "    decode_prediction = infenc.predict(decode_sequence)\n",
    "    label_prediction_probs = decode_prediction[0][0]\n",
    "    label_prediction = np.argmax(label_prediction_probs)\n",
    "    decode_label_prediction = region_dict[label_prediction]\n",
    "    decode_label_certainty = label_prediction_probs[label_prediction]\n",
    "    \n",
    "    return (source_string, source_label_prediction, source_label_certainty,\n",
    "            decode_string, decode_label_prediction, decode_label_certainty, decode_sequence)\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm, used to scale gradient ascent\n",
    "    #return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "    return x / K.max(x)\n",
    "'''\n",
    "def gradient_ascent(seq, model, target):\n",
    "\n",
    "    target_probability = .95 # You want the model to be this certain the string is in the target category\n",
    "    \n",
    "    # Identify the target model layers and tensors for input and output\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    lstm_input = layer_dict['encoder_lstm_1'].input # Use the layer that accepts the embeddings\n",
    "    model_input = model.input\n",
    "    \n",
    "    loss = K.mean(model.output[0][:, target]) # The loss value for the target category\n",
    "    states = K.identity(model.output[1:]) # The h, c values for this iteration\n",
    "    grads = K.gradients(loss, lstm_input)[0] # The gradients for the lstm_input layer w/respect to the loss\n",
    "    grads = normalize(grads) # Play with this function to scale the speed of the ascent\n",
    "    \n",
    "    # Define input/output functions\n",
    "    get_embeddings = K.function([model_input], [lstm_input])\n",
    "    run_ascent = K.function([lstm_input], [loss, grads, states])\n",
    "\n",
    "    # Input sequence to model to initiate the ascent\n",
    "    embeddings_value = get_embeddings([seq])[0]\n",
    "    \n",
    "    # Iterate through the model until loss exceeds target probability\n",
    "    while True:\n",
    "        loss_value, grads_value, states_value = run_ascent([embeddings_value])\n",
    "        if loss_value > target_probability: # Exit the ascent\n",
    "            target_shape = states_value.shape[2]\n",
    "            f_state = [np.reshape(final_output[0][0], (1,target_shape)), np.reshape(final_output[1][0], (1,target_shape))]\n",
    "            b_state = [np.reshape(final_output[2][0], (1,target_shape)), np.reshape(final_output[3][0], (1,target_shape))]\n",
    "            return (f_state, b_state)\n",
    "            break\n",
    "        elif loss_value <= 0.: # Some inputs can zero out the loss\n",
    "            break\n",
    "        else:\n",
    "            embeddings_value += grads_value\n",
    "\n",
    "'''\n",
    "def gradient_ascent(seq, model, target, show_steps=False, step_rate=.5, target_prob=.9):\n",
    "    target_probability = target_prob # You want the model to be this certain the string is in the target category\n",
    "    \n",
    "    # Identify the target model layers and tensors for input and output\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    lstm_input = layer_dict['encoder_lstm_1'].input # Use the layer that accepts the embeddings\n",
    "    model_input = model.input\n",
    "    \n",
    "    loss = K.mean(model.output[0][:, target]) # The loss value for the target category\n",
    "    states = K.identity(model.output[1:]) # The h, c values for this iteration\n",
    "    grads = K.gradients(loss, lstm_input)[0] # The gradients for the lstm_input layer w/respect to the loss\n",
    "    grads = normalize(grads) # Play with this function to scale the speed of the ascent\n",
    "    \n",
    "    # Define input/output functions\n",
    "    get_embeddings = K.function([model_input], [lstm_input])\n",
    "    run_ascent = K.function([lstm_input], [loss, grads, states])\n",
    "\n",
    "    # Input sequence to model to initiate the ascent\n",
    "    embeddings_value = get_embeddings([seq])[0]\n",
    "    \n",
    "    # Iterate through the model until loss exceeds target probability\n",
    "    counter = 0\n",
    "    while counter < 20:\n",
    "        loss_value, grads_value, states_value = run_ascent([embeddings_value])\n",
    "        target_shape = states_value.shape[2]\n",
    "        fstate = [np.reshape(states_value[0][0], (1,target_shape)), np.reshape(states_value[1][0], (1,target_shape))]\n",
    "        bstate = [np.reshape(states_value[2][0], (1,target_shape)), np.reshape(states_value[3][0], (1,target_shape))]        \n",
    "        if show_steps:\n",
    "            print(\"{}. ({:.2f}) {}\".format(counter, loss_value, decode_padded_sequence(decode_latent(fstate, bstate))))\n",
    "        if loss_value > target_probability and not show_steps: # Exit the ascent\n",
    "            return (fstate, bstate) if not show_steps else \"Complete\"\n",
    "            break\n",
    "        elif loss_value <= 0.: # Some inputs can zero out the loss\n",
    "            break\n",
    "        else:\n",
    "            grads_value = grads_value * step_rate\n",
    "            embeddings_value += grads_value\n",
    "            counter += 1\n",
    "\n",
    "            \n",
    "\n",
    "def decode_latent(fstate, bstate):\n",
    "    '''\n",
    "    Given a pair of state vectors, run iteratively through the decoder to build an output sequence.\n",
    "    Returns the padded output sequence\n",
    "    '''\n",
    "    # start of sequence input\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = 1 # Start Character index\n",
    "    output = list()\n",
    "    stop = False\n",
    "    word_counter = 1\n",
    "    while not stop:\n",
    "        output_tokens, fh, fc, bh, bc = decoder_model.predict([target_seq] + fstate + bstate)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        # Exit if sampled character is end token or we've reached max length\n",
    "        decoded_char = index_word[sampled_token_index] if sampled_token_index > 0 else ''\n",
    "        if (decoded_char == '<e>' or word_counter == x_length):\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        output.append(sampled_token_index)\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states and counter\n",
    "        fstate = [fh, fc]\n",
    "        bstate = [bh, bc]\n",
    "        word_counter += 1\n",
    "\n",
    "    while len(output) < x_length:\n",
    "        output.append(0)\n",
    "    predicted_sequence = np.array([output])\n",
    "    return predicted_sequence\n",
    "\n",
    "def get_embeddings(seq):\n",
    "    # Given a sequence, get the word embeddings for this model\n",
    "    layer_dict = dict([(layer.name, layer) for layer in encoder_model.layers[1:]])\n",
    "\n",
    "    model_input = layer_dict['encoder_embedding'].input\n",
    "    embedding_layer_output = encoder_model.layers[1].output\n",
    "    lookup_embeddings = K.function([model_input], [embedding_layer_output])\n",
    "    embeddings = lookup_embeddings([seq])[0].flatten()\n",
    "\n",
    "    return list(embeddings)\n",
    "\n",
    "def decode_padded_sequence(seq):\n",
    "    return \" \".join([index_word[x] for x in seq if x > 0])\n",
    "\n",
    "def get_padded_sequence(sentence):\n",
    "    # Given a sentence string, return the padded sequence of index numbers according to the tokenizer\n",
    "    s_seq = [word_index[x] for x in sentence.split()]\n",
    "    s_padded = sequence.pad_sequences([s_seq], maxlen=x_length, padding='post')\n",
    "    return s_padded\n",
    "\n",
    "def translate_sentence(sentence, target):\n",
    "    # Put in a sentence and a target region, get a result\n",
    "    source = get_padded_sequence(sentence)\n",
    "    result = predict_sequence(encoder_model, decoder_model, source, x_length, target)\n",
    "    return (\"Input: {} ({} {:.2f}) \\nOutput: {} ({} {:.2f})\"\n",
    "            .format(result[0], result[1], result[2], result[3], result[4], result[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (4.87%) [[  34  207  174   11   73    8 2979  220   42   14  407 1530   67  154\n",
      "    66    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n",
      "1: (22.26%) [[  34  207  174   11   73    8 2979  220   42   14  407 1530   66   58\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n",
      "2: (68.14%) [[ 174   42  174   11  174    8   82  220   11   14  407 1530   67  154\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n",
      "3: (90.47%) [[  82   67    8   82   67    8   82  127   18    6   82  714 2163   66\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n",
      "4: (94.70%) [[ 50  71 233  19  71 414  72  25   6 133  19   6 133   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "5: (97.48%) [[279  27  19  71 181  19  71 133  19   6 357  19  21   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "region_dict = {0: \"albuquerque\", 1: \"billings\", 2: \"calgary\", 3: \"charlotte\", 4: \"chicago\", 5: \"cincinnati\", 6: \"denver\", \n",
    "               7: \"houston\", 8: \"kansas city\", 9: \"las vegas\", 10: \"los angeles\", 11: \"minneapolis\", 12: \"montreal\", \n",
    "               13: \"nashville\", 14: \"new york\", 15: \"oklahoma city\", 16: \"phoenix\", 17: \"pittsburgh\", 18: \"san francisco\", \n",
    "               19: \"seattle\", 20: \"tampa\", 21: \"toronto\", 22: \"washington\"}\n",
    "\n",
    "seq = X_train[[2]]\n",
    "target = 20\n",
    "\n",
    "target_probability = .95 # You want the model to be this certain the string is in the target category\n",
    "layer_dict = dict([(layer.name, layer) for layer in encoder_model.layers[1:]])\n",
    "layer_name = 'encoder_lstm_1'\n",
    "layer_input = layer_dict[layer_name].input\n",
    "input_txt = encoder_model.input\n",
    "loss = K.mean(encoder_model.output[0][:, target])\n",
    "states = K.identity(encoder_model.output[1:])\n",
    "grads = K.gradients(loss, layer_input)[0]\n",
    "grads = normalize(grads)\n",
    "initiate = K.function([input_txt], [layer_input])\n",
    "iterate = K.function([layer_input], [loss, grads])\n",
    "terminate = K.function([layer_input], [states])\n",
    "\n",
    "embedding = initiate([seq])[0]\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([embedding])\n",
    "    final_output = terminate([embedding])[0]\n",
    "    target_shape = final_output.shape[2]\n",
    "    f_state = [np.reshape(final_output[0][0], (1,target_shape)), np.reshape(final_output[1][0], (1,target_shape))]\n",
    "    b_state = [np.reshape(final_output[2][0], (1,target_shape)), np.reshape(final_output[3][0], (1,target_shape))]\n",
    "    print(\"{}: ({:.2f}%) {}\".format(i, loss_value*100, decode_latent(f_state, b_state)))\n",
    "\n",
    "    if loss_value > target_probability:\n",
    "        final_output = terminate([embedding])[0]\n",
    "        target_shape = final_output.shape[2]\n",
    "        print(\"All Done\")\n",
    "        break\n",
    "    elif loss_value <= 0.:\n",
    "        break\n",
    "    else:\n",
    "        embedding += grads_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. (0.06) this season the packers fans have zero room to talk shit about the bears or\n",
      "1. (0.14) this season the packers fans have zero room to talk shit about the bears or\n",
      "2. (0.85) this season the #steelers packers have available somewhere to his talk about the bears or\n",
      "3. (0.98) this season the #steelers #design have zero chicago to talk about his or bears to\n",
      "4. (1.00) this sunday, the #steelers benefit event. have chicago to attention about to talk the evil\n",
      "5. (1.00) this throwback the #steelers benefit fl have west him to talk about the black blue\n",
      "6. (1.00) iowa yet. the matchup sports have fair york to shit talk about the bears or\n",
      "7. (1.00) this throwback the matchup i've vs. local park, to talk about it the evil or\n",
      "8. (1.00) iowa was the best. matchup weekend! york have to talk about just the evil\n",
      "9. (1.00) section yet. the matchup #fall have sympathy minor to talk about just the evil or\n",
      "10. (1.00) 17th turnout the matchup (@ paradise have known to talk about it the evil or\n",
      "11. (1.00) this 17th year. matchup were sports the greater times to talk about the devil or\n",
      "12. (1.00) 17th year. the matchup mostly great turnout to me 2 shit the week ðŸ’›\n",
      "13. (1.00) earlier year. i'm hardly matchup for paradise the excitement to talk about last title\n",
      "14. (1.00) earlier was year. matchup it's welcoming the york lot to shit 2 - super\n",
      "15. (1.00) earlier year. i'm hardly the sports weekend! had nothing to go times every year. lol\n",
      "16. (1.00) earlier year. i'm hardly hosting the beach, college times to go last word no.\n",
      "17. (1.00) earlier was super entertaining i've ontario have the nothing time to say last year?\n",
      "18. (1.00) earlier year. i'm hardly super nominated for york had shit go happened 1 bye\n",
      "19. (1.00) this year. i've hardly been hosting miami in times to go old time... @\n"
     ]
    }
   ],
   "source": [
    "# Show the steps for gradient ascent\n",
    "gradient_ascent(X_padded[[500]], encoder_model, 14, show_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_sequence() takes from 1 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-db4ffcf0afea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscore_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_sequence() takes from 1 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# Evaluate success of training to see similarity of input and output sentences\n",
    "\n",
    "decoder_results = {'Source':[], 'Source Prediction':[], 'Source Certainty':[],\n",
    "                   'Decoded':[], 'Decoded Prediction':[], 'Decoded Certainty':[], 'Score': []}\n",
    "\n",
    "score_list = []\n",
    "\n",
    "for _ in range(100):\n",
    "    target = predict_sequence(encoder_model, decoder_model, X_train[[_]], x_length, False)\n",
    "    score = 1 - spatial.distance.cosine(get_embeddings(X_train[[_]]), get_embeddings(target[6]))\n",
    "    score_list.append(score)\n",
    "    decoder_results['Source'].append(target[0])\n",
    "    decoder_results['Source Prediction'].append(target[1])\n",
    "    decoder_results['Source Certainty'].append(target[2])\n",
    "    decoder_results['Decoded'].append(target[3])\n",
    "    decoder_results['Decoded Prediction'].append(target[4])\n",
    "    decoder_results['Decoded Certainty'].append(target[5])\n",
    "    decoder_results['Score'].append(score)\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(decoder_results)[['Source', 'Source Prediction', 'Source Certainty', \n",
    "                                         'Decoded', 'Decoded Prediction', 'Decoded Certainty',\n",
    "                                         'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6844770941138267"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Decoded</th>\n",
       "      <th>Decoded Prediction</th>\n",
       "      <th>Decoded Certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we got the earth in the blunt..</td>\n",
       "      <td>we got the in the city</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.119070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iâ€™m bout to be up doing this hw assignment that was due last week. i didnâ€™t go to class last week cause i didnâ€™t do it. now here we meet again &amp;amp; i still didnâ€™t do the shit but need to hand it ...</td>\n",
       "      <td>iâ€™m going to be to be at this assignment this and iâ€™m so iâ€™m to see i am at the week. i am iâ€™m at the week. i am here in the i am in the i am here in the i am iâ€™m in the life. lol</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.143612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you're looking for work in #woodstock, va, check out this #job: #schoolpsychology #hiring #careerarc</td>\n",
       "      <td>if you're looking for work in ca, check out this #job: #manufacturing #job #jobs #hiring</td>\n",
       "      <td>chicago</td>\n",
       "      <td>0.142153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have the craving for peanut butter</td>\n",
       "      <td>i have the craving for peanut butter</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.155811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a weight has been lifted off my shoulder and i feel so great omg ðŸ˜­â¤ï¸</td>\n",
       "      <td>a few has been been been my mom and i was so iâ€™m so ðŸ˜­â¤ï¸</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.188428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i found this screenshot from a few years ago and iâ€™m crying why am i so funny</td>\n",
       "      <td>i was a few and i was a phone and i am so so so i hate</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.109619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you had me with your words lost me with your actionsðŸ’–\\r\\n#styledbysilvay @ los angeles, california</td>\n",
       "      <td>you you with me with your favorite restaurant &amp;amp; you @ los angeles, california</td>\n",
       "      <td>toronto</td>\n",
       "      <td>0.129498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the lyft ride wasnâ€™t any better. i felt in danger and iâ€™m a lyft driver myself ðŸ˜‚</td>\n",
       "      <td>the nail in the l i donâ€™t donâ€™t be a pair and iâ€™m a lyft is. lol</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.145585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this member was happy to get a #zerowaste flu shot! way to #conserve paper ðŸŒ³ðŸ‘ðŸ½</td>\n",
       "      <td>this is a way to be to be to be a great way to</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.108641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>citadel - cit-adel = trends from military words!</td>\n",
       "      <td>(feels: - humidity: 88% - block of words!</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.135278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i literally am sick and without the aca i might very well be dead. chemo, mris, scans are not cheap. and even with ðŸ„ðŸ† trying to kill it, prices have gone down this year. maybe try reading some rep...</td>\n",
       "      <td>i am so i am the and i have a lot of we are so i'm they are a lot of people are many people are to be &amp;amp; if they are some and itâ€™s be some and they are they get up on</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.156384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>glock a fronting ass nigga</td>\n",
       "      <td>lil a fronting nigga nigga</td>\n",
       "      <td>washington</td>\n",
       "      <td>0.143658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i'm gonna get a tattoo when i go to london.</td>\n",
       "      <td>iâ€™m gonna get a fuck back to get to london.</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.175602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>in all my years... i canâ€™t believe it</td>\n",
       "      <td>in the years... i donâ€™t know it to</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.184836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>who trynna match leafs/wood only</td>\n",
       "      <td>who who match leafs/wood play</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.142713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>every 2 to 4 yearsâ€”or at whichever point an election white people cannot get done what they need to get done on their ownâ€”the nation turns again to the negro and demands: do my biding.</td>\n",
       "      <td>every morning to yearsâ€”or on whichever and going to get on 4 and if it are going to do in the ownâ€”the run. and do in the other way to do the negro and</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.126105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>why am i up</td>\n",
       "      <td>why am i am</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.152493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>washington high and usc commit. absolute beast @ georgia's 4th congressional district</td>\n",
       "      <td>washington are high high commit. commit. beast @ georgia's pointe congressional congressional</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.203179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>iâ€™m so glad you enjoyed it and i could help bring you joyðŸ¥°ðŸ¥°ðŸ¥° youâ€™re amazing</td>\n",
       "      <td>iâ€™m so so you you and i want to be and you canâ€™t joyðŸ¥°ðŸ¥°ðŸ¥° you!!!</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.167463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this movie i was a casting ast. is hitting redbox dec 11th if you could all go rent it; yâ€™all would rock! ps itâ€™s very suspenseful &amp;amp; filled with very cool special effects.</td>\n",
       "      <td>this is i just a casting ast. is redbox redbox redbox if i am sooo sooo thank you feel so iâ€™m like iâ€™m suspenseful &amp;amp; suspenseful &amp;amp; very suspenseful &amp;amp; very effects. effects.</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.151871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>see our latest #seattle, wa #job and click to apply: software quality engineer - #delljobs #qa #hiring #careerarc</td>\n",
       "      <td>see our latest #job and click to apply: software engineer - engineer - #kellyjobs #kellyservices #kellyservices #hiring #careerarc</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.132466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>see our latest ma #job and click to apply: clinical scientist - #engineering #hiring #careerarc</td>\n",
       "      <td>see our latest #job and click to apply: software engineer - #engineering #hiring #careerarc</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.129162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i have the bestest friends around no doubt â™¥ï¸</td>\n",
       "      <td>i have the bestest friends so much friends ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.152073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ohio state has sec speed , michigan does not .. ohio state has the capability to beat anyone in the country due to their talent .. michigan does not .. you wanna argue till next year , iâ€™ll let th...</td>\n",
       "      <td>ohio state has sec has sec michigan state state has not ohio state is the capability to the state has not the site &amp;amp; not not not to the cost. if if you go in the exhibitor &amp;amp; you go in the</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.253640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>iâ€™m up by like $2.50. the trophy will stay where it belongs.</td>\n",
       "      <td>iâ€™m going up and the trophy will be the way it belongs.</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.197499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>found out how easy and cheap it is to make mochi!ðŸ¤¤</td>\n",
       "      <td>and it out and iâ€™m going to be to be mochi!ðŸ¤¤</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.162506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>he had a song that was popular from 2009 you think the lions would do everything possible to forget about anything before 2011</td>\n",
       "      <td>he was a only has that was a way to the way to see the way about why if you know about it.</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.155652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>poking the bear...</td>\n",
       "      <td>poking the</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.129868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>if u ask me to smoke u out donâ€™t be offended by what i ask for in return.</td>\n",
       "      <td>if if u go to ask if me to be and if i donâ€™t be in my #kellyservices</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.189069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>no cap ðŸ‘ŽðŸ¾</td>\n",
       "      <td>no cap ðŸ‘ŽðŸ¾</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.136320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>all you say are all the same things i did</td>\n",
       "      <td>all you are all the most we have to do</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.113879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>thanks for a great season at gower sports 18u and 12u</td>\n",
       "      <td>thanks for a great team for gower and 18u and 12u</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.148155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>see our latest #madison, wi #job and click to apply: call center agent - #marketing #careerarc</td>\n",
       "      <td>see our latest #job and click to apply: software engineer - engineer - #engineering #hiring #careerarc</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.134660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>my schedule today is fucking garbage. i fly back to houston, sit in houston for 4 hours and then come back to charlotte. let me just stay here and get drunk bitch</td>\n",
       "      <td>my morning is my phone is i go to sit up and i sit on 4 hours and iâ€™m up and get up and get up and get to be punk lmao</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.112129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>if youâ€™re not obsessed with the art of then you simply have not seen the art of migdalia paceðŸ¤·ðŸ»â€â™€ï¸ this #petraindigodoll t-shirt is a fave!! go check out her work on herâ€¦</td>\n",
       "      <td>if if not not not the victim is a positive if you is a or if you is a new \\r\\n3/5/1770 or the great or or your app or my ipad &amp;amp;</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.197050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>i always think about this. imagine 'bohemian rhapsody' or some marvin gaye or the beatles coming out. i just watched lauryn hill's 'doo wop' and i feel real weird about time.</td>\n",
       "      <td>i think about my rap ast. or but... iâ€™m diff or diff or the phone or why is why i know up on iâ€™m so iâ€™m about the shit on</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.111086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>i've done some fucked up shit to people i love. it sucks tbh.</td>\n",
       "      <td>iâ€™m some shit about iâ€™m up to shit i donâ€™t think it</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.133098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>dr. nurbakhsh, opening chapter of his account of the sufi path, â€œthe path: sufi practices.â€</td>\n",
       "      <td>dr. nurbakhsh, of america of his of the sufi path, path, path: path: practices.â€</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.179254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>annoyed af that i was so stupid when i was younger</td>\n",
       "      <td>thatâ€™s iâ€™m so i was so i was so that was</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.174936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>yâ€™all sheda came in clutch with this scarf today</td>\n",
       "      <td>iâ€™m up in 5 in 5 this day this</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>thatâ€™s fine iâ€™ll cut the friendship before it starts.</td>\n",
       "      <td>iâ€™m iâ€™m iâ€™m going the the way it starts.</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.143666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>she probably does at the rate she goin.</td>\n",
       "      <td>she she has at the top is now</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.190473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>it's brietbart, shane. did you expect anything less? or more honest? they are the worst journalist in the world. i blocked them lo ago. ðŸ˜¡\\r\\nstill adore you though! ðŸ˜€ðŸ‘</td>\n",
       "      <td>thatâ€™s you're shane. if you donâ€™t bump no music. in you in the journalist in the journalist in i can't even no blocked in adore here oh \\r\\n\\r\\n</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.116386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>we're back. you're welcome. #rctid\\r\\n\\r\\n \\r\\n\\r\\n#rctid</td>\n",
       "      <td>we're #hiring! you're welcome. #rctid\\r\\n\\r\\n \\r\\n\\r\\n#rctid</td>\n",
       "      <td>seattle</td>\n",
       "      <td>0.114537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>but thatâ€™s true about literally everything, not just smt.</td>\n",
       "      <td>but thatâ€™s iâ€™m iâ€™m like about that that</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.148519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>can you recommend anyone for this #job? epic application coordinator (orders) - #it spartanburg, sc #hiring</td>\n",
       "      <td>can you recommend anyone for this #job? #job - associate - #kellyjobs #kellyservices #kellyservices #generalscience #hiring</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>0.128213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>i let this car into my lane on mopac because they had a beto sticker and then when they got in front me i saw they had an ou sticker too. i am...conflicted?</td>\n",
       "      <td>i got this on my car and i got a phone and i got a beto years and i have been been on my but i have been a few sticker and i</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>0.123459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>what was your point lolðŸ¤¨ðŸ¤·ðŸ½â€â™€ï¸</td>\n",
       "      <td>what was my face of</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.120023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>itâ€™s 1:02am est on thanksgiving and iâ€™ve already spent almost $500 ðŸ˜‚</td>\n",
       "      <td>itâ€™s 1:02am est on today and itâ€™s so itâ€™s so $500 ðŸ˜‚</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.097156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>ðŸ™ŒðŸ¼â¤ï¸ thank you!</td>\n",
       "      <td>thank thank you!</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.125777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>only reason i ainâ€™t work both jobs i got in is because you wanted to move foh clown shit on top of clown shit</td>\n",
       "      <td>only only i have been about in i donâ€™t even the phone on why i know up on the shit on my shit on</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.143606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>really just wanna krash</td>\n",
       "      <td>just just wanna sleep</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.174238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>join the accenture team! see our latest #job opening here: #it #toronto, on #hiring #careerarc</td>\n",
       "      <td>join the team! see our latest #job opening here: latest #job opening here: #hiring #careerarc</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.149956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>matter a fact im bouta see the whole country</td>\n",
       "      <td>itâ€™s a lot is why we just the world</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.127807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>if you're looking for work in #cedarrapids, ia, check out this #job: #internship #hiring #careerarc</td>\n",
       "      <td>if you're looking for work in ca, check out this #job: #manufacturing #job #jobs #hiring</td>\n",
       "      <td>chicago</td>\n",
       "      <td>0.142153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>join the zurich team! see our latest #job opening here: #underwriting #houston, tx #hiring</td>\n",
       "      <td>join the team! see our latest #job opening here: #hospitality #job #jobs #hiring #careerarc</td>\n",
       "      <td>nashville</td>\n",
       "      <td>0.160634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>this explains a lot, i wonâ€™t forget this</td>\n",
       "      <td>this explains a lot, i can do this</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.110023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>the cyrkle - red rubber ball - 1966 via</td>\n",
       "      <td>the cyrkle - - rubber - - -</td>\n",
       "      <td>new york</td>\n",
       "      <td>0.207053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>everyone stuck on somebody ðŸ˜´</td>\n",
       "      <td>when on on on</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>0.119652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>when you do good shit, good shit happen for ya ðŸ¤žðŸ¾</td>\n",
       "      <td>when you really good good good good good for damn lol</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>0.137117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                     Source  \\\n",
       "0                                                                                                                                                                           we got the earth in the blunt..   \n",
       "1   iâ€™m bout to be up doing this hw assignment that was due last week. i didnâ€™t go to class last week cause i didnâ€™t do it. now here we meet again &amp; i still didnâ€™t do the shit but need to hand it ...   \n",
       "2                                                                                                   if you're looking for work in #woodstock, va, check out this #job: #schoolpsychology #hiring #careerarc   \n",
       "3                                                                                                                                                                      i have the craving for peanut butter   \n",
       "4                                                                                                                                      a weight has been lifted off my shoulder and i feel so great omg ðŸ˜­â¤ï¸   \n",
       "5                                                                                                                             i found this screenshot from a few years ago and iâ€™m crying why am i so funny   \n",
       "6                                                                                                        you had me with your words lost me with your actionsðŸ’–\\r\\n#styledbysilvay @ los angeles, california   \n",
       "7                                                                                                                          the lyft ride wasnâ€™t any better. i felt in danger and iâ€™m a lyft driver myself ðŸ˜‚   \n",
       "8                                                                                                                            this member was happy to get a #zerowaste flu shot! way to #conserve paper ðŸŒ³ðŸ‘ðŸ½   \n",
       "9                                                                                                                                                          citadel - cit-adel = trends from military words!   \n",
       "10  i literally am sick and without the aca i might very well be dead. chemo, mris, scans are not cheap. and even with ðŸ„ðŸ† trying to kill it, prices have gone down this year. maybe try reading some rep...   \n",
       "11                                                                                                                                                                               glock a fronting ass nigga   \n",
       "12                                                                                                                                                              i'm gonna get a tattoo when i go to london.   \n",
       "13                                                                                                                                                                    in all my years... i canâ€™t believe it   \n",
       "14                                                                                                                                                                         who trynna match leafs/wood only   \n",
       "15                 every 2 to 4 yearsâ€”or at whichever point an election white people cannot get done what they need to get done on their ownâ€”the nation turns again to the negro and demands: do my biding.   \n",
       "16                                                                                                                                                                                              why am i up   \n",
       "17                                                                                                                    washington high and usc commit. absolute beast @ georgia's 4th congressional district   \n",
       "18                                                                                                                              iâ€™m so glad you enjoyed it and i could help bring you joyðŸ¥°ðŸ¥°ðŸ¥° youâ€™re amazing   \n",
       "19                          this movie i was a casting ast. is hitting redbox dec 11th if you could all go rent it; yâ€™all would rock! ps itâ€™s very suspenseful &amp; filled with very cool special effects.   \n",
       "20                                                                                        see our latest #seattle, wa #job and click to apply: software quality engineer - #delljobs #qa #hiring #careerarc   \n",
       "21                                                                                                          see our latest ma #job and click to apply: clinical scientist - #engineering #hiring #careerarc   \n",
       "22                                                                                                                                                            i have the bestest friends around no doubt â™¥ï¸   \n",
       "23  ohio state has sec speed , michigan does not .. ohio state has the capability to beat anyone in the country due to their talent .. michigan does not .. you wanna argue till next year , iâ€™ll let th...   \n",
       "24                                                                                                                                             iâ€™m up by like $2.50. the trophy will stay where it belongs.   \n",
       "25                                                                                                                                                       found out how easy and cheap it is to make mochi!ðŸ¤¤   \n",
       "26                                                                           he had a song that was popular from 2009 you think the lions would do everything possible to forget about anything before 2011   \n",
       "27                                                                                                                                                                                       poking the bear...   \n",
       "28                                                                                                                                if u ask me to smoke u out donâ€™t be offended by what i ask for in return.   \n",
       "29                                                                                                                                                                                                no cap ðŸ‘ŽðŸ¾   \n",
       "..                                                                                                                                                                                                      ...   \n",
       "70                                                                                                                                                                all you say are all the same things i did   \n",
       "71                                                                                                                                                    thanks for a great season at gower sports 18u and 12u   \n",
       "72                                                                                                           see our latest #madison, wi #job and click to apply: call center agent - #marketing #careerarc   \n",
       "73                                       my schedule today is fucking garbage. i fly back to houston, sit in houston for 4 hours and then come back to charlotte. let me just stay here and get drunk bitch   \n",
       "74                               if youâ€™re not obsessed with the art of then you simply have not seen the art of migdalia paceðŸ¤·ðŸ»â€â™€ï¸ this #petraindigodoll t-shirt is a fave!! go check out her work on herâ€¦   \n",
       "75                           i always think about this. imagine 'bohemian rhapsody' or some marvin gaye or the beatles coming out. i just watched lauryn hill's 'doo wop' and i feel real weird about time.   \n",
       "76                                                                                                                                            i've done some fucked up shit to people i love. it sucks tbh.   \n",
       "77                                                                                                              dr. nurbakhsh, opening chapter of his account of the sufi path, â€œthe path: sufi practices.â€   \n",
       "78                                                                                                                                                       annoyed af that i was so stupid when i was younger   \n",
       "79                                                                                                                                                         yâ€™all sheda came in clutch with this scarf today   \n",
       "80                                                                                                                                                    thatâ€™s fine iâ€™ll cut the friendship before it starts.   \n",
       "81                                                                                                                                                                  she probably does at the rate she goin.   \n",
       "82                                  it's brietbart, shane. did you expect anything less? or more honest? they are the worst journalist in the world. i blocked them lo ago. ðŸ˜¡\\r\\nstill adore you though! ðŸ˜€ðŸ‘   \n",
       "83                                                                                                                                                we're back. you're welcome. #rctid\\r\\n\\r\\n \\r\\n\\r\\n#rctid   \n",
       "84                                                                                                                                                but thatâ€™s true about literally everything, not just smt.   \n",
       "85                                                                                              can you recommend anyone for this #job? epic application coordinator (orders) - #it spartanburg, sc #hiring   \n",
       "86                                             i let this car into my lane on mopac because they had a beto sticker and then when they got in front me i saw they had an ou sticker too. i am...conflicted?   \n",
       "87                                                                                                                                                                            what was your point lolðŸ¤¨ðŸ¤·ðŸ½â€â™€ï¸   \n",
       "88                                                                                                                                     itâ€™s 1:02am est on thanksgiving and iâ€™ve already spent almost $500 ðŸ˜‚   \n",
       "89                                                                                                                                                                                          ðŸ™ŒðŸ¼â¤ï¸ thank you!   \n",
       "90                                                                                            only reason i ainâ€™t work both jobs i got in is because you wanted to move foh clown shit on top of clown shit   \n",
       "91                                                                                                                                                                                  really just wanna krash   \n",
       "92                                                                                                           join the accenture team! see our latest #job opening here: #it #toronto, on #hiring #careerarc   \n",
       "93                                                                                                                                                             matter a fact im bouta see the whole country   \n",
       "94                                                                                                      if you're looking for work in #cedarrapids, ia, check out this #job: #internship #hiring #careerarc   \n",
       "95                                                                                                               join the zurich team! see our latest #job opening here: #underwriting #houston, tx #hiring   \n",
       "96                                                                                                                                                                 this explains a lot, i wonâ€™t forget this   \n",
       "97                                                                                                                                                                  the cyrkle - red rubber ball - 1966 via   \n",
       "98                                                                                                                                                                             everyone stuck on somebody ðŸ˜´   \n",
       "99                                                                                                                                                        when you do good shit, good shit happen for ya ðŸ¤žðŸ¾   \n",
       "\n",
       "                                                                                                                                                                                                Decoded  \\\n",
       "0                                                                                                                                                                                we got the in the city   \n",
       "1                   iâ€™m going to be to be at this assignment this and iâ€™m so iâ€™m to see i am at the week. i am iâ€™m at the week. i am here in the i am in the i am here in the i am iâ€™m in the life. lol   \n",
       "2                                                                                                              if you're looking for work in ca, check out this #job: #manufacturing #job #jobs #hiring   \n",
       "3                                                                                                                                                                  i have the craving for peanut butter   \n",
       "4                                                                                                                                               a few has been been been my mom and i was so iâ€™m so ðŸ˜­â¤ï¸   \n",
       "5                                                                                                                                                i was a few and i was a phone and i am so so so i hate   \n",
       "6                                                                                                                     you you with me with your favorite restaurant &amp; you @ los angeles, california   \n",
       "7                                                                                                                                      the nail in the l i donâ€™t donâ€™t be a pair and iâ€™m a lyft is. lol   \n",
       "8                                                                                                                                                        this is a way to be to be to be a great way to   \n",
       "9                                                                                                                                                             (feels: - humidity: 88% - block of words!   \n",
       "10                            i am so i am the and i have a lot of we are so i'm they are a lot of people are many people are to be &amp; if they are some and itâ€™s be some and they are they get up on   \n",
       "11                                                                                                                                                                           lil a fronting nigga nigga   \n",
       "12                                                                                                                                                          iâ€™m gonna get a fuck back to get to london.   \n",
       "13                                                                                                                                                                   in the years... i donâ€™t know it to   \n",
       "14                                                                                                                                                                        who who match leafs/wood play   \n",
       "15                                               every morning to yearsâ€”or on whichever and going to get on 4 and if it are going to do in the ownâ€”the run. and do in the other way to do the negro and   \n",
       "16                                                                                                                                                                                          why am i am   \n",
       "17                                                                                                        washington are high high commit. commit. beast @ georgia's pointe congressional congressional   \n",
       "18                                                                                                                                       iâ€™m so so you you and i want to be and you canâ€™t joyðŸ¥°ðŸ¥°ðŸ¥° you!!!   \n",
       "19             this is i just a casting ast. is redbox redbox redbox if i am sooo sooo thank you feel so iâ€™m like iâ€™m suspenseful &amp; suspenseful &amp; very suspenseful &amp; very effects. effects.   \n",
       "20                                                                   see our latest #job and click to apply: software engineer - engineer - #kellyjobs #kellyservices #kellyservices #hiring #careerarc   \n",
       "21                                                                                                          see our latest #job and click to apply: software engineer - #engineering #hiring #careerarc   \n",
       "22                                                                                                                                                     i have the bestest friends so much friends ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚   \n",
       "23  ohio state has sec has sec michigan state state has not ohio state is the capability to the state has not the site &amp; not not not to the cost. if if you go in the exhibitor &amp; you go in the   \n",
       "24                                                                                                                                              iâ€™m going up and the trophy will be the way it belongs.   \n",
       "25                                                                                                                                                         and it out and iâ€™m going to be to be mochi!ðŸ¤¤   \n",
       "26                                                                                                           he was a only has that was a way to the way to see the way about why if you know about it.   \n",
       "27                                                                                                                                                                                           poking the   \n",
       "28                                                                                                                                 if if u go to ask if me to be and if i donâ€™t be in my #kellyservices   \n",
       "29                                                                                                                                                                                            no cap ðŸ‘ŽðŸ¾   \n",
       "..                                                                                                                                                                                                  ...   \n",
       "70                                                                                                                                                               all you are all the most we have to do   \n",
       "71                                                                                                                                                    thanks for a great team for gower and 18u and 12u   \n",
       "72                                                                                               see our latest #job and click to apply: software engineer - engineer - #engineering #hiring #careerarc   \n",
       "73                                                                               my morning is my phone is i go to sit up and i sit on 4 hours and iâ€™m up and get up and get up and get to be punk lmao   \n",
       "74                                                                  if if not not not the victim is a positive if you is a or if you is a new \\r\\n3/5/1770 or the great or or your app or my ipad &amp;   \n",
       "75                                                                            i think about my rap ast. or but... iâ€™m diff or diff or the phone or why is why i know up on iâ€™m so iâ€™m about the shit on   \n",
       "76                                                                                                                                                  iâ€™m some shit about iâ€™m up to shit i donâ€™t think it   \n",
       "77                                                                                                                     dr. nurbakhsh, of america of his of the sufi path, path, path: path: practices.â€   \n",
       "78                                                                                                                                                             thatâ€™s iâ€™m so i was so i was so that was   \n",
       "79                                                                                                                                                                       iâ€™m up in 5 in 5 this day this   \n",
       "80                                                                                                                                                             iâ€™m iâ€™m iâ€™m going the the way it starts.   \n",
       "81                                                                                                                                                                        she she has at the top is now   \n",
       "82                                                     thatâ€™s you're shane. if you donâ€™t bump no music. in you in the journalist in the journalist in i can't even no blocked in adore here oh \\r\\n\\r\\n   \n",
       "83                                                                                                                                         we're #hiring! you're welcome. #rctid\\r\\n\\r\\n \\r\\n\\r\\n#rctid   \n",
       "84                                                                                                                                                              but thatâ€™s iâ€™m iâ€™m like about that that   \n",
       "85                                                                          can you recommend anyone for this #job? #job - associate - #kellyjobs #kellyservices #kellyservices #generalscience #hiring   \n",
       "86                                                                         i got this on my car and i got a phone and i got a beto years and i have been been on my but i have been a few sticker and i   \n",
       "87                                                                                                                                                                                  what was my face of   \n",
       "88                                                                                                                                                  itâ€™s 1:02am est on today and itâ€™s so itâ€™s so $500 ðŸ˜‚   \n",
       "89                                                                                                                                                                                     thank thank you!   \n",
       "90                                                                                                     only only i have been about in i donâ€™t even the phone on why i know up on the shit on my shit on   \n",
       "91                                                                                                                                                                                just just wanna sleep   \n",
       "92                                                                                                        join the team! see our latest #job opening here: latest #job opening here: #hiring #careerarc   \n",
       "93                                                                                                                                                                  itâ€™s a lot is why we just the world   \n",
       "94                                                                                                             if you're looking for work in ca, check out this #job: #manufacturing #job #jobs #hiring   \n",
       "95                                                                                                          join the team! see our latest #job opening here: #hospitality #job #jobs #hiring #careerarc   \n",
       "96                                                                                                                                                                   this explains a lot, i can do this   \n",
       "97                                                                                                                                                                          the cyrkle - - rubber - - -   \n",
       "98                                                                                                                                                                                        when on on on   \n",
       "99                                                                                                                                                when you really good good good good good for damn lol   \n",
       "\n",
       "   Decoded Prediction  Decoded Certainty  \n",
       "0       san francisco           0.119070  \n",
       "1       oklahoma city           0.143612  \n",
       "2             chicago           0.142153  \n",
       "3       oklahoma city           0.155811  \n",
       "4       oklahoma city           0.188428  \n",
       "5         los angeles           0.109619  \n",
       "6             toronto           0.129498  \n",
       "7           charlotte           0.145585  \n",
       "8       oklahoma city           0.108641  \n",
       "9             houston           0.135278  \n",
       "10      oklahoma city           0.156384  \n",
       "11         washington           0.143658  \n",
       "12      oklahoma city           0.175602  \n",
       "13      san francisco           0.184836  \n",
       "14            seattle           0.142713  \n",
       "15        los angeles           0.126105  \n",
       "16      oklahoma city           0.152493  \n",
       "17          charlotte           0.203179  \n",
       "18      oklahoma city           0.167463  \n",
       "19      oklahoma city           0.151871  \n",
       "20         cincinnati           0.132466  \n",
       "21         cincinnati           0.129162  \n",
       "22            houston           0.152073  \n",
       "23         cincinnati           0.253640  \n",
       "24         cincinnati           0.197499  \n",
       "25         cincinnati           0.162506  \n",
       "26      oklahoma city           0.155652  \n",
       "27        los angeles           0.129868  \n",
       "28      oklahoma city           0.189069  \n",
       "29      oklahoma city           0.136320  \n",
       "..                ...                ...  \n",
       "70      oklahoma city           0.113879  \n",
       "71      oklahoma city           0.148155  \n",
       "72         cincinnati           0.134660  \n",
       "73            houston           0.112129  \n",
       "74      oklahoma city           0.197050  \n",
       "75      oklahoma city           0.111086  \n",
       "76         cincinnati           0.133098  \n",
       "77           new york           0.179254  \n",
       "78      oklahoma city           0.174936  \n",
       "79         cincinnati           0.147700  \n",
       "80         cincinnati           0.143666  \n",
       "81      oklahoma city           0.190473  \n",
       "82      oklahoma city           0.116386  \n",
       "83            seattle           0.114537  \n",
       "84      oklahoma city           0.148519  \n",
       "85         cincinnati           0.128213  \n",
       "86        los angeles           0.123459  \n",
       "87      san francisco           0.120023  \n",
       "88      san francisco           0.097156  \n",
       "89          charlotte           0.125777  \n",
       "90      san francisco           0.143606  \n",
       "91      oklahoma city           0.174238  \n",
       "92          nashville           0.149956  \n",
       "93      oklahoma city           0.127807  \n",
       "94            chicago           0.142153  \n",
       "95          nashville           0.160634  \n",
       "96          charlotte           0.110023  \n",
       "97           new york           0.207053  \n",
       "98      oklahoma city           0.119652  \n",
       "99      san francisco           0.137117  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "pd.DataFrame.from_dict(decoder_results)[['Source', 'Decoded', 'Decoded Prediction', 'Decoded Certainty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-85a6079a2c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "encode = encoder_model.predict(X_test[[0]])\n",
    "encode[1:][:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# 1Bi x 1LSTM\n",
    "# 9m params with 1000 words\n",
    "\n",
    "# define training encoder\n",
    "encoder_inputs = Input(shape=(None, ), name=\"encoder_input\")\n",
    "encoder = Embedding(V, embedding_vector_length, weights=[embedding_matrix], trainable=True, \n",
    "                    name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(CuDNNLSTM(H, return_state=True), name=\"encoder_lstm_1\")(encoder)\n",
    "state_h = Concatenate(name=\"concatenate_state_h\")([forward_h, backward_h])\n",
    "state_c = Concatenate(name=\"concatenate_state_c\")([forward_c, backward_c])\n",
    "encoder_dropout = Dropout(0.4)\n",
    "encoder_dense = Dense(num_classes, activation='softmax', name=\"encoder_dense\")\n",
    "encoder_outputs = encoder_dropout(encoder_outputs)\n",
    "encoder_outputs = encoder_dense(encoder_outputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# define training decoder\n",
    "decoder_inputs = Input(shape=(None, ), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(V, embedding_vector_length, name=\"decoder_embedding\")\n",
    "embedded_input = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = CuDNNLSTM(H*2, return_sequences=True, return_state=True, name=\"decoder_lstm_1\")\n",
    "decoder_outputs, _, _ = decoder_lstm(embedded_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(V, activation='softmax', name=\"decoder_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Combine training inputs into a single training model\n",
    "model = Model([encoder_inputs, decoder_inputs], [encoder_outputs, decoder_outputs])\n",
    "\n",
    "# define inference encoder\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "\n",
    "# define inference decoder\n",
    "decoder_state_input_h = Input(shape=(H*2,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = Input(shape=(H*2,), name=\"decoder_state_input_c\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_input_2 = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_input_2, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
