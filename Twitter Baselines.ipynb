{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>albuquerque</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>billings</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calgary</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charlotte</th>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicago</th>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cincinnati</th>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denver</th>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>houston</th>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas city</th>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>las vegas</th>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>los angeles</th>\n",
       "      <td>1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minneapolis</th>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>montreal</th>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nashville</th>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york</th>\n",
       "      <td>2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oklahoma city</th>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phoenix</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pittsburgh</th>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san francisco</th>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tampa</th>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toronto</th>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington</th>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text\n",
       "region             \n",
       "albuquerque     259\n",
       "billings         95\n",
       "calgary          16\n",
       "charlotte      1126\n",
       "chicago        1030\n",
       "cincinnati      797\n",
       "denver          316\n",
       "houston        1887\n",
       "kansas city     465\n",
       "las vegas       335\n",
       "los angeles    1759\n",
       "minneapolis     339\n",
       "montreal        242\n",
       "nashville      1229\n",
       "new york       2368\n",
       "oklahoma city   932\n",
       "phoenix         332\n",
       "pittsburgh      648\n",
       "san francisco   930\n",
       "seattle         644\n",
       "tampa          1193\n",
       "toronto         704\n",
       "washington     1100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv(x):\n",
    "    try:\n",
    "        return x.astype(np.int64)\n",
    "    except:\n",
    "        return 99\n",
    "\n",
    "df_twitter = pd.read_csv('labelled_tweet_locations.csv')\n",
    "df_twitter.dropna(inplace=True)\n",
    "#df_twitter['region'] = df_twitter['region'].astype(np.int64)\n",
    "\n",
    "df_counts = df_twitter.groupby('region').count()\n",
    "top_category_num = max(df_counts['text'])\n",
    "top_category_name = df_counts[df_counts['text']==max(df_counts['text'])].index[0]\n",
    "\n",
    "categories = df_counts.index.tolist()\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  If we just guessed 'new york' every time we would have accuracy of 12.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy:  If we just guessed '{}' every time we would have accuracy of {:.2f}%\"\n",
    "      .format(top_category_name, (top_category_num/df_twitter.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 14059 examples in 23 categories, test set has 4687 examples\n"
     ]
    }
   ],
   "source": [
    "X = df_twitter['text'].tolist()\n",
    "y = df_twitter['region'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Training set has {} examples in {} categories, test set has {} examples\".format(len(X_train), len(np.unique(y_train)), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22,828 unique words in the vocabulary set, averaging 13 words per example.\n",
      "   0.0006 of the entries in the matrix are non-zero.\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "print(\"There are {:,} unique words in the vocabulary set, averaging {:.0f} words per example.\"\n",
    "      .format(train_vocab.shape[1], train_vocab.nnz/train_vocab.shape[0]))\n",
    "print(\"   {:.4f} of the entries in the matrix are non-zero.\"\n",
    "     .format(train_vocab.nnz/(train_vocab.shape[1]*train_vocab.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=0.001: 0.1550, accuracy: 16.02% \n",
      "F1 score for alpha=0.01: 0.1572, accuracy: 16.17% \n",
      "F1 score for alpha=0.1: 0.1657, accuracy: 17.43% \n",
      "F1 score for alpha=0.5: 0.1597, accuracy: 18.41% \n",
      "F1 score for alpha=1.0: 0.1381, accuracy: 17.28% \n",
      "F1 score for alpha=2.0: 0.1100, accuracy: 15.38% \n",
      "F1 score for alpha=3.0: 0.0992, accuracy: 14.70% \n",
      "F1 score for alpha=5.0: 0.0795, accuracy: 13.74% \n",
      "F1 score for alpha=10.0: 0.0641, accuracy: 12.93% \n",
      "Best alpha parameter found in test results: 0.1, returns an f1 score of 0.1657 \n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "amax = [0, 0]\n",
    "# Fit a MNB model for each value of alpha\n",
    "for a in alpha_values:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, y_train)\n",
    "    mnb_predicted_labels = mnb.predict(test_vocab)\n",
    "    mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "    mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each alpha level\n",
    "    print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(a, mnb_f1, mnb_acc*100))\n",
    "    # Keep track of which alpha value results in the highest accuracy\n",
    "    if mnb_f1 > amax[1]:\n",
    "        amax = [a, mnb_f1]    \n",
    "# Print the optimal alpha value\n",
    "print(\"Best alpha parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(amax[0], amax[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=0.1: 0.1306, accuracy: 14.08% \n"
     ]
    }
   ],
   "source": [
    "# Define a bigram vocabulary\n",
    "vec_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "train_vocab_b = vec_bigram.fit_transform(X_train)\n",
    "test_vocab_b = vec_bigram.transform(X_test)\n",
    "\n",
    "# Fit vocabulary to a Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB(alpha=amax[0])\n",
    "mnb.fit(train_vocab_b, y_train)\n",
    "mnb_predicted_labels = mnb.predict(test_vocab_b)\n",
    "mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each alpha level\n",
    "print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(amax[0], mnb_f1, mnb_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=0.01: 0.0933, accuracy: 14.02% \n",
      "F1 score for C=0.1: 0.1645, accuracy: 18.99% \n",
      "F1 score for C=0.3: 0.1858, accuracy: 19.93% \n",
      "F1 score for C=0.5: 0.1859, accuracy: 19.54% \n",
      "F1 score for C=1.0: 0.1818, accuracy: 18.90% \n",
      "F1 score for C=2.0: 0.1802, accuracy: 18.54% \n",
      "F1 score for C=3.0: 0.1775, accuracy: 18.22% \n",
      "Best C parameter found in test results: 0.5, returns an f1 score of 0.1859 \n"
     ]
    }
   ],
   "source": [
    "cmax = [0, 0]\n",
    "c_values = [0.010, 0.1000, 0.3000, 0.5000, 1.000, 2.000, 3.0]\n",
    "\n",
    "# Fit a LR model for each value of C\n",
    "for c in c_values:\n",
    "    log = LogisticRegression(C=c, penalty='l2', random_state=42, solver='lbfgs', max_iter=3000, multi_class='multinomial')\n",
    "    log.fit(train_vocab, y_train)\n",
    "    log_predicted_labels = log.predict(test_vocab)\n",
    "    log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "    log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each value of C\n",
    "    print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(c, log_f1, log_acc*100))\n",
    "    # Keep track of which C value results in the highest accuracy\n",
    "    if log_f1 > cmax[1]:\n",
    "        cmax = [c, log_f1]  \n",
    "\n",
    "# Print the optimal C value\n",
    "print(\"Best C parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(cmax[0], cmax[1]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=0.5: 0.1357, accuracy: 16.19% \n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(C=cmax[0], penalty='l2', random_state=42, solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "log.fit(train_vocab_b, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab_b)\n",
    "log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each value of C\n",
    "print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(cmax[0], log_f1, log_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    rs = s.lower()\n",
    "    # Replace some separators with spaces\n",
    "    rs = re.sub('\\n|-|/|\\.', ' ', rs)\n",
    "    # Eliminate everything else that isn't a letter or number\n",
    "    rs = re.sub('[^0-9a-z ]+', '', rs)\n",
    "    # Eliminate extraneous spaces\n",
    "    rs = re.sub('\\s{2,}', ' ', rs)\n",
    "    prs = []\n",
    "    # Drop some low-value words\n",
    "    dumbwords = ['is', 'it', 'to', 'the', 'and', 'not', 'no', 'on', 'of', 'for', 'as', 'by', 'in', 'by', 'am', 'etc', \\\n",
    "                 'was', 'that', 'has', 'at', 'or', 'we', 'be', 'had']\n",
    "    for word in rs.split():\n",
    "        # Eliminate the -ing and -ly suffices\n",
    "        word = word[:-3] if word[-3:]=='ing' and len(word) > 5 else word\n",
    "        word = word[:-2] if word[-2:]=='ly' and len(word) > 5 else word\n",
    "        # Trim words to 9 characters\n",
    "        word = word[:9] if len(word) > 9 else word\n",
    "        # Eliminate single-character words\n",
    "        if len(word) > 1 and word not in dumbwords:\n",
    "            prs.append(word)\n",
    "    \n",
    "    return \" \".join(prs)\n",
    "\n",
    "proc_train_data = [better_preprocessor(x) for x in X_train]\n",
    "proc_test_data = [better_preprocessor(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed: 22,828 words with accuracy: 0.1954\n",
      "Pre-processed: 21,719 words with accuracy: 0.1931\n",
      "Improvement: -0.0023\n",
      "\n",
      "Sample wrong answer from the preprocessed set, post #187:\n",
      "unprocessed prediction: tampa\n",
      "preprocessed prediction: tampa\n",
      "true label: chicago\n",
      "true data:  id like know more about heard he hid out southern state\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a baseline vectorizer\n",
    "vec = CountVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Make a preprocessed vectorizer\n",
    "vec_proc = CountVectorizer(preprocessor=better_preprocessor)\n",
    "vocab_proc = vec_proc.fit_transform(X_train)\n",
    "test_vocab_proc = vec_proc.transform(X_test)\n",
    "\n",
    "# Fit and predict the baseline\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(vocab, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab)\n",
    "log_score = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Fit and predict the pre-processed set\n",
    "log_proc = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log_proc.fit(vocab_proc, y_train)\n",
    "log_proc_predicted_labels = log_proc.predict(test_vocab_proc)\n",
    "log_proc_score = metrics.accuracy_score(y_test, log_proc_predicted_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Unprocessed: {:,} words with accuracy: {:.4f}\\nPre-processed: {:,} words with accuracy: {:.4f}\"\n",
    "      .format(vocab.shape[1], log_score, vocab_proc.shape[1], log_proc_score))\n",
    "print(\"Improvement: {:.4f}\".format(log_proc_score-log_score))\n",
    "\n",
    "# Find a wrong answer and print it out for better analysis\n",
    "wrong = np.random.choice(np.where(y_test != log_proc_predicted_labels)[0].ravel())\n",
    "\n",
    "print(\"\\nSample wrong answer from the preprocessed set, post #{}:\".format(wrong))\n",
    "print(\"unprocessed prediction: {}\".format(log_predicted_labels[wrong]))\n",
    "print(\"preprocessed prediction: {}\".format(log_proc_predicted_labels[wrong]))\n",
    "print(\"true label: {}\".format(y_test[wrong]))\n",
    "print(\"true data: \",proc_test_data[wrong])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 for TfidfVectorizer: 0.17, accuracy 17.26%\n",
      "\n",
      "TOP 3 MISIDENTIFIED DOCUMENTS:\n",
      "DOCUMENT #1\n",
      "Predicted label: new york (P99.7%), True label: nashville (P0.0%)\n",
      "R ratio: 285295.66\n",
      "Looking for a good checking alternative to  . Suggestions?\n",
      "----\n",
      "\n",
      "DOCUMENT #2\n",
      "Predicted label: los angeles (P98.9%), True label: seattle (P0.0%)\n",
      "R ratio: 341757.33\n",
      "Send some to California\n",
      "----\n",
      "\n",
      "DOCUMENT #3\n",
      "Predicted label: charlotte (P99.5%), True label: houston (P0.0%)\n",
      "R ratio: 897102.93\n",
      "This guy is taking full responsibility of his Dreambox journey! He is very consistent on seeing how many standards he is meeting each week!   😍 \n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a TFIDF Vectorizer and fit the training and dev vocabularies\n",
    "vec = TfidfVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Inverse vocabulary dictionary for word lookup\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "# Fit and predict a logistic regression based on the results\n",
    "lr = LogisticRegression(C=100, solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "lr.fit(vocab, y_train)\n",
    "lr_predicted_labels = lr.predict(test_vocab)\n",
    "# Calculate and print the resulting score\n",
    "lr_score = metrics.f1_score(y_test, lr_predicted_labels, average='weighted')\n",
    "lr_acc = metrics.accuracy_score(y_test, lr_predicted_labels)\n",
    "\n",
    "print(\"Baseline F1 for TfidfVectorizer: {:.2f}, accuracy {:.2f}%\\n\".format(lr_score, lr_acc*100))\n",
    "\n",
    "# Get the probabilities for each class prediction\n",
    "probs = lr.predict_proba(test_vocab)\n",
    "R = []\n",
    "# Run through the probabilities and calculate the R ratio as defined in the prompt, saving the value in the R list\n",
    "for x in range(0, len(probs)):\n",
    "    num = np.max(probs[x])\n",
    "    den = probs[x][np.unique(y_test).tolist().index(y_test[x])]\n",
    "    R.append(num/den)\n",
    "# Get the highest x number of R values\n",
    "top = np.argsort(np.array(R))[len(R)-3:]\n",
    "\n",
    "# Print the top misidentified documents as well as their TFIDF score and coefficients by class\n",
    "print(\"TOP {} MISIDENTIFIED DOCUMENTS:\".format(3))\n",
    "c = 1\n",
    "for i in top:\n",
    "    print(\"DOCUMENT #{}\".format(c))\n",
    "    print(\"Predicted label: {} (P{:.1f}%), True label: {} (P{:.1f}%)\"\n",
    "          .format(lr_predicted_labels[i], np.max(probs[i])*100, y_test[i], probs[i][categories.index(y_test[1])]*100))\n",
    "    print(\"R ratio: {:.2f}\".format(R[i]))\n",
    "    print(X_test[i])\n",
    "    '''\n",
    "    print(\"\\n{:10} {:>10} {:>15} {:>15} {:>15} {:>22} \".format(\"word\", \"Tfidf\", categories[0], categories[1], \\\n",
    "                                                               categories[2], categories[3]))\n",
    "    for w in np.nonzero(dev_vocab[i])[1]:\n",
    "        coefs = np.round(lr.coef_[:,w], 2).flat\n",
    "        print(\"{:10} {:10.3f} {:>15} {:>15} {:>15} {:>22}\".format(inv_vocab[w], dev_vocab[i][0,w], \\\n",
    "                                                                  coefs[0], coefs[1], coefs[2], coefs[3])\n",
    "    '''\n",
    "    print(\"----\\n\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albuquerque</th>\n",
       "      <th>billings</th>\n",
       "      <th>calgary</th>\n",
       "      <th>charlotte</th>\n",
       "      <th>chicago</th>\n",
       "      <th>cincinnati</th>\n",
       "      <th>denver</th>\n",
       "      <th>houston</th>\n",
       "      <th>kansas city</th>\n",
       "      <th>las vegas</th>\n",
       "      <th>...</th>\n",
       "      <th>nashville</th>\n",
       "      <th>new york</th>\n",
       "      <th>oklahoma city</th>\n",
       "      <th>phoenix</th>\n",
       "      <th>pittsburgh</th>\n",
       "      <th>san francisco</th>\n",
       "      <th>seattle</th>\n",
       "      <th>tampa</th>\n",
       "      <th>toronto</th>\n",
       "      <th>washington</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm</td>\n",
       "      <td>mt</td>\n",
       "      <td>id</td>\n",
       "      <td>nc</td>\n",
       "      <td>il</td>\n",
       "      <td>ohio</td>\n",
       "      <td>co</td>\n",
       "      <td>tx</td>\n",
       "      <td>mo</td>\n",
       "      <td>vegas</td>\n",
       "      <td>...</td>\n",
       "      <td>al</td>\n",
       "      <td>ct</td>\n",
       "      <td>dallas</td>\n",
       "      <td>arizona</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>ca</td>\n",
       "      <td>wa</td>\n",
       "      <td>fl</td>\n",
       "      <td>toronto</td>\n",
       "      <td>va</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tx</td>\n",
       "      <td>id</td>\n",
       "      <td>accounting</td>\n",
       "      <td>sc</td>\n",
       "      <td>wi</td>\n",
       "      <td>ky</td>\n",
       "      <td>colorado</td>\n",
       "      <td>houston</td>\n",
       "      <td>ne</td>\n",
       "      <td>ut</td>\n",
       "      <td>...</td>\n",
       "      <td>tn</td>\n",
       "      <td>nj</td>\n",
       "      <td>tx</td>\n",
       "      <td>az</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>nv</td>\n",
       "      <td>portland</td>\n",
       "      <td>florida</td>\n",
       "      <td>ny</td>\n",
       "      <td>md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progress</td>\n",
       "      <td>wy</td>\n",
       "      <td>with</td>\n",
       "      <td>carolina</td>\n",
       "      <td>chicago</td>\n",
       "      <td>louisville</td>\n",
       "      <td>denver</td>\n",
       "      <td>la</td>\n",
       "      <td>ia</td>\n",
       "      <td>nv</td>\n",
       "      <td>...</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>ny</td>\n",
       "      <td>ar</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>pa</td>\n",
       "      <td>california</td>\n",
       "      <td>id</td>\n",
       "      <td>miami</td>\n",
       "      <td>mi</td>\n",
       "      <td>maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does</td>\n",
       "      <td>great</td>\n",
       "      <td>mt</td>\n",
       "      <td>ga</td>\n",
       "      <td>milwaukee</td>\n",
       "      <td>indianapolis</td>\n",
       "      <td>trade</td>\n",
       "      <td>winniesun</td>\n",
       "      <td>ks</td>\n",
       "      <td>nut</td>\n",
       "      <td>...</td>\n",
       "      <td>ms</td>\n",
       "      <td>ma</td>\n",
       "      <td>ok</td>\n",
       "      <td>native</td>\n",
       "      <td>oh</td>\n",
       "      <td>oakland</td>\n",
       "      <td>seattle</td>\n",
       "      <td>orlando</td>\n",
       "      <td>ontario</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mexico</td>\n",
       "      <td>needs</td>\n",
       "      <td>hiring</td>\n",
       "      <td>va</td>\n",
       "      <td>michigan</td>\n",
       "      <td>marion</td>\n",
       "      <td>salt</td>\n",
       "      <td>austin</td>\n",
       "      <td>nebraska</td>\n",
       "      <td>utah</td>\n",
       "      <td>...</td>\n",
       "      <td>ga</td>\n",
       "      <td>boston</td>\n",
       "      <td>texas</td>\n",
       "      <td>americans</td>\n",
       "      <td>number</td>\n",
       "      <td>sf</td>\n",
       "      <td>vancouver</td>\n",
       "      <td>beach</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>number</td>\n",
       "      <td>too</td>\n",
       "      <td>summer</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>mi</td>\n",
       "      <td>comes</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>texas</td>\n",
       "      <td>desmoines</td>\n",
       "      <td>mountain</td>\n",
       "      <td>...</td>\n",
       "      <td>nashville</td>\n",
       "      <td>york</td>\n",
       "      <td>landscapechat</td>\n",
       "      <td>fine</td>\n",
       "      <td>wv</td>\n",
       "      <td>relax</td>\n",
       "      <td>exciting</td>\n",
       "      <td>disney</td>\n",
       "      <td>daughter</td>\n",
       "      <td>baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ruthless</td>\n",
       "      <td>careerarc</td>\n",
       "      <td>godaddy</td>\n",
       "      <td>ncat</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>wy</td>\n",
       "      <td>sanantonio</td>\n",
       "      <td>iowa</td>\n",
       "      <td>booth</td>\n",
       "      <td>...</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>brooklyn</td>\n",
       "      <td>weeks</td>\n",
       "      <td>costume</td>\n",
       "      <td>watch</td>\n",
       "      <td>sacramento</td>\n",
       "      <td>election</td>\n",
       "      <td>single</td>\n",
       "      <td>lmfao</td>\n",
       "      <td>washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>beto</td>\n",
       "      <td>idaho</td>\n",
       "      <td>ssl</td>\n",
       "      <td>congrats</td>\n",
       "      <td>illinois</td>\n",
       "      <td>kickback</td>\n",
       "      <td>decisions</td>\n",
       "      <td>fw</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>turned</td>\n",
       "      <td>...</td>\n",
       "      <td>mississippi</td>\n",
       "      <td>ri</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>november</td>\n",
       "      <td>bill</td>\n",
       "      <td>slap</td>\n",
       "      <td>nampa</td>\n",
       "      <td>kingdom</td>\n",
       "      <td>cry</td>\n",
       "      <td>happened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alright</td>\n",
       "      <td>set</td>\n",
       "      <td>certificate</td>\n",
       "      <td>argue</td>\n",
       "      <td>lafayette</td>\n",
       "      <td>dang</td>\n",
       "      <td>spell</td>\n",
       "      <td>weeks</td>\n",
       "      <td>kinda</td>\n",
       "      <td>hotel</td>\n",
       "      <td>...</td>\n",
       "      <td>ky</td>\n",
       "      <td>nyc</td>\n",
       "      <td>tulsa</td>\n",
       "      <td>forward</td>\n",
       "      <td>akron</td>\n",
       "      <td>memories</td>\n",
       "      <td>test</td>\n",
       "      <td>easy</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>away</td>\n",
       "      <td>wow</td>\n",
       "      <td>cad</td>\n",
       "      <td>cousin</td>\n",
       "      <td>cedarrapids</td>\n",
       "      <td>proud</td>\n",
       "      <td>coloradosprings</td>\n",
       "      <td>growing</td>\n",
       "      <td>somebody</td>\n",
       "      <td>sema2018</td>\n",
       "      <td>...</td>\n",
       "      <td>memphis</td>\n",
       "      <td>pa</td>\n",
       "      <td>laughing</td>\n",
       "      <td>morning</td>\n",
       "      <td>philly</td>\n",
       "      <td>anime</td>\n",
       "      <td>oregon</td>\n",
       "      <td>tampa</td>\n",
       "      <td>union</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>car</td>\n",
       "      <td>twinfalls</td>\n",
       "      <td>mcguire</td>\n",
       "      <td>doubt</td>\n",
       "      <td>ia</td>\n",
       "      <td>columbus</td>\n",
       "      <td>99</td>\n",
       "      <td>shreveport</td>\n",
       "      <td>blunt</td>\n",
       "      <td>lasvegas</td>\n",
       "      <td>...</td>\n",
       "      <td>knoxville</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>hoes</td>\n",
       "      <td>goddamn</td>\n",
       "      <td>commit</td>\n",
       "      <td>san</td>\n",
       "      <td>wouldn</td>\n",
       "      <td>nah</td>\n",
       "      <td>oprah</td>\n",
       "      <td>smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>albuquerque</td>\n",
       "      <td>but</td>\n",
       "      <td>melnick</td>\n",
       "      <td>haven</td>\n",
       "      <td>survived</td>\n",
       "      <td>wv</td>\n",
       "      <td>definitely</td>\n",
       "      <td>aint</td>\n",
       "      <td>bro</td>\n",
       "      <td>kind</td>\n",
       "      <td>...</td>\n",
       "      <td>mind</td>\n",
       "      <td>newyork</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>boom</td>\n",
       "      <td>bday</td>\n",
       "      <td>doesn</td>\n",
       "      <td>samhain</td>\n",
       "      <td>business</td>\n",
       "      <td>canadian</td>\n",
       "      <td>columbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eat</td>\n",
       "      <td>manager</td>\n",
       "      <td>scotty</td>\n",
       "      <td>bestfriend</td>\n",
       "      <td>goodness</td>\n",
       "      <td>fucked</td>\n",
       "      <td>dark</td>\n",
       "      <td>hole</td>\n",
       "      <td>tv</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>glasses</td>\n",
       "      <td>incident</td>\n",
       "      <td>lil</td>\n",
       "      <td>herself</td>\n",
       "      <td>story</td>\n",
       "      <td>bay</td>\n",
       "      <td>boise</td>\n",
       "      <td>promotion</td>\n",
       "      <td>contest</td>\n",
       "      <td>pull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>round</td>\n",
       "      <td>trump</td>\n",
       "      <td>impossible</td>\n",
       "      <td>yo</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>oh</td>\n",
       "      <td>red</td>\n",
       "      <td>raining</td>\n",
       "      <td>moving</td>\n",
       "      <td>am</td>\n",
       "      <td>...</td>\n",
       "      <td>pull</td>\n",
       "      <td>jersey</td>\n",
       "      <td>raining</td>\n",
       "      <td>thanksgiving</td>\n",
       "      <td>mi</td>\n",
       "      <td>carsoncity</td>\n",
       "      <td>tweets</td>\n",
       "      <td>ft</td>\n",
       "      <td>mine</td>\n",
       "      <td>nah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roswell</td>\n",
       "      <td>again</td>\n",
       "      <td>jordana</td>\n",
       "      <td>madea</td>\n",
       "      <td>dollar</td>\n",
       "      <td>course</td>\n",
       "      <td>lyft</td>\n",
       "      <td>purchase</td>\n",
       "      <td>voteblueforhumanity</td>\n",
       "      <td>kinda</td>\n",
       "      <td>...</td>\n",
       "      <td>square</td>\n",
       "      <td>chain</td>\n",
       "      <td>campaign</td>\n",
       "      <td>help</td>\n",
       "      <td>virginia</td>\n",
       "      <td>amzn</td>\n",
       "      <td>maybe</td>\n",
       "      <td>joke</td>\n",
       "      <td>happyhalloween</td>\n",
       "      <td>pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>texas</td>\n",
       "      <td>isn</td>\n",
       "      <td>careerarc</td>\n",
       "      <td>ghoe</td>\n",
       "      <td>middle</td>\n",
       "      <td>shut</td>\n",
       "      <td>does</td>\n",
       "      <td>sleepy</td>\n",
       "      <td>money</td>\n",
       "      <td>sema</td>\n",
       "      <td>...</td>\n",
       "      <td>alabama</td>\n",
       "      <td>broadway</td>\n",
       "      <td>wichita</td>\n",
       "      <td>25</td>\n",
       "      <td>tell</td>\n",
       "      <td>francisco</td>\n",
       "      <td>trumps</td>\n",
       "      <td>nation</td>\n",
       "      <td>photos</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>relationship</td>\n",
       "      <td>missoula</td>\n",
       "      <td>generation</td>\n",
       "      <td>goes</td>\n",
       "      <td>richland</td>\n",
       "      <td>huntington</td>\n",
       "      <td>every</td>\n",
       "      <td>bday</td>\n",
       "      <td>isn</td>\n",
       "      <td>yeah</td>\n",
       "      <td>...</td>\n",
       "      <td>netflix</td>\n",
       "      <td>saving</td>\n",
       "      <td>opener</td>\n",
       "      <td>won</td>\n",
       "      <td>rn</td>\n",
       "      <td>fresno</td>\n",
       "      <td>kill</td>\n",
       "      <td>phone</td>\n",
       "      <td>london</td>\n",
       "      <td>howard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cheese</td>\n",
       "      <td>people</td>\n",
       "      <td>sandpoint</td>\n",
       "      <td>civic</td>\n",
       "      <td>grill</td>\n",
       "      <td>dublin</td>\n",
       "      <td>math</td>\n",
       "      <td>playin</td>\n",
       "      <td>holding</td>\n",
       "      <td>shoot</td>\n",
       "      <td>...</td>\n",
       "      <td>bottle</td>\n",
       "      <td>massachusetts</td>\n",
       "      <td>omg</td>\n",
       "      <td>writing</td>\n",
       "      <td>since</td>\n",
       "      <td>cant</td>\n",
       "      <td>shoulders</td>\n",
       "      <td>yo</td>\n",
       "      <td>billion</td>\n",
       "      <td>grind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>because</td>\n",
       "      <td>transportation</td>\n",
       "      <td>apta</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>passionate</td>\n",
       "      <td>zulily</td>\n",
       "      <td>speak</td>\n",
       "      <td>post</td>\n",
       "      <td>did</td>\n",
       "      <td>14th</td>\n",
       "      <td>...</td>\n",
       "      <td>piece</td>\n",
       "      <td>owner</td>\n",
       "      <td>friday</td>\n",
       "      <td>school</td>\n",
       "      <td>players</td>\n",
       "      <td>hook</td>\n",
       "      <td>amen</td>\n",
       "      <td>lee</td>\n",
       "      <td>trickortreat</td>\n",
       "      <td>aight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lived</td>\n",
       "      <td>lot</td>\n",
       "      <td>mo</td>\n",
       "      <td>original</td>\n",
       "      <td>taxes</td>\n",
       "      <td>florida</td>\n",
       "      <td>wanted</td>\n",
       "      <td>bayou</td>\n",
       "      <td>corn</td>\n",
       "      <td>experience</td>\n",
       "      <td>...</td>\n",
       "      <td>mane</td>\n",
       "      <td>success</td>\n",
       "      <td>1st</td>\n",
       "      <td>whatever</td>\n",
       "      <td>16</td>\n",
       "      <td>recently</td>\n",
       "      <td>nationalist</td>\n",
       "      <td>girls</td>\n",
       "      <td>fake</td>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     albuquerque        billings      calgary   charlotte        chicago  \\\n",
       "0             nm              mt           id          nc             il   \n",
       "1             tx              id   accounting          sc             wi   \n",
       "2       progress              wy         with    carolina        chicago   \n",
       "3           does           great           mt          ga      milwaukee   \n",
       "4         mexico           needs       hiring          va       michigan   \n",
       "5         number             too       summer   charlotte             mi   \n",
       "6       ruthless       careerarc      godaddy        ncat      wisconsin   \n",
       "7           beto           idaho          ssl    congrats       illinois   \n",
       "8        alright             set  certificate       argue      lafayette   \n",
       "9           away             wow          cad      cousin    cedarrapids   \n",
       "10           car       twinfalls      mcguire       doubt             ia   \n",
       "11   albuquerque             but      melnick       haven       survived   \n",
       "12           eat         manager       scotty  bestfriend       goodness   \n",
       "13         round           trump   impossible          yo  unfortunately   \n",
       "14       roswell           again      jordana       madea         dollar   \n",
       "15         texas             isn    careerarc        ghoe         middle   \n",
       "16  relationship        missoula   generation        goes       richland   \n",
       "17        cheese          people    sandpoint       civic          grill   \n",
       "18       because  transportation         apta       enjoy     passionate   \n",
       "19         lived             lot           mo    original          taxes   \n",
       "\n",
       "      cincinnati           denver     houston          kansas city  \\\n",
       "0           ohio               co          tx                   mo   \n",
       "1             ky         colorado     houston                   ne   \n",
       "2     louisville           denver          la                   ia   \n",
       "3   indianapolis            trade   winniesun                   ks   \n",
       "4         marion             salt      austin             nebraska   \n",
       "5          comes        beautiful       texas            desmoines   \n",
       "6     cincinnati               wy  sanantonio                 iowa   \n",
       "7       kickback        decisions          fw         disappointed   \n",
       "8           dang            spell       weeks                kinda   \n",
       "9          proud  coloradosprings     growing             somebody   \n",
       "10      columbus               99  shreveport                blunt   \n",
       "11            wv       definitely        aint                  bro   \n",
       "12        fucked             dark        hole                   tv   \n",
       "13            oh              red     raining               moving   \n",
       "14        course             lyft    purchase  voteblueforhumanity   \n",
       "15          shut             does      sleepy                money   \n",
       "16    huntington            every        bday                  isn   \n",
       "17        dublin             math      playin              holding   \n",
       "18        zulily            speak        post                  did   \n",
       "19       florida           wanted       bayou                 corn   \n",
       "\n",
       "     las vegas     ...        nashville       new york  oklahoma city  \\\n",
       "0        vegas     ...               al             ct         dallas   \n",
       "1           ut     ...               tn             nj             tx   \n",
       "2           nv     ...          atlanta             ny             ar   \n",
       "3          nut     ...               ms             ma             ok   \n",
       "4         utah     ...               ga         boston          texas   \n",
       "5     mountain     ...        nashville           york  landscapechat   \n",
       "6        booth     ...        tennessee       brooklyn          weeks   \n",
       "7       turned     ...      mississippi             ri       oklahoma   \n",
       "8        hotel     ...               ky            nyc          tulsa   \n",
       "9     sema2018     ...          memphis             pa       laughing   \n",
       "10    lasvegas     ...        knoxville   philadelphia           hoes   \n",
       "11        kind     ...             mind        newyork       arkansas   \n",
       "12        true     ...          glasses       incident            lil   \n",
       "13          am     ...             pull         jersey        raining   \n",
       "14       kinda     ...           square          chain       campaign   \n",
       "15        sema     ...          alabama       broadway        wichita   \n",
       "16        yeah     ...          netflix         saving         opener   \n",
       "17       shoot     ...           bottle  massachusetts            omg   \n",
       "18        14th     ...            piece          owner         friday   \n",
       "19  experience     ...             mane        success            1st   \n",
       "\n",
       "         phoenix  pittsburgh san francisco      seattle      tampa  \\\n",
       "0        arizona   cleveland            ca           wa         fl   \n",
       "1             az  pittsburgh            nv     portland    florida   \n",
       "2        phoenix          pa    california           id      miami   \n",
       "3         native          oh       oakland      seattle    orlando   \n",
       "4      americans      number            sf    vancouver      beach   \n",
       "5           fine          wv         relax     exciting     disney   \n",
       "6        costume       watch    sacramento     election     single   \n",
       "7       november        bill          slap        nampa    kingdom   \n",
       "8        forward       akron      memories         test       easy   \n",
       "9        morning      philly         anime       oregon      tampa   \n",
       "10       goddamn      commit           san       wouldn        nah   \n",
       "11          boom        bday         doesn      samhain   business   \n",
       "12       herself       story           bay        boise  promotion   \n",
       "13  thanksgiving          mi    carsoncity       tweets         ft   \n",
       "14          help    virginia          amzn        maybe       joke   \n",
       "15            25        tell     francisco       trumps     nation   \n",
       "16           won          rn        fresno         kill      phone   \n",
       "17       writing       since          cant    shoulders         yo   \n",
       "18        school     players          hook         amen        lee   \n",
       "19      whatever          16      recently  nationalist      girls   \n",
       "\n",
       "           toronto  washington  \n",
       "0          toronto          va  \n",
       "1               ny          md  \n",
       "2               mi    maryland  \n",
       "3          ontario          dc  \n",
       "4         hamilton    virginia  \n",
       "5         daughter   baltimore  \n",
       "6            lmfao  washington  \n",
       "7              cry    happened  \n",
       "8          buffalo     episode  \n",
       "9            union          de  \n",
       "10           oprah         smh  \n",
       "11        canadian    columbia  \n",
       "12         contest        pull  \n",
       "13            mine         nah  \n",
       "14  happyhalloween          pa  \n",
       "15          photos    inspired  \n",
       "16          london      howard  \n",
       "17         billion       grind  \n",
       "18    trickortreat       aight  \n",
       "19            fake        deal  \n",
       "\n",
       "[20 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = 20\n",
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "# Make an inverse vocabulary to look up words by index\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(train_vocab, y_train)\n",
    "# Get the words with the highest coefficients from each class\n",
    "topwords = np.argsort(log.coef_, 1)[:, train_vocab.shape[1]-top:]\n",
    "df_topwords = pd.DataFrame()\n",
    "\n",
    "for x in range(topwords.shape[0]):\n",
    "    wordlist = [inv_vocab[x] for x in topwords[x][::-1]]\n",
    "    df_topwords[categories[x]] = wordlist\n",
    "\n",
    "df_topwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
