{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ngram_lm\n",
    "import tweets_processor\n",
    "import importlib\n",
    "import vocabulary\n",
    "import utils\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload to get the recent changes ortherwise have to restart the notebook\n",
    "importlib.reload(tweets_processor)\n",
    "# get the tweets and the region labels from csv file\n",
    "tweets_text, tweets_regions = tweets_processor.get_tweets_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into test and train\n",
    "######TODO: Split into train, dev, test -use kfold cross validation #########\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(tweets_text, tweets_regions, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processing the tweets\n",
    "def get_tokenized_tweets(tweets, mode):\n",
    "    tokenized_tweets = []\n",
    "    tokens = []\n",
    "    vocabulary_tokens = []\n",
    "    for tweet in tweets:\n",
    "        # preprocess the tweets\n",
    "        processed_tweet = tweets_processor.preprocessor(tweet)\n",
    "        # pass through tokenizer\n",
    "        tokenized_tweet = tweets_processor.tokenizer(processed_tweet)\n",
    "        #tokens without <s></s> for creating the train vocabulary\n",
    "        if mode == 'train':\n",
    "            vocabulary_tokens.extend(tokenized_tweet)\n",
    "        tokenized_tweet = [\"<s>\", \"<s>\"] + tokenized_tweet + [\"</s>\"]\n",
    "        # for each tweet get the tokens\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "        # get all the tokens for all tweets in train set\n",
    "        tokens.extend(tokenized_tweet)\n",
    "    return tokenized_tweets, tokens, vocabulary_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processing tokens to replace anything not in vocab with <unk>\n",
    "def process_tokens_unk(tokens):\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in tokens], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get train tokens and vocabulary tokens from trian data set\n",
    "train_tokenized_tweets, train_tokens, vocabulary_tokens = get_tokenized_tweets(train_data, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13711\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w,digits=False) for w in vocabulary_tokens)\n",
    "print(vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process the train_tokenized_tweets\n",
    "train_tokens = process_tokens_unk(train_tokens)\n",
    "\n",
    "processed_train_tokenized_tweets = []\n",
    "for tokenized_tweet in train_tokenized_tweets:\n",
    "    processed_unk_tweet = process_tokens_unk(tokenized_tweet)\n",
    "    processed_train_tokenized_tweets.append(processed_unk_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test tokens\n",
    "test_tokenized_tweets, test_tokens, _ = get_tokenized_tweets(test_data, 'test')\n",
    "test_tokens = process_tokens_unk(test_tokens)\n",
    "\n",
    "# process the test_tokenized_tweets\n",
    "processed_test_tokenized_tweets = []\n",
    "for tokenized_tweet in test_tokenized_tweets:\n",
    "    processed_unk_tweet = process_tokens_unk(tokenized_tweet)\n",
    "    processed_test_tokenized_tweets.append(processed_unk_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 0.36 s\n",
      "=== N-gram Language Model stats ===\n",
      "  13,710 unique 1-grams\n",
      "  48,633 unique 2-grams\n",
      "  57,948 unique 3-grams\n",
      "Optimal memory usage (counts only): 2.72 MB\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "Model = ngram_lm.KNTrigramLM\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Building trigram LM... \", end=\"\")\n",
    "lm = Model(train_tokens)\n",
    "print(\"done in {:.02f} s\".format(time.time() - t0))\n",
    "lm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/divyagorantla/Documents/MIDS/w266/final-project/utils.py'>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ngram_lm)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build feature vectors for passing to the classifier\n",
    "def get_input_features(tokenized_tweets, ngram):\n",
    "    feature_vector = []\n",
    "    for sequence in tokenized_tweets:\n",
    "        probabilities, count = lm.get_seq_probability(sequence, n_order=ngram)\n",
    "        #print(probabilities)\n",
    "        feature_vector.append(probabilities)\n",
    "    # pad probabilities with 0.0 for lengths less than 50 ??????????????what should we set this length\n",
    "    padded_feature_vector, _ = utils.pad_np_array_float(feature_vector, max_len=50, pad_id=0.0)\n",
    "    return padded_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for trigrams\n",
    "# get train fetaure vectors\n",
    "classfier_train_data = get_input_features(processed_train_tokenized_tweets, 3)\n",
    "\n",
    "# get test feature vectors\n",
    "classfier_test_data = get_input_features(processed_test_tokenized_tweets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.4030586e-02, 8.5744292e-01, 7.6563349e-03, 4.2354516e-03,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.5627249e-03, 7.5044453e-02,\n",
       "       5.7978872e-05, 1.2116993e-05, 7.7108137e-04, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.8505953e-04, 1.5421627e-05, 0.0000000e+00,\n",
       "       2.0562169e-04, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classfier_train_data[1]\n",
    "classfier_test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<s>', '<s>', 'happy', 'halloween', 'favorite', 'holiday', '<unk>',\n",
       "       '<unk>', 'happyhalloween', 'halloween', 'witch', 'blackcat',\n",
       "       'spooky', '<unk>', '<unk>', 'haunted', 'graveyard', '<unk>', 'the',\n",
       "       '</s>'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_tokenized_tweets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for bigrams\n",
    "# get train fetaure vectors\n",
    "new_classfier_train_data = get_input_features(processed_train_tokenized_tweets, 2)\n",
    "\n",
    "# get test feature vectors\n",
    "new_classfier_test_data = get_input_features(processed_test_tokenized_tweets, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.40327176e-02, 7.49111712e-01, 2.88975495e-03, 5.64726861e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.56272494e-03, 7.50444531e-02,\n",
       "       1.15957744e-04, 1.21169933e-05, 7.71081366e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.85059529e-04, 1.54216268e-05, 0.00000000e+00,\n",
       "       2.05621691e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_classfier_train_data[1]\n",
    "new_classfier_test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 13.12%\n"
     ]
    }
   ],
   "source": [
    "# using naive bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(classfier_train_data, train_labels)\n",
    "y_pred = nb.predict(classfier_test_data)\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 10.85%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "lg.fit(classfier_train_data, train_labels)\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(lg.score(classfier_test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 11.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyagorantla/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, ),)\n",
    "mlp.fit(classfier_train_data, train_labels)\n",
    "y_pred = mlp.predict(classfier_test_data)\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
