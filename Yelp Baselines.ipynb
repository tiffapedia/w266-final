{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  If we just guessed 'Las Vegas' every time we would have accuracy of 34.37%\n"
     ]
    }
   ],
   "source": [
    "def conv(x):\n",
    "    try:\n",
    "        return x.astype(np.int64)\n",
    "    except:\n",
    "        return 99\n",
    "\n",
    "df_yelp = pd.read_csv('yelp_reviews_labelled_mini.csv')\n",
    "df_yelp.dropna(inplace=True)\n",
    "df_yelp['region'] = df_yelp['region'].astype(np.int64)\n",
    "#df_yelp = df_yelp[df_yelp['region']<10]\n",
    "categories = ['Phoenix', 'Las Vegas', 'Toronto', 'Charlotte', 'Cleveland', 'Pittsburgh', \n",
    "              'Montreal', 'Calgary', 'Madison', 'Champaign']\n",
    "\n",
    "df_counts = df_yelp.groupby('region').count()\n",
    "top_category_num = max(df_counts['text'])\n",
    "top_category_name = categories[df_counts[df_counts['text']==max(df_counts['text'])].index[0]]\n",
    "\n",
    "print(\"Baseline accuracy:  If we just guessed '{}' every time we would have accuracy of {:.2f}%\"\n",
    "      .format(top_category_name, (top_category_num/df_yelp.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_yelp['text'].tolist()\n",
    "y = df_yelp['region'].tolist()\n",
    "y = list(map(int, y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Training set has {} examples in {} categories, test set has {} examples\".format(len(X_train), np.unique(y_train), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24,298 unique words in the vocabulary set, averaging 72 words per example.\n",
      "   0.0030 of the entries in the matrix are non-zero.\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "print(\"There are {:,} unique words in the vocabulary set, averaging {:.0f} words per example.\"\n",
    "      .format(train_vocab.shape[1], train_vocab.nnz/train_vocab.shape[0]))\n",
    "print(\"   {:.4f} of the entries in the matrix are non-zero.\"\n",
    "     .format(train_vocab.nnz/(train_vocab.shape[1]*train_vocab.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=0.001: 0.4561, accuracy: 47.66% \n",
      "F1 score for alpha=0.01: 0.4678, accuracy: 48.26% \n",
      "F1 score for alpha=0.1: 0.4769, accuracy: 48.94% \n",
      "F1 score for alpha=0.5: 0.4831, accuracy: 51.30% \n",
      "F1 score for alpha=1.0: 0.4463, accuracy: 49.26% \n",
      "F1 score for alpha=2.0: 0.4113, accuracy: 46.01% \n",
      "F1 score for alpha=3.0: 0.3830, accuracy: 44.33% \n",
      "F1 score for alpha=5.0: 0.3890, accuracy: 42.44% \n",
      "F1 score for alpha=10.0: 0.3626, accuracy: 38.92% \n",
      "Best alpha parameter found in test results: 0.5, returns an f1 score of 0.4831 \n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "amax = [0, 0]\n",
    "# Fit a MNB model for each value of alpha\n",
    "for a in alpha_values:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train_vocab, y_train)\n",
    "    mnb_predicted_labels = mnb.predict(test_vocab)\n",
    "    mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "    mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each alpha level\n",
    "    print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(a, mnb_f1, mnb_acc*100))\n",
    "    # Keep track of which alpha value results in the highest accuracy\n",
    "    if mnb_f1 > amax[1]:\n",
    "        amax = [a, mnb_f1]    \n",
    "# Print the optimal alpha value\n",
    "print(\"Best alpha parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(amax[0], amax[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for alpha=0.5: 0.3882, accuracy: 44.09% \n"
     ]
    }
   ],
   "source": [
    "# Define a bigram vocabulary\n",
    "vec_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "train_vocab_b = vec_bigram.fit_transform(X_train)\n",
    "test_vocab_b = vec_bigram.transform(X_test)\n",
    "\n",
    "# Fit vocabulary to a Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB(alpha=amax[0])\n",
    "mnb.fit(train_vocab_b, y_train)\n",
    "mnb_predicted_labels = mnb.predict(test_vocab_b)\n",
    "mnb_f1 = metrics.f1_score(y_test, mnb_predicted_labels, average='weighted', labels=np.unique(mnb_predicted_labels))\n",
    "mnb_acc = metrics.accuracy_score(y_test, mnb_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each alpha level\n",
    "print(\"F1 score for alpha={}: {:.4f}, accuracy: {:.2f}% \".format(amax[0], mnb_f1, mnb_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=0.01: 0.4132, accuracy: 46.41% \n",
      "F1 score for C=0.1: 0.4459, accuracy: 47.33% \n",
      "F1 score for C=0.3: 0.4450, accuracy: 46.45% \n",
      "F1 score for C=0.5: 0.4444, accuracy: 46.09% \n",
      "F1 score for C=1.0: 0.4465, accuracy: 46.01% \n",
      "F1 score for C=2.0: 0.4417, accuracy: 45.29% \n",
      "Best C parameter found in test results: 1.0, returns an f1 score of 0.4465 \n"
     ]
    }
   ],
   "source": [
    "cmax = [0, 0]\n",
    "c_values = [0.010, 0.1000, 0.3000, 0.5000, 1.000, 2.000]\n",
    "\n",
    "# Fit a LR model for each value of C\n",
    "for c in c_values:\n",
    "    log = LogisticRegression(C=c, penalty='l2', random_state=42, solver='lbfgs', max_iter=3000, multi_class='multinomial')\n",
    "    log.fit(train_vocab, y_train)\n",
    "    log_predicted_labels = log.predict(test_vocab)\n",
    "    log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "    log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "    # Print out the accuracy score for each value of C\n",
    "    print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(c, log_f1, log_acc*100))\n",
    "    # Keep track of which C value results in the highest accuracy\n",
    "    if log_f1 > cmax[1]:\n",
    "        cmax = [c, log_f1]  \n",
    "\n",
    "# Print the optimal C value\n",
    "print(\"Best C parameter found in test results: {}, returns an f1 score of {:.4f} \"\n",
    "      .format(cmax[0], cmax[1]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for C=1.0: 0.3725, accuracy: 41.16% \n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(C=cmax[0], penalty='l2', random_state=42, solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "log.fit(train_vocab_b, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab_b)\n",
    "log_f1 = metrics.f1_score(y_test, log_predicted_labels, average='weighted', labels=np.unique(log_predicted_labels))\n",
    "log_acc = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Print out the accuracy score for each value of C\n",
    "print(\"F1 score for C={}: {:.4f}, accuracy: {:.2f}% \".format(cmax[0], log_f1, log_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    rs = s.lower()\n",
    "    # Replace some separators with spaces\n",
    "    rs = re.sub('\\n|-|/|\\.', ' ', rs)\n",
    "    # Eliminate everything else that isn't a letter or number\n",
    "    rs = re.sub('[^0-9a-z ]+', '', rs)\n",
    "    # Eliminate extraneous spaces\n",
    "    rs = re.sub('\\s{2,}', ' ', rs)\n",
    "    prs = []\n",
    "    # Drop some low-value words\n",
    "    dumbwords = ['is', 'it', 'to', 'the', 'and', 'not', 'no', 'on', 'of', 'for', 'as', 'by', 'in', 'by', 'am', 'etc', \\\n",
    "                 'was', 'that', 'has', 'at', 'or', 'we', 'be', 'had']\n",
    "    for word in rs.split():\n",
    "        # Eliminate the -ing and -ly suffices\n",
    "        word = word[:-3] if word[-3:]=='ing' and len(word) > 5 else word\n",
    "        word = word[:-2] if word[-2:]=='ly' and len(word) > 5 else word\n",
    "        # Trim words to 9 characters\n",
    "        word = word[:9] if len(word) > 9 else word\n",
    "        # Eliminate single-character words\n",
    "        if len(word) > 1 and word not in dumbwords:\n",
    "            prs.append(word)\n",
    "    \n",
    "    return \" \".join(prs)\n",
    "\n",
    "proc_train_data = [better_preprocessor(x) for x in X_train]\n",
    "proc_test_data = [better_preprocessor(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed: 24,298 words with accuracy: 0.4601\n",
      "Pre-processed: 22,994 words with accuracy: 0.4629\n",
      "Improvement: 0.0028\n",
      "\n",
      "Sample wrong answer from the preprocessed set, post #1785:\n",
      "unprocessed prediction: Las Vegas (1)\n",
      "preprocessed prediction: Las Vegas (1)\n",
      "true label: Phoenix (0)\n",
      "true data:  eddie great help very accommoda however this location suffers particula from lack organizat honesty multiple occasions have been here only have technicia oblivious lie straight my face claim needed unnecessa repairs my most recent visit 10 17 took them hours fulfill basic oil change three half hours all while other customers were com out per usual after hours employee grunt advised me stand outside wait them they would finished short stand outside observed absence sense urgency technicia monotonou grudge around slow carry out their orders until every order finished except mine observed group technicia most stand around semi circle loud curs profanity racial slurs careful they will purpose dodge try squeeze dime out your penny all while smil your face with blatant lie there even customer service phone number wall if your visit complete exception if youre wonder if called number your guess right buster customer service phone number joke rerouted me an endless loop dial tones call exception com big tires big mistake do come here save yourself stress money\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a baseline vectorizer\n",
    "vec = CountVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Make a preprocessed vectorizer\n",
    "vec_proc = CountVectorizer(preprocessor=better_preprocessor)\n",
    "vocab_proc = vec_proc.fit_transform(X_train)\n",
    "test_vocab_proc = vec_proc.transform(X_test)\n",
    "\n",
    "# Fit and predict the baseline\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(vocab, y_train)\n",
    "log_predicted_labels = log.predict(test_vocab)\n",
    "log_score = metrics.accuracy_score(y_test, log_predicted_labels)\n",
    "\n",
    "# Fit and predict the pre-processed set\n",
    "log_proc = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log_proc.fit(vocab_proc, y_train)\n",
    "log_proc_predicted_labels = log_proc.predict(test_vocab_proc)\n",
    "log_proc_score = metrics.accuracy_score(y_test, log_proc_predicted_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Unprocessed: {:,} words with accuracy: {:.4f}\\nPre-processed: {:,} words with accuracy: {:.4f}\"\n",
    "      .format(vocab.shape[1], log_score, vocab_proc.shape[1], log_proc_score))\n",
    "print(\"Improvement: {:.4f}\".format(log_proc_score-log_score))\n",
    "\n",
    "# Find a wrong answer and print it out for better analysis\n",
    "wrong = np.random.choice(np.where(y_test != log_proc_predicted_labels)[0].ravel())\n",
    "\n",
    "print(\"\\nSample wrong answer from the preprocessed set, post #{}:\".format(wrong))\n",
    "print(\"unprocessed prediction: {} ({})\".format(categories[log_predicted_labels[wrong]], log_predicted_labels[wrong]))\n",
    "print(\"preprocessed prediction: {} ({})\".format(categories[log_proc_predicted_labels[wrong]], log_proc_predicted_labels[wrong]))\n",
    "print(\"true label: {} ({})\".format(categories[y_test[wrong]], y_test[wrong]))\n",
    "print(\"true data: \",proc_test_data[wrong])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 for TfidfVectorizer: 0.46, accuracy 47.37%\n",
      "\n",
      "TOP 3 MISIDENTIFIED DOCUMENTS:\n",
      "DOCUMENT #1\n",
      "Predicted label: Las Vegas[1] (P100.0%), True label: Phoenix[0] (P0.0%)\n",
      "R ratio: 37951.19\n",
      "Been here twice and both times I thought both times the value for the money was great.  The happy hour and lunch specials will keep me coming back.  Awesome Vegas roll.  I judge all sushi restaurants by the Vegas roll! We also had sashimi and spicy tuna roll which were great!\n",
      "----\n",
      "\n",
      "DOCUMENT #2\n",
      "Predicted label: Phoenix[0] (P100.0%), True label: Las Vegas[1] (P0.0%)\n",
      "R ratio: 45201.00\n",
      "Friendly and excellent guest services. Thank you Arizona our Waitness and Phi the manager. We will be back.\n",
      "----\n",
      "\n",
      "DOCUMENT #3\n",
      "Predicted label: Phoenix[0] (P100.0%), True label: Las Vegas[1] (P0.0%)\n",
      "R ratio: 38261497.63\n",
      "Just as good the one in Scottsdale, AZ. Great bartenders, lots of TV and lots of great beer on tap.\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a TFIDF Vectorizer and fit the training and dev vocabularies\n",
    "vec = TfidfVectorizer()\n",
    "vocab = vec.fit_transform(X_train)\n",
    "test_vocab = vec.transform(X_test)\n",
    "# Inverse vocabulary dictionary for word lookup\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "# Fit and predict a logistic regression based on the results\n",
    "lr = LogisticRegression(C=100, solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "lr.fit(vocab, y_train)\n",
    "lr_predicted_labels = lr.predict(test_vocab)\n",
    "# Calculate and print the resulting score\n",
    "lr_score = metrics.f1_score(y_test, lr_predicted_labels, average='weighted')\n",
    "lr_acc = metrics.accuracy_score(y_test, lr_predicted_labels)\n",
    "\n",
    "print(\"Baseline F1 for TfidfVectorizer: {:.2f}, accuracy {:.2f}%\\n\".format(lr_score, lr_acc*100))\n",
    "\n",
    "# Get the probabilities for each class prediction\n",
    "probs = lr.predict_proba(test_vocab)\n",
    "R = []\n",
    "# Run through the probabilities and calculate the R ratio as defined in the prompt, saving the value in the R list\n",
    "for x in range(0, len(probs)):\n",
    "    num = np.max(probs[x])\n",
    "    den = probs[x][np.unique(y_test).tolist().index(y_test[x])]\n",
    "    R.append(num/den)\n",
    "# Get the highest x number of R values\n",
    "top = np.argsort(np.array(R))[len(R)-3:]\n",
    "\n",
    "# Print the top misidentified documents as well as their TFIDF score and coefficients by class\n",
    "print(\"TOP {} MISIDENTIFIED DOCUMENTS:\".format(3))\n",
    "c = 1\n",
    "for i in top:\n",
    "    print(\"DOCUMENT #{}\".format(c))\n",
    "    print(\"Predicted label: {}[{}] (P{:.1f}%), True label: {}[{}] (P{:.1f}%)\"\n",
    "          .format(categories[lr_predicted_labels[i]], lr_predicted_labels[i], np.max(probs[i])*100, \\\n",
    "                            categories[y_test[i]], y_test[i], probs[i][y_test[i]]*100))\n",
    "    print(\"R ratio: {:.2f}\".format(R[i]))\n",
    "    print(X_test[i])\n",
    "    '''\n",
    "    print(\"\\n{:10} {:>10} {:>15} {:>15} {:>15} {:>22} \".format(\"word\", \"Tfidf\", categories[0], categories[1], \\\n",
    "                                                               categories[2], categories[3]))\n",
    "    for w in np.nonzero(dev_vocab[i])[1]:\n",
    "        coefs = np.round(lr.coef_[:,w], 2).flat\n",
    "        print(\"{:10} {:10.3f} {:>15} {:>15} {:>15} {:>22}\".format(inv_vocab[w], dev_vocab[i][0,w], \\\n",
    "                                                                  coefs[0], coefs[1], coefs[2], coefs[3])\n",
    "    '''\n",
    "    print(\"----\\n\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phoenix</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Charlotte</th>\n",
       "      <th>Cleveland</th>\n",
       "      <th>Pittsburgh</th>\n",
       "      <th>Montreal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>sucked</td>\n",
       "      <td>village</td>\n",
       "      <td>arrive</td>\n",
       "      <td>spots</td>\n",
       "      <td>remember</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tostadas</td>\n",
       "      <td>baby</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>terrific</td>\n",
       "      <td>sarah</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wonton</td>\n",
       "      <td>tempura</td>\n",
       "      <td>westerns</td>\n",
       "      <td>interested</td>\n",
       "      <td>window</td>\n",
       "      <td>break</td>\n",
       "      <td>peanut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adequate</td>\n",
       "      <td>wynn</td>\n",
       "      <td>waffles</td>\n",
       "      <td>blvd</td>\n",
       "      <td>favorites</td>\n",
       "      <td>filling</td>\n",
       "      <td>sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ray</td>\n",
       "      <td>lv</td>\n",
       "      <td>canada</td>\n",
       "      <td>lady</td>\n",
       "      <td>fall</td>\n",
       "      <td>florida</td>\n",
       "      <td>cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>orange</td>\n",
       "      <td>nevada</td>\n",
       "      <td>rooftop</td>\n",
       "      <td>concord</td>\n",
       "      <td>beers</td>\n",
       "      <td>laws</td>\n",
       "      <td>regulars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cooks</td>\n",
       "      <td>following</td>\n",
       "      <td>multiple</td>\n",
       "      <td>flight</td>\n",
       "      <td>deliver</td>\n",
       "      <td>hoagies</td>\n",
       "      <td>session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flies</td>\n",
       "      <td>banana</td>\n",
       "      <td>court</td>\n",
       "      <td>inventory</td>\n",
       "      <td>american</td>\n",
       "      <td>mt</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brews</td>\n",
       "      <td>tooth</td>\n",
       "      <td>upload</td>\n",
       "      <td>smelled</td>\n",
       "      <td>margarita</td>\n",
       "      <td>burgh</td>\n",
       "      <td>dish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chandler</td>\n",
       "      <td>noodle</td>\n",
       "      <td>yonge</td>\n",
       "      <td>gnocchi</td>\n",
       "      <td>le</td>\n",
       "      <td>milkshakes</td>\n",
       "      <td>rad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>parm</td>\n",
       "      <td>hash</td>\n",
       "      <td>rushed</td>\n",
       "      <td>recommending</td>\n",
       "      <td>boring</td>\n",
       "      <td>period</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>brisket</td>\n",
       "      <td>fremont</td>\n",
       "      <td>interior</td>\n",
       "      <td>waxhaw</td>\n",
       "      <td>dumpling</td>\n",
       "      <td>evening</td>\n",
       "      <td>banzo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bruschetta</td>\n",
       "      <td>las</td>\n",
       "      <td>markham</td>\n",
       "      <td>joe</td>\n",
       "      <td>normally</td>\n",
       "      <td>chain</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mesa</td>\n",
       "      <td>99</td>\n",
       "      <td>hakka</td>\n",
       "      <td>champaign</td>\n",
       "      <td>laid</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>peaceful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tempe</td>\n",
       "      <td>smoothies</td>\n",
       "      <td>gta</td>\n",
       "      <td>meatloaf</td>\n",
       "      <td>cle</td>\n",
       "      <td>lost</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>arizona</td>\n",
       "      <td>summerlin</td>\n",
       "      <td>flavours</td>\n",
       "      <td>boarded</td>\n",
       "      <td>lovely</td>\n",
       "      <td>cash</td>\n",
       "      <td>pies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>az</td>\n",
       "      <td>henderson</td>\n",
       "      <td>neighbourhood</td>\n",
       "      <td>pulled</td>\n",
       "      <td>poutine</td>\n",
       "      <td>hoagie</td>\n",
       "      <td>absolute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>valley</td>\n",
       "      <td>strip</td>\n",
       "      <td>favourite</td>\n",
       "      <td>uptown</td>\n",
       "      <td>mr</td>\n",
       "      <td>superb</td>\n",
       "      <td>therapists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>scottsdale</td>\n",
       "      <td>casino</td>\n",
       "      <td>flavour</td>\n",
       "      <td>calgary</td>\n",
       "      <td>montreal</td>\n",
       "      <td>update</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phoenix</td>\n",
       "      <td>vegas</td>\n",
       "      <td>toronto</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>madison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Phoenix  Las Vegas        Toronto     Charlotte  Cleveland  Pittsburgh  \\\n",
       "0         true     sucked        village        arrive      spots    remember   \n",
       "1     tostadas       baby  uncomfortable      terrific      sarah   starbucks   \n",
       "2       wonton    tempura       westerns    interested     window       break   \n",
       "3     adequate       wynn        waffles          blvd  favorites     filling   \n",
       "4          ray         lv         canada          lady       fall     florida   \n",
       "5       orange     nevada        rooftop       concord      beers        laws   \n",
       "6        cooks  following       multiple        flight    deliver     hoagies   \n",
       "7        flies     banana          court     inventory   american          mt   \n",
       "8        brews      tooth         upload       smelled  margarita       burgh   \n",
       "9     chandler     noodle          yonge       gnocchi         le  milkshakes   \n",
       "10        parm       hash         rushed  recommending     boring      period   \n",
       "11     brisket    fremont       interior        waxhaw   dumpling     evening   \n",
       "12  bruschetta        las        markham           joe   normally       chain   \n",
       "13        mesa         99          hakka     champaign       laid     buffalo   \n",
       "14       tempe  smoothies            gta      meatloaf        cle        lost   \n",
       "15     arizona  summerlin       flavours       boarded     lovely        cash   \n",
       "16          az  henderson  neighbourhood        pulled    poutine      hoagie   \n",
       "17      valley      strip      favourite        uptown         mr      superb   \n",
       "18  scottsdale     casino        flavour       calgary   montreal      update   \n",
       "19     phoenix      vegas        toronto     charlotte  cleveland  pittsburgh   \n",
       "\n",
       "        Montreal  \n",
       "0           only  \n",
       "1         around  \n",
       "2         peanut  \n",
       "3      sometimes  \n",
       "4         cheese  \n",
       "5       regulars  \n",
       "6        session  \n",
       "7   conversation  \n",
       "8           dish  \n",
       "9            rad  \n",
       "10          list  \n",
       "11         banzo  \n",
       "12           sad  \n",
       "13      peaceful  \n",
       "14          free  \n",
       "15          pies  \n",
       "16      absolute  \n",
       "17    therapists  \n",
       "18          game  \n",
       "19       madison  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = 20\n",
    "vec = CountVectorizer()\n",
    "train_vocab = vec.fit_transform(X_train)\n",
    "# Make an inverse vocabulary to look up words by index\n",
    "inv_vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "log = LogisticRegression(C=cmax[0], penalty='l2', solver='lbfgs', max_iter=4000, multi_class='multinomial')\n",
    "log.fit(train_vocab, y_train)\n",
    "# Get the words with the highest coefficients from each class\n",
    "topwords = np.argsort(log.coef_, 1)[:, train_vocab.shape[1]-top:]\n",
    "df_topwords = pd.DataFrame()\n",
    "\n",
    "for x in range(topwords.shape[0]):\n",
    "    wordlist = [inv_vocab[x] for x in topwords[x]]\n",
    "    df_topwords[categories[x]] = wordlist\n",
    "\n",
    "df_topwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
